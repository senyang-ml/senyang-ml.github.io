<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sen Yang</title>
  
  <subtitle>杨森</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://senyang-ml.github.io/"/>
  <updated>2020-09-22T13:30:16.492Z</updated>
  <id>http://senyang-ml.github.io/</id>
  
  <author>
    <name>杨森 &amp; yangsenius</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CoordConv - My Surprising Finding.</title>
    <link href="http://senyang-ml.github.io/2020/09/22/coordconv/"/>
    <id>http://senyang-ml.github.io/2020/09/22/coordconv/</id>
    <published>2020-09-22T11:53:41.000Z</published>
    <updated>2020-09-22T13:30:16.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="coordconv-的研究与分析">CoordConv 的研究与分析</h1><p>本文主要进行了如下的代码实验和分析：</p><ul><li><p>数据集：构造由坐标生成的one-hot heatmap与数值坐标之间的数据集：遵循<a href="https://arxiv.org/abs/1807.03247" target="_blank" rel="noopener">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</a>的<code>quarter split</code>方式。</p></li><li><p>模型：利用神经网络加坐标嵌入（MLP+Coord and Conv+Coord）的方式进行拟合与泛化测试。</p></li><li><p>效果分析与发现：用Pytorch复现验证了An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution中Superivised Coordinates Regression任务的CoordConv的泛化性能！</p></li></ul><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage.filters <span class="keyword">import</span> gaussian</span><br><span class="line"></span><br><span class="line">h=<span class="number">64</span></span><br><span class="line">w=<span class="number">64</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">position</span><span class="params">(H, W, is_cuda=False)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> is_cuda:</span><br><span class="line">        loc_w = torch.linspace(<span class="number">-1.0</span>, <span class="number">1.0</span>, W).cuda().unsqueeze(<span class="number">0</span>).repeat(H, <span class="number">1</span>)</span><br><span class="line">        loc_h = torch.linspace(<span class="number">-1.0</span>, <span class="number">1.0</span>, H).cuda().unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, W)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        loc_w = torch.linspace(<span class="number">-1.0</span>, <span class="number">1.0</span>, W).unsqueeze(<span class="number">0</span>).repeat(H, <span class="number">1</span>)</span><br><span class="line">        loc_h = torch.linspace(<span class="number">-1.0</span>, <span class="number">1.0</span>, H).unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, W)</span><br><span class="line">    loc = torch.cat([loc_w.unsqueeze(<span class="number">0</span>), loc_h.unsqueeze(<span class="number">0</span>)], <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> loc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">heatmap_generator</span><span class="params">(torch.utils.data.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, size=<span class="params">(h, w)</span>, sigma=<span class="number">2</span>, num=<span class="number">1000</span>, is_train=True, uniform=False, seed=<span class="number">0</span>, coordadd=False)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.dataset_size = num</span><br><span class="line">        self.heatmap_size = size</span><br><span class="line">        self.sigma = sigma</span><br><span class="line">        self.is_train = is_train</span><br><span class="line">        self.uniform = uniform</span><br><span class="line">        self.coordadd = coordadd</span><br><span class="line">        self.generator(seed, num)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generator</span><span class="params">(self,seed,num)</span>:</span></span><br><span class="line">        torch.manual_seed(seed)</span><br><span class="line">        np.random.seed(seed)</span><br><span class="line">        </span><br><span class="line">        self.db = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">            heatmap = np.zeros(shape=self.heatmap_size, dtype=np.float32)</span><br><span class="line">            <span class="keyword">if</span> self.uniform:</span><br><span class="line">                y = np.random.randint(self.heatmap_size[<span class="number">0</span>])</span><br><span class="line">                x = np.random.randint(self.heatmap_size[<span class="number">1</span>])</span><br><span class="line">            <span class="keyword">elif</span> self.is_train:</span><br><span class="line">                <span class="keyword">if</span> np.random.rand() &lt; <span class="number">1</span>/<span class="number">3</span>:  <span class="comment"># 1/3 probability: fall in this area</span></span><br><span class="line">                    x = np.random.randint(<span class="number">0</span>,  self.heatmap_size[<span class="number">1</span>]/<span class="number">2</span>)</span><br><span class="line">                    y = np.random.randint(self.heatmap_size[<span class="number">0</span>]/<span class="number">2</span>, self.heatmap_size[<span class="number">0</span>])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    x = np.random.randint(self.heatmap_size[<span class="number">1</span>])</span><br><span class="line">                    y = np.random.randint(self.heatmap_size[<span class="number">0</span>]/<span class="number">2</span>)     </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                y = np.random.randint(self.heatmap_size[<span class="number">0</span>]/<span class="number">2</span>, self.heatmap_size[<span class="number">0</span>])</span><br><span class="line">                x = np.random.randint(self.heatmap_size[<span class="number">1</span>]/<span class="number">2</span>, self.heatmap_size[<span class="number">1</span>])</span><br><span class="line">                </span><br><span class="line">            heatmap[y,x] = <span class="number">1</span></span><br><span class="line"><span class="comment">#             heatmap = gaussian(heatmap, sigma=self.sigma)</span></span><br><span class="line"><span class="comment">#             heatmap /= np.amax(heatmap)</span></span><br><span class="line">            self.db.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">'heatmap'</span>:heatmap,</span><br><span class="line">                    <span class="string">'pos'</span>: [x,y]  <span class="comment">#[x/self.heatmap_size[1],y/self.heatmap_size[0]]</span></span><br><span class="line">                &#125;)</span><br><span class="line">  </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self,)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.dataset_size</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        heatmap = self.db[idx][<span class="string">'heatmap'</span>]</span><br><span class="line">        heatmap = torch.as_tensor(heatmap, dtype=torch.float32)</span><br><span class="line">        pos = self.db[idx][<span class="string">'pos'</span>]</span><br><span class="line">        pos = torch.as_tensor(pos, dtype=torch.float32)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.coordadd:</span><br><span class="line">            loc = position(heatmap.size(<span class="number">0</span>), heatmap.size(<span class="number">1</span>))</span><br><span class="line">            heatmap = torch.cat([heatmap.unsqueeze(<span class="number">0</span>), loc], dim=<span class="number">0</span>) <span class="comment"># [3,h,w]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> heatmap, pos</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x=heatmap_generator(num=<span class="number">2</span>, seed=<span class="number">0</span>, is_train=<span class="literal">True</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line">d, _ = x[<span class="number">0</span>]</span><br><span class="line">d = d.numpy()</span><br><span class="line">print(d.shape)</span><br><span class="line">figs, axs = plt.subplots(<span class="number">1</span>,<span class="number">3</span>)</span><br><span class="line">axs.flat[<span class="number">0</span>].imshow(d[<span class="number">0</span>], cmap=<span class="string">'jet'</span>)</span><br><span class="line">axs.flat[<span class="number">1</span>].imshow(d[<span class="number">1</span>], cmap=<span class="string">'jet'</span>)</span><br><span class="line">axs.flat[<span class="number">2</span>].imshow(d[<span class="number">2</span>], cmap=<span class="string">'jet'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>(3, 64, 64)</code></pre><figure><img src="/2020/09/22/coordconv/image-20200922211840620.png" alt="image-20200922211840620"><figcaption>image-20200922211840620</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim, h, output_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.l = nn.Sequential(</span><br><span class="line">            nn.Linear(input_dim, h),</span><br><span class="line">            nn.LayerNorm(h),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(h, h),</span><br><span class="line">            nn.LayerNorm(h),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(h, output_dim),</span><br><span class="line">            </span><br><span class="line">        )</span><br><span class="line">       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.flatten(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.l(x).sigmoid()</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">convnet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, img_size, h, output_dim)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.conv = nn.Conv2d(<span class="number">3</span>,h,kernel_size=<span class="number">3</span>,padding=<span class="number">1</span>)</span><br><span class="line">        dim = img_size[<span class="number">0</span>]*img_size[<span class="number">1</span>]*h</span><br><span class="line">        self.l = nn.Sequential(</span><br><span class="line">            nn.Linear(dim, h),</span><br><span class="line">            nn.LayerNorm(h),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(h, h),</span><br><span class="line">            nn.LayerNorm(h),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(h, output_dim),</span><br><span class="line">            </span><br><span class="line">        )</span><br><span class="line">       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> self.l(x).sigmoid()</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0,1"</span></span><br><span class="line">    </span><br><span class="line">train_set = heatmap_generator(num=<span class="number">2000</span>, seed=<span class="number">0</span>, is_train=<span class="literal">True</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line">valid_set = heatmap_generator(num=<span class="number">500</span>, seed=<span class="number">1</span>, is_train=<span class="literal">False</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line">test_set = heatmap_generator(num=<span class="number">1000</span>, seed=<span class="number">2</span>, is_train=<span class="literal">False</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">64</span>, drop_last=<span class="literal">False</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=<span class="number">64</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">64</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">length = <span class="number">3</span>*h*w</span><br><span class="line">model = net(length, <span class="number">256</span>, <span class="number">2</span>).cuda()</span><br><span class="line">epochs = <span class="number">200</span></span><br><span class="line">criterion = nn.L1Loss(reduction=<span class="string">'mean'</span>).cuda()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">train_losses = []</span><br><span class="line">valid_losses = []</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    n1 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> id, (heatmaps, coords) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        heatmaps, coords = heatmaps.cuda(), coords.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        out = model(heatmaps)</span><br><span class="line">        wh = torch.tensor([w,h]).unsqueeze(<span class="number">0</span>).to(out.device) </span><br><span class="line">        out = out*wh</span><br><span class="line">        coords[:,<span class="number">0</span>]  = coords[:,<span class="number">0</span>] * w</span><br><span class="line">        coords[:,<span class="number">1</span>]  = coords[:,<span class="number">1</span>] * h</span><br><span class="line">        </span><br><span class="line">        loss = criterion(out, coords)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        t +=loss.item()</span><br><span class="line">        n1 +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> id%<span class="number">5</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"[&#123;&#125;/&#123;&#125;]:train loss:&#123;:.3f&#125;"</span>.format(e,id,loss.item()))</span><br><span class="line">            </span><br><span class="line">    train_losses.append(t/n1)</span><br><span class="line">    v = <span class="number">0</span></span><br><span class="line">    n2 = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">for</span> id, (heatmaps, coords) <span class="keyword">in</span> enumerate(valid_loader):</span><br><span class="line">        heatmaps, coords = heatmaps.cuda(), coords.cuda()</span><br><span class="line">        out = model(heatmaps)</span><br><span class="line">        </span><br><span class="line">        wh = torch.tensor([w,h]).unsqueeze(<span class="number">0</span>).to(out.device)      </span><br><span class="line">        out = out*wh     </span><br><span class="line">        coords[:,<span class="number">0</span>]  = coords[:,<span class="number">0</span>] * w</span><br><span class="line">        coords[:,<span class="number">1</span>]  = coords[:,<span class="number">1</span>] * h</span><br><span class="line">        </span><br><span class="line">        loss = criterion(out, coords)</span><br><span class="line">        v +=loss.item()</span><br><span class="line">        n2 +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> id%<span class="number">5</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"[&#123;&#125;/&#123;&#125;]:valid loss:&#123;:.3f&#125;"</span>.format(e,id,loss.item()))</span><br><span class="line">    </span><br><span class="line">    valid_losses.append(v/n2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>/<span class="number">0</span>]:train loss:<span class="number">66.604</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">5</span>]:train loss:<span class="number">56.174</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">10</span>]:train loss:<span class="number">62.741</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">15</span>]:train loss:<span class="number">61.951</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">20</span>]:train loss:<span class="number">54.515</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">25</span>]:train loss:<span class="number">56.364</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">30</span>]:train loss:<span class="number">56.916</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">0</span>]:valid loss:<span class="number">85.686</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">5</span>]:valid loss:<span class="number">86.086</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">0</span>]:train loss:<span class="number">55.726</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">5</span>]:train loss:<span class="number">62.358</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">10</span>]:train loss:<span class="number">65.846</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">15</span>]:train loss:<span class="number">59.389</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">20</span>]:train loss:<span class="number">62.986</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">25</span>]:train loss:<span class="number">63.098</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">30</span>]:train loss:<span class="number">58.376</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">0</span>]:valid loss:<span class="number">93.375</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">5</span>]:valid loss:<span class="number">93.775</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">0</span>]:train loss:<span class="number">61.380</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">5</span>]:train loss:<span class="number">61.412</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">10</span>]:train loss:<span class="number">58.127</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">15</span>]:train loss:<span class="number">60.215</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">20</span>]:train loss:<span class="number">59.366</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">25</span>]:train loss:<span class="number">58.879</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">30</span>]:train loss:<span class="number">60.355</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">0</span>]:valid loss:<span class="number">91.508</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">5</span>]:valid loss:<span class="number">91.908</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">0</span>]:train loss:<span class="number">60.517</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">5</span>]:train loss:<span class="number">55.383</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">10</span>]:train loss:<span class="number">63.124</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">15</span>]:train loss:<span class="number">56.669</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">20</span>]:train loss:<span class="number">62.911</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">25</span>]:train loss:<span class="number">62.907</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">30</span>]:train loss:<span class="number">51.110</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">0</span>]:valid loss:<span class="number">87.514</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">5</span>]:valid loss:<span class="number">87.915</span></span><br><span class="line">[<span class="number">4</span>/<span class="number">0</span>]:train loss:<span class="number">54.624</span></span><br><span class="line">[<span class="number">4</span>/<span class="number">5</span>]:train loss:<span class="number">59.512</span></span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">.....</span><br><span class="line">[<span class="number">194</span>/<span class="number">20</span>]:train loss:<span class="number">13.425</span></span><br><span class="line">[<span class="number">194</span>/<span class="number">25</span>]:train loss:<span class="number">8.053</span></span><br><span class="line">[<span class="number">194</span>/<span class="number">30</span>]:train loss:<span class="number">19.025</span></span><br><span class="line">[<span class="number">194</span>/<span class="number">0</span>]:valid loss:<span class="number">77.454</span></span><br><span class="line">[<span class="number">194</span>/<span class="number">5</span>]:valid loss:<span class="number">76.766</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">0</span>]:train loss:<span class="number">28.187</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">5</span>]:train loss:<span class="number">19.203</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">10</span>]:train loss:<span class="number">9.961</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">15</span>]:train loss:<span class="number">8.364</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">20</span>]:train loss:<span class="number">17.076</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">25</span>]:train loss:<span class="number">20.528</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">30</span>]:train loss:<span class="number">8.590</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">0</span>]:valid loss:<span class="number">80.181</span></span><br><span class="line">[<span class="number">195</span>/<span class="number">5</span>]:valid loss:<span class="number">80.544</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">0</span>]:train loss:<span class="number">10.859</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">5</span>]:train loss:<span class="number">9.294</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">10</span>]:train loss:<span class="number">19.587</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">15</span>]:train loss:<span class="number">23.072</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">20</span>]:train loss:<span class="number">9.886</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">25</span>]:train loss:<span class="number">20.046</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">30</span>]:train loss:<span class="number">17.368</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">0</span>]:valid loss:<span class="number">98.249</span></span><br><span class="line">[<span class="number">196</span>/<span class="number">5</span>]:valid loss:<span class="number">98.891</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">0</span>]:train loss:<span class="number">17.661</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">5</span>]:train loss:<span class="number">16.903</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">10</span>]:train loss:<span class="number">16.224</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">15</span>]:train loss:<span class="number">14.529</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">20</span>]:train loss:<span class="number">12.648</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">25</span>]:train loss:<span class="number">16.197</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">30</span>]:train loss:<span class="number">20.337</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">0</span>]:valid loss:<span class="number">89.428</span></span><br><span class="line">[<span class="number">197</span>/<span class="number">5</span>]:valid loss:<span class="number">90.051</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">0</span>]:train loss:<span class="number">8.844</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">5</span>]:train loss:<span class="number">8.096</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">10</span>]:train loss:<span class="number">5.853</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">15</span>]:train loss:<span class="number">12.344</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">20</span>]:train loss:<span class="number">7.346</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">25</span>]:train loss:<span class="number">14.299</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">30</span>]:train loss:<span class="number">20.001</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">0</span>]:valid loss:<span class="number">65.037</span></span><br><span class="line">[<span class="number">198</span>/<span class="number">5</span>]:valid loss:<span class="number">64.701</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">0</span>]:train loss:<span class="number">13.411</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">5</span>]:train loss:<span class="number">14.213</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">10</span>]:train loss:<span class="number">10.256</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">15</span>]:train loss:<span class="number">17.550</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">20</span>]:train loss:<span class="number">19.302</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">25</span>]:train loss:<span class="number">14.925</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">30</span>]:train loss:<span class="number">14.372</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">0</span>]:valid loss:<span class="number">74.663</span></span><br><span class="line">[<span class="number">199</span>/<span class="number">5</span>]:valid loss:<span class="number">75.049</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.style.use(<span class="string">'seaborn-bright'</span>)</span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">epochnum = list(range(<span class="number">0</span>,len(train_losses)))</span><br><span class="line"></span><br><span class="line">plt.plot(epochnum, train_losses, color=<span class="string">'black'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.plot(epochnum, valid_losses, color=<span class="string">'red'</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>, fontdict=&#123;<span class="string">'family'</span>:<span class="string">'Arial'</span>&#125;)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>,fontdict=&#123;<span class="string">'family'</span>:<span class="string">'Arial'</span>&#125;)</span><br><span class="line">plt.xlim(<span class="number">0</span>, len(train_losses))</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">100</span>)</span><br><span class="line"><span class="comment">#plt.xticks(epochnum,100*list(epochnum) )</span></span><br><span class="line">plt.legend((<span class="string">'Train'</span>,</span><br><span class="line">            <span class="string">'Valid'</span>), loc=<span class="string">'best'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.title(<span class="string">"generalization error with coordadd by CoordMLP"</span>)</span><br><span class="line"></span><br><span class="line">plt.grid(linestyle=<span class="string">':'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/2020/09/22/coordconv/image-20200922211620495.png" alt="image-20200922211620495" style="zoom:50%;"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">model.cpu()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="keyword">for</span> id, (heatmaps, coords) <span class="keyword">in</span> enumerate(test_loader):</span><br><span class="line">        <span class="comment">#heatmaps, coords = heatmaps, coords.cuda()</span></span><br><span class="line">        out = model(heatmaps)</span><br><span class="line">        loss = criterion(out, coords)</span><br><span class="line">        <span class="keyword">if</span> id%<span class="number">5</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"[&#123;&#125;/&#123;&#125;]:test loss:&#123;:.3f&#125;"</span>.format(e,id,loss.item()))</span><br></pre></td></tr></table></figure><pre><code>[199/0]:test loss:0.339[199/5]:test loss:0.297[199/10]:test loss:0.317[199/15]:test loss:0.308</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">fig, ax1 = plt.subplots()</span><br><span class="line">fig2, ax2 = plt.subplots()</span><br><span class="line">print(model)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, p <span class="keyword">in</span> model.state_dict().items():</span><br><span class="line">    print(k)</span><br><span class="line">    <span class="comment">#print(p)</span></span><br><span class="line">    <span class="keyword">if</span> <span class="string">'weight'</span> <span class="keyword">in</span> k:</span><br><span class="line">        layer = k.split(<span class="string">'.'</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="keyword">if</span> i.dim() &gt; <span class="number">0</span> :</span><br><span class="line">                <span class="keyword">for</span> z <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                    ax1.scatter(layer, i[z].item(), s=<span class="number">1.5</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ax1.scatter(layer, i.item(), s=<span class="number">1.5</span>)</span><br><span class="line">                </span><br><span class="line">    <span class="keyword">if</span> <span class="string">'bias'</span> <span class="keyword">in</span> k:</span><br><span class="line">        layer = k.split(<span class="string">'.'</span>)[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> p:</span><br><span class="line">            <span class="comment">#</span></span><br><span class="line">            <span class="keyword">if</span> i.dim() &gt; <span class="number">0</span> :</span><br><span class="line">                <span class="keyword">for</span> z <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                    ax2.scatter(layer, i[z].item(), s=<span class="number">1.5</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                ax2.scatter(layer, i.item(), s=<span class="number">1.5</span>)</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">net(</span><br><span class="line">  (l): Sequential(</span><br><span class="line">    (<span class="number">0</span>): Linear(in_features=<span class="number">196608</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">1</span>): LayerNorm((<span class="number">256</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">2</span>): ReLU()</span><br><span class="line">    (<span class="number">3</span>): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">256</span>, bias=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">4</span>): LayerNorm((<span class="number">256</span>,), eps=<span class="number">1e-05</span>, elementwise_affine=<span class="literal">True</span>)</span><br><span class="line">    (<span class="number">5</span>): ReLU()</span><br><span class="line">    (<span class="number">6</span>): Linear(in_features=<span class="number">256</span>, out_features=<span class="number">2</span>, bias=<span class="literal">True</span>)</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">l<span class="number">.0</span>.weight</span><br><span class="line">l<span class="number">.0</span>.bias</span><br><span class="line">l<span class="number">.1</span>.weight</span><br><span class="line">l<span class="number">.1</span>.bias</span><br><span class="line">l<span class="number">.3</span>.weight</span><br><span class="line">l<span class="number">.3</span>.bias</span><br><span class="line">l<span class="number">.4</span>.weight</span><br><span class="line">l<span class="number">.4</span>.bias</span><br><span class="line">l<span class="number">.6</span>.weight</span><br><span class="line">l<span class="number">.6</span>.bias</span><br></pre></td></tr></table></figure><p><img src="/2020/09/22/coordconv/image-20200922211705256.png" alt="image-20200922211705256" style="zoom:50%;"></p><h2 id="可视化mlp的权重参数">可视化MLP的权重参数</h2><p>通过可视化MLP权重参数的取值，我们发现了不同层的取值范围的分布是不同的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">train_set = heatmap_generator(num=<span class="number">2000</span>, seed=<span class="number">0</span>, uniform=<span class="literal">True</span>)</span><br><span class="line">valid_set = heatmap_generator(num=<span class="number">500</span>, seed=<span class="number">1</span>, uniform=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> train_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]*w</span><br><span class="line">    y = pos[<span class="number">1</span>]*h</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'red'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> valid_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]*w</span><br><span class="line">    y = pos[<span class="number">1</span>]*h</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'blue'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, w)</span><br><span class="line">plt.ylim(<span class="number">0</span>, h)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="/2020/09/22/coordconv/image-20200922213013965.png" alt="image-20200922213013965"><figcaption>image-20200922213013965</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">train_set = heatmap_generator(num=<span class="number">2000</span>, seed=<span class="number">0</span>, is_train=<span class="literal">True</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line">valid_set = heatmap_generator(num=<span class="number">500</span>, seed=<span class="number">1</span>, is_train=<span class="literal">False</span>, coordadd=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">9</span>, <span class="number">4.5</span>), tight_layout=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> train_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]</span><br><span class="line">    y = pos[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'red'</span>, s=<span class="number">3</span>)</span><br><span class="line">    out = model(heatmap.flatten(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).cuda()).squeeze().detach().cpu().numpy()</span><br><span class="line">    x = out[<span class="number">0</span>]</span><br><span class="line">    y = out[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">1</span>].scatter(x, y, c=<span class="string">'red'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> valid_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]</span><br><span class="line">    y = pos[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'blue'</span>, s=<span class="number">3</span>)</span><br><span class="line">    out = model(heatmap.flatten(<span class="number">0</span>).unsqueeze(<span class="number">0</span>).cuda()).squeeze().detach().cpu().numpy()</span><br><span class="line">    x = out[<span class="number">0</span>]</span><br><span class="line">    y = out[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">1</span>].scatter(x, y, c=<span class="string">'blue'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, w)</span><br><span class="line">plt.ylim(<span class="number">0</span>, h)</span><br><span class="line">plt.title(<span class="string">"Unable to predict coords with coordadd by CoordMLP"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="/2020/09/22/coordconv/image-20200922211731117.png" alt="image-20200922211731117"><figcaption>image-20200922211731117</figcaption></figure><h2 id="coordmlp不具备预测坐标的泛化性能">CoordMLP不具备预测坐标的泛化性能</h2><p>把heatmap嵌入Coordinate Maps，然后Flatten，然后送进MLP中，模型可以在训练集上预测得很好，但是却不具备泛化能力。可以看到这种训练和验证集的划分方式还是非常有趣，并且是有难度的，它可以检测出模型预测坐标的泛化性能。</p><h1 id="coordconv-怎么样呢">CoordConv 怎么样呢？</h1><p>我用Pytorch复现了这个论文</p><p><a href="https://arxiv.org/abs/1807.03247" target="_blank" rel="noopener">An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution</a>论文提出了一种坐标嵌入的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> coordconv <span class="keyword">import</span> CoordConv1d, CoordConv2d, CoordConv3d</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">convnet1</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(convnet1, self).__init__()</span><br><span class="line">        self.coordconv = CoordConv2d(<span class="number">1</span>, <span class="number">8</span>, <span class="number">1</span>, with_r=<span class="literal">True</span>)</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv4 = nn.Conv2d(<span class="number">8</span>, <span class="number">8</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.conv5 = nn.Conv2d(<span class="number">8</span>, <span class="number">2</span>, <span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.pool = nn.MaxPool2d(kernel_size=<span class="number">64</span>, stride=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.unsqueeze(<span class="number">1</span>)</span><br><span class="line">        x = self.coordconv(x)</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.conv2(x)</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line">        x = self.conv4(x)</span><br><span class="line">        x = self.conv5(x)</span><br><span class="line">        x = self.pool(x)  <span class="comment"># [bs, 2, 1, 1]</span></span><br><span class="line">        x = x.squeeze()</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = <span class="string">"0"</span></span><br><span class="line"></span><br><span class="line">train_set = heatmap_generator(num=<span class="number">2000</span>, seed=<span class="number">0</span>, is_train=<span class="literal">True</span>,) <span class="comment"># coordadd=True)</span></span><br><span class="line">valid_set = heatmap_generator(num=<span class="number">500</span>, seed=<span class="number">1</span>, is_train=<span class="literal">False</span>,) <span class="comment"># coordadd=True)</span></span><br><span class="line">test_set = heatmap_generator(num=<span class="number">1000</span>, seed=<span class="number">2</span>, is_train=<span class="literal">False</span>,) <span class="comment"># coordadd=True)</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(train_set, batch_size=<span class="number">256</span>, drop_last=<span class="literal">False</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line">valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=<span class="number">32</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(test_set, batch_size=<span class="number">32</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">model = convnet1().cuda()</span><br><span class="line">epochs = <span class="number">1000</span></span><br><span class="line">criterion = nn.MSELoss(reduction=<span class="string">'mean'</span>).cuda()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.01</span>, weight_decay=<span class="number">0.00001</span>)</span><br><span class="line"></span><br><span class="line">train_losses = []</span><br><span class="line">valid_losses = []</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    t = <span class="number">0</span></span><br><span class="line">    n1 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> id, (heatmaps, coords) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        heatmaps, coords = heatmaps.cuda(), coords.cuda()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        out = model(heatmaps)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(out, coords)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        t +=loss.item()</span><br><span class="line">        n1 +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> id%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"[&#123;&#125;/&#123;&#125;]:train loss:&#123;:.3f&#125;"</span>.format(e,id,loss.item()))</span><br><span class="line">            </span><br><span class="line">    train_losses.append(t/n1)</span><br><span class="line">    v = <span class="number">0</span></span><br><span class="line">    n2 = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    <span class="keyword">for</span> id, (heatmaps, coords) <span class="keyword">in</span> enumerate(valid_loader):</span><br><span class="line">        heatmaps, coords = heatmaps.cuda(), coords.cuda()</span><br><span class="line">        out = model(heatmaps)</span><br><span class="line">        </span><br><span class="line">        loss = criterion(out, coords)</span><br><span class="line">        v +=loss.item()</span><br><span class="line">        n2 +=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> id%<span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"[&#123;&#125;/&#123;&#125;]:valid loss:&#123;:.3f&#125;"</span>.format(e,id,loss.item()))</span><br><span class="line">    </span><br><span class="line">    valid_losses.append(v/n2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>/<span class="number">0</span>]:train loss:<span class="number">1027.723</span></span><br><span class="line">[<span class="number">0</span>/<span class="number">0</span>]:valid loss:<span class="number">658.191</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">0</span>]:train loss:<span class="number">340.205</span></span><br><span class="line">[<span class="number">1</span>/<span class="number">0</span>]:valid loss:<span class="number">882.184</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">0</span>]:train loss:<span class="number">382.191</span></span><br><span class="line">[<span class="number">2</span>/<span class="number">0</span>]:valid loss:<span class="number">364.457</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">0</span>]:train loss:<span class="number">344.636</span></span><br><span class="line">[<span class="number">3</span>/<span class="number">0</span>]:valid loss:<span class="number">629.423</span></span><br><span class="line">[<span class="number">4</span>/<span class="number">0</span>]:train loss:<span class="number">343.474</span></span><br><span class="line">[<span class="number">4</span>/<span class="number">0</span>]:valid loss:<span class="number">511.996</span></span><br><span class="line">[<span class="number">5</span>/<span class="number">0</span>]:train loss:<span class="number">310.839</span></span><br><span class="line">[<span class="number">5</span>/<span class="number">0</span>]:valid loss:<span class="number">521.019</span></span><br><span class="line">[<span class="number">6</span>/<span class="number">0</span>]:train loss:<span class="number">330.839</span></span><br><span class="line">[<span class="number">6</span>/<span class="number">0</span>]:valid loss:<span class="number">530.305</span></span><br><span class="line">[<span class="number">7</span>/<span class="number">0</span>]:train loss:<span class="number">310.050</span></span><br><span class="line">[<span class="number">7</span>/<span class="number">0</span>]:valid loss:<span class="number">500.017</span></span><br><span class="line">[<span class="number">8</span>/<span class="number">0</span>]:train loss:<span class="number">313.915</span></span><br><span class="line">[<span class="number">8</span>/<span class="number">0</span>]:valid loss:<span class="number">526.495</span></span><br><span class="line">[<span class="number">9</span>/<span class="number">0</span>]:train loss:<span class="number">335.083</span></span><br><span class="line">[<span class="number">9</span>/<span class="number">0</span>]:valid loss:<span class="number">578.623</span></span><br><span class="line">[<span class="number">10</span>/<span class="number">0</span>]:train loss:<span class="number">321.808</span></span><br><span class="line">[<span class="number">10</span>/<span class="number">0</span>]:valid loss:<span class="number">590.830</span></span><br><span class="line">[<span class="number">11</span>/<span class="number">0</span>]:train loss:<span class="number">332.714</span></span><br><span class="line">[<span class="number">11</span>/<span class="number">0</span>]:valid loss:<span class="number">520.583</span></span><br><span class="line">[<span class="number">12</span>/<span class="number">0</span>]:train loss:<span class="number">315.320</span></span><br><span class="line">[<span class="number">12</span>/<span class="number">0</span>]:valid loss:<span class="number">506.796</span></span><br><span class="line">[<span class="number">13</span>/<span class="number">0</span>]:train loss:<span class="number">329.973</span></span><br><span class="line">[<span class="number">13</span>/<span class="number">0</span>]:valid loss:<span class="number">516.331</span></span><br><span class="line">[<span class="number">14</span>/<span class="number">0</span>]:train loss:<span class="number">296.927</span></span><br><span class="line">[<span class="number">14</span>/<span class="number">0</span>]:valid loss:<span class="number">440.843</span></span><br><span class="line">[<span class="number">15</span>/<span class="number">0</span>]:train loss:<span class="number">319.363</span></span><br><span class="line">[<span class="number">15</span>/<span class="number">0</span>]:valid loss:<span class="number">565.522</span></span><br><span class="line">[<span class="number">16</span>/<span class="number">0</span>]:train loss:<span class="number">339.193</span></span><br><span class="line">[<span class="number">16</span>/<span class="number">0</span>]:valid loss:<span class="number">630.012</span></span><br><span class="line">[<span class="number">17</span>/<span class="number">0</span>]:train loss:<span class="number">319.562</span></span><br><span class="line">[<span class="number">17</span>/<span class="number">0</span>]:valid loss:<span class="number">588.987</span></span><br><span class="line">[<span class="number">18</span>/<span class="number">0</span>]:train loss:<span class="number">325.077</span></span><br><span class="line">[<span class="number">18</span>/<span class="number">0</span>]:valid loss:<span class="number">555.196</span></span><br><span class="line">[<span class="number">19</span>/<span class="number">0</span>]:train loss:<span class="number">324.888</span></span><br><span class="line">[<span class="number">19</span>/<span class="number">0</span>]:valid loss:<span class="number">504.099</span></span><br><span class="line">[<span class="number">20</span>/<span class="number">0</span>]:train loss:<span class="number">328.839</span></span><br><span class="line">[<span class="number">20</span>/<span class="number">0</span>]:valid loss:<span class="number">501.208</span></span><br><span class="line">[<span class="number">21</span>/<span class="number">0</span>]:train loss:<span class="number">322.016</span></span><br></pre></td></tr></table></figure><p>​<br>​ ......</p><p>​<br>​ ......</p><p>​<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">489</span>/<span class="number">0</span>]:train loss:<span class="number">313.746</span></span><br><span class="line">[<span class="number">489</span>/<span class="number">0</span>]:valid loss:<span class="number">498.646</span></span><br><span class="line">[<span class="number">490</span>/<span class="number">0</span>]:train loss:<span class="number">331.695</span></span><br><span class="line">[<span class="number">490</span>/<span class="number">0</span>]:valid loss:<span class="number">570.283</span></span><br><span class="line">[<span class="number">491</span>/<span class="number">0</span>]:train loss:<span class="number">308.808</span></span><br><span class="line">[<span class="number">491</span>/<span class="number">0</span>]:valid loss:<span class="number">543.668</span></span><br><span class="line">[<span class="number">492</span>/<span class="number">0</span>]:train loss:<span class="number">331.526</span></span><br><span class="line">[<span class="number">492</span>/<span class="number">0</span>]:valid loss:<span class="number">609.543</span></span><br><span class="line">[<span class="number">493</span>/<span class="number">0</span>]:train loss:<span class="number">314.055</span></span><br><span class="line">[<span class="number">493</span>/<span class="number">0</span>]:valid loss:<span class="number">530.251</span></span><br><span class="line">[<span class="number">494</span>/<span class="number">0</span>]:train loss:<span class="number">303.886</span></span><br><span class="line">[<span class="number">494</span>/<span class="number">0</span>]:valid loss:<span class="number">557.143</span></span><br><span class="line">[<span class="number">495</span>/<span class="number">0</span>]:train loss:<span class="number">321.458</span></span><br><span class="line">[<span class="number">495</span>/<span class="number">0</span>]:valid loss:<span class="number">543.295</span></span><br><span class="line">[<span class="number">496</span>/<span class="number">0</span>]:train loss:<span class="number">311.247</span></span><br><span class="line">[<span class="number">496</span>/<span class="number">0</span>]:valid loss:<span class="number">497.568</span></span><br><span class="line">[<span class="number">497</span>/<span class="number">0</span>]:train loss:<span class="number">326.582</span></span><br><span class="line">[<span class="number">497</span>/<span class="number">0</span>]:valid loss:<span class="number">515.231</span></span><br><span class="line">[<span class="number">498</span>/<span class="number">0</span>]:train loss:<span class="number">333.465</span></span><br><span class="line">[<span class="number">498</span>/<span class="number">0</span>]:valid loss:<span class="number">553.942</span></span><br><span class="line">[<span class="number">499</span>/<span class="number">0</span>]:train loss:<span class="number">308.450</span></span><br><span class="line">[<span class="number">499</span>/<span class="number">0</span>]:valid loss:<span class="number">573.775</span></span><br><span class="line">[<span class="number">500</span>/<span class="number">0</span>]:train loss:<span class="number">304.610</span></span><br><span class="line">[<span class="number">500</span>/<span class="number">0</span>]:valid loss:<span class="number">585.026</span></span><br><span class="line">[<span class="number">501</span>/<span class="number">0</span>]:train loss:<span class="number">325.851</span></span><br><span class="line">[<span class="number">501</span>/<span class="number">0</span>]:valid loss:<span class="number">577.704</span></span><br><span class="line">[<span class="number">502</span>/<span class="number">0</span>]:train loss:<span class="number">313.319</span></span><br><span class="line">[<span class="number">502</span>/<span class="number">0</span>]:valid loss:<span class="number">510.165</span></span><br><span class="line">[<span class="number">503</span>/<span class="number">0</span>]:train loss:<span class="number">301.331</span></span><br><span class="line">[<span class="number">503</span>/<span class="number">0</span>]:valid loss:<span class="number">563.254</span></span><br><span class="line">[<span class="number">504</span>/<span class="number">0</span>]:train loss:<span class="number">294.513</span></span><br><span class="line">[<span class="number">504</span>/<span class="number">0</span>]:valid loss:<span class="number">550.867</span></span><br><span class="line">[<span class="number">505</span>/<span class="number">0</span>]:train loss:<span class="number">337.322</span></span><br><span class="line">[<span class="number">505</span>/<span class="number">0</span>]:valid loss:<span class="number">405.499</span></span><br><span class="line">[<span class="number">506</span>/<span class="number">0</span>]:train loss:<span class="number">210.640</span></span><br><span class="line">[<span class="number">506</span>/<span class="number">0</span>]:valid loss:<span class="number">440.354</span></span><br><span class="line">[<span class="number">507</span>/<span class="number">0</span>]:train loss:<span class="number">177.821</span></span><br><span class="line">[<span class="number">507</span>/<span class="number">0</span>]:valid loss:<span class="number">372.500</span></span><br><span class="line">[<span class="number">508</span>/<span class="number">0</span>]:train loss:<span class="number">152.111</span></span><br><span class="line">[<span class="number">508</span>/<span class="number">0</span>]:valid loss:<span class="number">314.090</span></span><br><span class="line">[<span class="number">509</span>/<span class="number">0</span>]:train loss:<span class="number">150.206</span></span><br><span class="line">[<span class="number">509</span>/<span class="number">0</span>]:valid loss:<span class="number">531.095</span></span><br><span class="line">[<span class="number">510</span>/<span class="number">0</span>]:train loss:<span class="number">131.546</span></span><br><span class="line">[<span class="number">510</span>/<span class="number">0</span>]:valid loss:<span class="number">320.094</span></span><br><span class="line">[<span class="number">511</span>/<span class="number">0</span>]:train loss:<span class="number">124.478</span></span><br><span class="line">[<span class="number">511</span>/<span class="number">0</span>]:valid loss:<span class="number">348.056</span></span><br><span class="line">[<span class="number">512</span>/<span class="number">0</span>]:train loss:<span class="number">82.704</span></span><br><span class="line">[<span class="number">512</span>/<span class="number">0</span>]:valid loss:<span class="number">138.187</span></span><br><span class="line">[<span class="number">513</span>/<span class="number">0</span>]:train loss:<span class="number">93.763</span></span><br><span class="line">[<span class="number">513</span>/<span class="number">0</span>]:valid loss:<span class="number">59.943</span></span><br><span class="line">[<span class="number">514</span>/<span class="number">0</span>]:train loss:<span class="number">28.917</span></span><br><span class="line">[<span class="number">514</span>/<span class="number">0</span>]:valid loss:<span class="number">83.261</span></span><br><span class="line">[<span class="number">515</span>/<span class="number">0</span>]:train loss:<span class="number">26.591</span></span><br><span class="line">[<span class="number">515</span>/<span class="number">0</span>]:valid loss:<span class="number">103.014</span></span><br><span class="line">[<span class="number">516</span>/<span class="number">0</span>]:train loss:<span class="number">20.655</span></span><br><span class="line">[<span class="number">516</span>/<span class="number">0</span>]:valid loss:<span class="number">90.283</span></span><br><span class="line">[<span class="number">517</span>/<span class="number">0</span>]:train loss:<span class="number">14.670</span></span><br><span class="line">[<span class="number">517</span>/<span class="number">0</span>]:valid loss:<span class="number">86.509</span></span><br><span class="line">[<span class="number">518</span>/<span class="number">0</span>]:train loss:<span class="number">7.979</span></span><br><span class="line">[<span class="number">518</span>/<span class="number">0</span>]:valid loss:<span class="number">81.485</span></span><br><span class="line">[<span class="number">519</span>/<span class="number">0</span>]:train loss:<span class="number">5.293</span></span><br><span class="line">[<span class="number">519</span>/<span class="number">0</span>]:valid loss:<span class="number">82.389</span></span><br><span class="line">[<span class="number">520</span>/<span class="number">0</span>]:train loss:<span class="number">6.404</span></span><br><span class="line">[<span class="number">520</span>/<span class="number">0</span>]:valid loss:<span class="number">79.925</span></span><br><span class="line">[<span class="number">521</span>/<span class="number">0</span>]:train loss:<span class="number">4.992</span></span><br><span class="line">[<span class="number">521</span>/<span class="number">0</span>]:valid loss:<span class="number">73.615</span></span><br><span class="line">[<span class="number">522</span>/<span class="number">0</span>]:train loss:<span class="number">2.514</span></span><br><span class="line">[<span class="number">522</span>/<span class="number">0</span>]:valid loss:<span class="number">68.852</span></span><br><span class="line">[<span class="number">523</span>/<span class="number">0</span>]:train loss:<span class="number">3.168</span></span><br><span class="line">[<span class="number">523</span>/<span class="number">0</span>]:valid loss:<span class="number">61.463</span></span><br><span class="line">[<span class="number">524</span>/<span class="number">0</span>]:train loss:<span class="number">2.194</span></span><br><span class="line">[<span class="number">524</span>/<span class="number">0</span>]:valid loss:<span class="number">58.418</span></span><br><span class="line">[<span class="number">525</span>/<span class="number">0</span>]:train loss:<span class="number">3.485</span></span><br><span class="line">[<span class="number">525</span>/<span class="number">0</span>]:valid loss:<span class="number">55.320</span></span><br><span class="line">[<span class="number">526</span>/<span class="number">0</span>]:train loss:<span class="number">2.697</span></span><br><span class="line">[<span class="number">526</span>/<span class="number">0</span>]:valid loss:<span class="number">46.612</span></span><br><span class="line">[<span class="number">527</span>/<span class="number">0</span>]:train loss:<span class="number">4.031</span></span><br><span class="line">[<span class="number">527</span>/<span class="number">0</span>]:valid loss:<span class="number">42.675</span></span><br><span class="line">[<span class="number">528</span>/<span class="number">0</span>]:train loss:<span class="number">3.537</span></span><br><span class="line">[<span class="number">528</span>/<span class="number">0</span>]:valid loss:<span class="number">35.026</span></span><br><span class="line">[<span class="number">529</span>/<span class="number">0</span>]:train loss:<span class="number">2.433</span></span><br><span class="line">[<span class="number">529</span>/<span class="number">0</span>]:valid loss:<span class="number">21.490</span></span><br><span class="line">[<span class="number">530</span>/<span class="number">0</span>]:train loss:<span class="number">1.950</span></span><br><span class="line">[<span class="number">530</span>/<span class="number">0</span>]:valid loss:<span class="number">6.599</span></span><br><span class="line">[<span class="number">531</span>/<span class="number">0</span>]:train loss:<span class="number">1.498</span></span><br><span class="line">[<span class="number">531</span>/<span class="number">0</span>]:valid loss:<span class="number">0.874</span></span><br><span class="line">[<span class="number">532</span>/<span class="number">0</span>]:train loss:<span class="number">0.641</span></span><br><span class="line">[<span class="number">532</span>/<span class="number">0</span>]:valid loss:<span class="number">0.786</span></span><br><span class="line">[<span class="number">533</span>/<span class="number">0</span>]:train loss:<span class="number">1.632</span></span><br><span class="line">[<span class="number">533</span>/<span class="number">0</span>]:valid loss:<span class="number">0.256</span></span><br><span class="line">[<span class="number">534</span>/<span class="number">0</span>]:train loss:<span class="number">0.318</span></span><br><span class="line">[<span class="number">534</span>/<span class="number">0</span>]:valid loss:<span class="number">1.699</span></span><br><span class="line">[<span class="number">535</span>/<span class="number">0</span>]:train loss:<span class="number">3.042</span></span><br><span class="line">[<span class="number">535</span>/<span class="number">0</span>]:valid loss:<span class="number">1.706</span></span><br><span class="line">[<span class="number">536</span>/<span class="number">0</span>]:train loss:<span class="number">1.189</span></span><br><span class="line">[<span class="number">536</span>/<span class="number">0</span>]:valid loss:<span class="number">1.651</span></span><br><span class="line">[<span class="number">537</span>/<span class="number">0</span>]:train loss:<span class="number">4.311</span></span><br><span class="line">[<span class="number">537</span>/<span class="number">0</span>]:valid loss:<span class="number">1.211</span></span><br><span class="line">[<span class="number">538</span>/<span class="number">0</span>]:train loss:<span class="number">2.011</span></span><br><span class="line">[<span class="number">538</span>/<span class="number">0</span>]:valid loss:<span class="number">4.156</span></span><br><span class="line">[<span class="number">539</span>/<span class="number">0</span>]:train loss:<span class="number">2.527</span></span><br><span class="line">[<span class="number">539</span>/<span class="number">0</span>]:valid loss:<span class="number">1.380</span></span><br><span class="line">[<span class="number">540</span>/<span class="number">0</span>]:train loss:<span class="number">1.754</span></span><br><span class="line">[<span class="number">540</span>/<span class="number">0</span>]:valid loss:<span class="number">7.980</span></span><br><span class="line">[<span class="number">541</span>/<span class="number">0</span>]:train loss:<span class="number">4.717</span></span><br><span class="line">[<span class="number">541</span>/<span class="number">0</span>]:valid loss:<span class="number">7.452</span></span><br><span class="line">[<span class="number">542</span>/<span class="number">0</span>]:train loss:<span class="number">2.004</span></span><br><span class="line">[<span class="number">542</span>/<span class="number">0</span>]:valid loss:<span class="number">0.953</span></span><br><span class="line">[<span class="number">543</span>/<span class="number">0</span>]:train loss:<span class="number">1.676</span></span><br><span class="line">[<span class="number">543</span>/<span class="number">0</span>]:valid loss:<span class="number">0.460</span></span><br><span class="line">[<span class="number">544</span>/<span class="number">0</span>]:train loss:<span class="number">1.610</span></span><br><span class="line">[<span class="number">544</span>/<span class="number">0</span>]:valid loss:<span class="number">0.386</span></span><br><span class="line">[<span class="number">545</span>/<span class="number">0</span>]:train loss:<span class="number">0.336</span></span><br><span class="line">[<span class="number">545</span>/<span class="number">0</span>]:valid loss:<span class="number">0.584</span></span><br></pre></td></tr></table></figure></p><p>​<br>​ ...... ​<br>......</p><p>​<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">992</span>/<span class="number">0</span>]:train loss:<span class="number">0.056</span></span><br><span class="line">[<span class="number">992</span>/<span class="number">0</span>]:valid loss:<span class="number">0.034</span></span><br><span class="line">[<span class="number">993</span>/<span class="number">0</span>]:train loss:<span class="number">0.034</span></span><br><span class="line">[<span class="number">993</span>/<span class="number">0</span>]:valid loss:<span class="number">0.043</span></span><br><span class="line">[<span class="number">994</span>/<span class="number">0</span>]:train loss:<span class="number">0.051</span></span><br><span class="line">[<span class="number">994</span>/<span class="number">0</span>]:valid loss:<span class="number">0.050</span></span><br><span class="line">[<span class="number">995</span>/<span class="number">0</span>]:train loss:<span class="number">0.041</span></span><br><span class="line">[<span class="number">995</span>/<span class="number">0</span>]:valid loss:<span class="number">0.061</span></span><br><span class="line">[<span class="number">996</span>/<span class="number">0</span>]:train loss:<span class="number">0.108</span></span><br><span class="line">[<span class="number">996</span>/<span class="number">0</span>]:valid loss:<span class="number">0.389</span></span><br><span class="line">[<span class="number">997</span>/<span class="number">0</span>]:train loss:<span class="number">0.539</span></span><br><span class="line">[<span class="number">997</span>/<span class="number">0</span>]:valid loss:<span class="number">0.133</span></span><br><span class="line">[<span class="number">998</span>/<span class="number">0</span>]:train loss:<span class="number">0.250</span></span><br><span class="line">[<span class="number">998</span>/<span class="number">0</span>]:valid loss:<span class="number">0.107</span></span><br><span class="line">[<span class="number">999</span>/<span class="number">0</span>]:train loss:<span class="number">0.247</span></span><br><span class="line">[<span class="number">999</span>/<span class="number">0</span>]:valid loss:<span class="number">1.734</span></span><br></pre></td></tr></table></figure></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib <span class="keyword">as</span> mpl</span><br><span class="line">mpl.style.use(<span class="string">'seaborn-bright'</span>)</span><br><span class="line">fig = plt.figure()</span><br><span class="line"></span><br><span class="line">epochnum = list(range(<span class="number">0</span>,len(train_losses)))</span><br><span class="line"></span><br><span class="line">plt.plot(epochnum, train_losses, color=<span class="string">'black'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.plot(epochnum, valid_losses, color=<span class="string">'red'</span>,linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>, fontdict=&#123;<span class="string">'family'</span>:<span class="string">'Arial'</span>&#125;)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>,fontdict=&#123;<span class="string">'family'</span>:<span class="string">'Arial'</span>&#125;)</span><br><span class="line">plt.xlim(<span class="number">0</span>, len(train_losses))</span><br><span class="line"></span><br><span class="line">plt.legend((<span class="string">'Train'</span>,</span><br><span class="line">            <span class="string">'Valid'</span>), loc=<span class="string">'best'</span>,fontsize=<span class="number">16</span>)</span><br><span class="line">plt.title(<span class="string">"generalization error with coordadd by CoordConv"</span>)</span><br><span class="line"></span><br><span class="line">plt.grid(linestyle=<span class="string">':'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="/2020/09/22/coordconv/image-20200922211755053.png" alt="image-20200922211755053"><figcaption>image-20200922211755053</figcaption></figure><h3 id="真的是太神奇了为什么在低500个周期开始急速收敛">真的是太神奇了，为什么在低500个周期开始急速收敛？？</h3><p>训练loss和验证的loss，开始极速收敛！</p><p>我怀疑这个代价函数的优化曲面有一个非常难以跳出的局部最优解，因为我的训练集和验证集的数据分布之间有一个非常明显的GAP！</p><p>如果能有人看到这个，和我探讨一下这个问题就好了！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">9</span>, <span class="number">4.5</span>), tight_layout=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> train_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]</span><br><span class="line">    y = pos[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'red'</span>, s=<span class="number">3</span>)</span><br><span class="line">    out = model(heatmap.unsqueeze(<span class="number">0</span>).cuda()).squeeze().detach().cpu().numpy()</span><br><span class="line">    x = out[<span class="number">0</span>]</span><br><span class="line">    y = out[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">1</span>].scatter(x, y, c=<span class="string">'red'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> heatmap, pos <span class="keyword">in</span> valid_set:</span><br><span class="line">    x = pos[<span class="number">0</span>]</span><br><span class="line">    y = pos[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">0</span>].scatter(x, y, c=<span class="string">'blue'</span>, s=<span class="number">3</span>)</span><br><span class="line">    out = model(heatmap.unsqueeze(<span class="number">0</span>).cuda()).squeeze().detach().cpu().numpy()</span><br><span class="line">    x = out[<span class="number">0</span>]</span><br><span class="line">    y = out[<span class="number">1</span>]</span><br><span class="line">    ax[<span class="number">1</span>].scatter(x, y, c=<span class="string">'blue'</span>, s=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.xlim(<span class="number">0</span>, w)</span><br><span class="line">plt.ylim(<span class="number">0</span>, h)</span><br><span class="line">plt.title(<span class="string">"Unable to predict coords with coordadd by CoordMLP"</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="/2020/09/22/coordconv/image-20200922211812693.png" alt="image-20200922211812693"><figcaption>image-20200922211812693</figcaption></figure><h4 id="很明显这个coordconv的网络具备了非常神奇的预测位置的能力">很明显，这个CoordConv的网络具备了非常神奇的预测位置的能力。</h4><h4 id="并且获取这个坐标回归能力是突然在某个周期获得的">并且获取这个坐标回归能力是突然在某个周期获得的！！！</h4><h4 id="后面还会继续思考这个问题">后面还会继续思考这个问题～</h4><blockquote><p>In this work, we expose and analyze a generic inability of CNNs to transform spatial representations between two different types: from a dense Cartesian representation to a sparse, pixel-based represen- tation or in the opposite direction. Though such transformations would seem simple for networks to learn, it turns out to be more difficult than expected, at least when models are comprised of the commonly used stacks of convolutional layers. While straightforward stacks of convolutional layers excel at tasks like image classification, they are not quite the right model for coordinate transform.</p></blockquote><blockquote><p>Throughout the rest of the paper, we examine the coordinate transform problem starting with the simplest scenario and ending with the most complex. Although results on toy problems should generally be taken with a degree of skepticism, starting small allows us to pinpoint the issue, exploring and understanding it in detail. Later sections then show that the phenomenon observed in the toy domain indeed appears in more real-world settings.</p><p>We begin by showing that coordinate transforms are surprisingly difficult even when the problem is <em>small and supervised</em>. In the <em>Supervised Coordinate Classification</em> task, given a pixel’s (x, y) coordinates as input, we train a CNN to highlight it as output. The <em>Supervised Coordinate Regression</em> task entails the inverse: given an input image containing a single white pixel, output its coordinates. We show that both problems are harder than expected using convolutional layers but become trivial by using a CoordConv layer (Section 4).</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;coordconv-的研究与分析&quot;&gt;CoordConv 的研究与分析&lt;/h1&gt;
&lt;p&gt;本文主要进行了如下的代码实验和分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;数据集：构造由坐标生成的one-hot heatmap与数值坐标之间的数据集：遵循&lt;a href=&quot;https://arxiv.org/abs/1807.03247&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution&lt;/a&gt;的&lt;code&gt;quarter split&lt;/code&gt;方式。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;模型：利用神经网络加坐标嵌入（MLP+Coord and Conv+Coord）的方式进行拟合与泛化测试。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;效果分析与发现：用Pytorch复现验证了An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution中Superivised Coordinates Regression任务的CoordConv的泛化性能！&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Reproduce" scheme="http://senyang-ml.github.io/tags/Reproduce/"/>
    
      <category term="Neural Network" scheme="http://senyang-ml.github.io/tags/Neural-Network/"/>
    
  </entry>
  
  <entry>
    <title>Detr</title>
    <link href="http://senyang-ml.github.io/2020/06/04/detr/"/>
    <id>http://senyang-ml.github.io/2020/06/04/detr/</id>
    <published>2020-06-04T10:19:09.000Z</published>
    <updated>2020-06-23T15:37:58.684Z</updated>
    
    <content type="html"><![CDATA[<p>Detr （DEtection TRansformer） 是最近很受关注的一个工作。论文叫做「End-to-end object detection with Transformers」， Facebook Research目前把它投稿到了2020年的ECCV。</p><p>鉴于网上有太多关于DETR的解读和评价，本文就不做太多的探讨，而致力于分析这两个概念：</p><ul><li>Set prediction and Hungarian Loss</li><li>Permutation Invariance</li></ul><a id="more"></a><h2 id="object-detection-set-prediction-loss">Object detection set prediction loss</h2><p>与以往的目标检测模型不一样，DETR模型推断一个固定长度（<span class="math inline">\(N\)</span>）的预测集合（可以理解为<span class="math inline">\(N\)</span>长度的序列），即输出<span class="math inline">\(N\)</span>个预测出的目标bbox和类别置信度<span class="math inline">\(\hat{y}=\left(\hat{b},\hat{c}\right)\)</span>，其中<span class="math inline">\(N\)</span>远大于图像中的真实目标数目。</p><blockquote><p>Note: 原文中，为了公式上的表达，作者假设真实目标数目也是<span class="math inline">\(N\)</span>,然后用<span class="math inline">\(\varnothing\)</span>来填充，表示非物体。</p></blockquote><h3 id="hungarian-loss">Hungarian Loss</h3><p>Hungarian loss 是在这篇论文<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>提出，是bipartite matching and Hungarian Algorithm 第一次中被用到Deep Learning的检测任务中。下面，我们来解释一下Hungarian Loss的用法和含义。</p><blockquote><p>如果需要了解「hungarian algorithm」，可以参考<a href="https://senyang-ml.github.io/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/">bipartite matching and hungarian algorithm</a>。</p></blockquote><p>Hungarian Algorithm是一种求解二分图（加权）最大匹配的一种算法，它必然可以求出一个最优解。首先，根据字面上，我们不要陷入一个误区：<code>Hungarian Loss是用来优化最大匹配的，用损失函数的方式来梯度反传求解最大匹配</code>。并不是这样，实际上是：<code>我们在使用Hungarian Algorithm求解出最大匹配之后，真实的目标框找到了与之相匹配的预测出的目标框，然后, 我们根据相匹配的的GT和Prediction,计算受损失函数。</code>计算的是两者的类别置信度对应的交叉熵损失和bbox对应<span class="math inline">\(\mathcal{L}_{\mathrm{box}}(\cdot)\)</span>损失。</p><p>所谓Hungarian Loss就是</p><p><strong>先用Hungarian Algorithm匹配，然后计算一个常规的loss。</strong></p><p>所以它是一个两步的过程：</p><ul><li>第一步是用Hungarian Method求解最优匹配</li></ul><p><span class="math display">\[\hat{\sigma}=\underset{\sigma \in \mathfrak{S}_{N}}{\arg \min } \sum_{i}^{N} \mathcal{L}_{\mathrm{match}}\left(y_{i}, \hat{y}_{\sigma(i)}\right)\]</span></p><p><span class="math display">\[\mathcal{L}_{\mathrm{match}} \left(y_{i}, \hat{y}_{\sigma(i)}\right)=-\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \hat{p}_{\sigma(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\sigma(i)}\right)\]</span></p><ul><li>第二步是固定匹配后，计算损失函数，进行梯度反传 <span class="math display">\[\mathcal{L}_{\text {Hungarian }}(y, \hat{y})=\sum_{i=1}^{N}\left[-\log \hat{p}_{\hat{\sigma}(i)}\left(c_{i}\right)+\mathbb{1}_{\left\{c_{i} \neq \varnothing\right\}} \mathcal{L}_{\mathrm{box}}\left(b_{i}, \hat{b}_{\hat{\sigma}}(i)\right)\right]\]</span></li></ul><blockquote><p>Sen Yang Note: 注意到一点，论文中在二分图匹配时，是以GT为参考，为每个<span class="math inline">\(y_{i}\)</span>考虑其最好的匹配选择, <span class="math inline">\(\hat{y}_{\sigma(i)}\)</span>. 所以Prediction的下标直接使用了真实gt的下标<span class="math inline">\(i\)</span>的索引<span class="math inline">\(\sigma(i)\)</span>，代表gt所匹配的prediciton。所以不管预测序列的<span class="math inline">\(N\)</span>个元素的排列顺序(permutation)是什么样的, Hungarian Method最后的匹配都是同一种最优解，用索引<span class="math inline">\(\sigma(i)\)</span>表示的好处就是，它可以来表达出<code>置换不变性(Permutation Invariant)</code>。这在论文中有提到。</p></blockquote><p>我们上面的论文都可以在Detr中代码中找到描述。</p><p><code>SetCriterion</code>的计算准则的说明文档是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SetCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" This class computes the loss for DETR.</span></span><br><span class="line"><span class="string">    The process happens in two steps:</span></span><br><span class="line"><span class="string">        1) we compute hungarian assignment between ground truth boxes and the outputs of the model</span></span><br><span class="line"><span class="string">        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes, matcher, weight_dict, eos_coef, losses)</span>:</span></span><br><span class="line">        <span class="string">""" Create the criterion.</span></span><br><span class="line"><span class="string">        Parameters:</span></span><br><span class="line"><span class="string">            num_classes: number of object categories, omitting the special no-object category</span></span><br><span class="line"><span class="string">            matcher: module able to compute a matching between targets and proposals</span></span><br><span class="line"><span class="string">            weight_dict: dict containing as key the names of the losses and as values their relative weight.</span></span><br><span class="line"><span class="string">            eos_coef: relative classification weight applied to the no-object category</span></span><br><span class="line"><span class="string">            losses: list of all the losses to be applied. See get_loss for list of available losses.</span></span><br><span class="line"><span class="string">        """</span></span><br></pre></td></tr></table></figure><p><code>HungarianMatcher</code>匹配的说明文档和代码是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Modules to compute the matching cost and solve the corresponding LSAP.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> scipy.optimize <span class="keyword">import</span> linear_sum_assignment</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> util.box_ops <span class="keyword">import</span> box_cxcywh_to_xyxy, generalized_box_iou</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HungarianMatcher</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""This class computes an assignment between the targets and the predictions of the network</span></span><br><span class="line"><span class="string">    For efficiency reasons, the targets don't include the no_object. Because of this, in general,</span></span><br><span class="line"><span class="string">    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,</span></span><br><span class="line"><span class="string">    while the others are un-matched (and thus treated as non-objects).</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, cost_class: float = <span class="number">1</span>, cost_bbox: float = <span class="number">1</span>, cost_giou: float = <span class="number">1</span>)</span>:</span></span><br><span class="line">        <span class="string">"""Creates the matcher</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            cost_class: This is the relative weight of the classification error in the matching cost</span></span><br><span class="line"><span class="string">            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost</span></span><br><span class="line"><span class="string">            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.cost_class = cost_class</span><br><span class="line">        self.cost_bbox = cost_bbox</span><br><span class="line">        self.cost_giou = cost_giou</span><br><span class="line">        <span class="keyword">assert</span> cost_class != <span class="number">0</span> <span class="keyword">or</span> cost_bbox != <span class="number">0</span> <span class="keyword">or</span> cost_giou != <span class="number">0</span>, <span class="string">"all costs cant be 0"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, outputs, targets)</span>:</span></span><br><span class="line">        <span class="string">""" Performs the matching</span></span><br><span class="line"><span class="string">        Params:</span></span><br><span class="line"><span class="string">            outputs: This is a dict that contains at least these entries:</span></span><br><span class="line"><span class="string">                 "pred_logits": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits</span></span><br><span class="line"><span class="string">                 "pred_boxes": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates</span></span><br><span class="line"><span class="string">            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:</span></span><br><span class="line"><span class="string">                 "labels": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth</span></span><br><span class="line"><span class="string">                           objects in the target) containing the class labels</span></span><br><span class="line"><span class="string">                 "boxes": Tensor of dim [num_target_boxes, 4] containing the target box coordinates</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            A list of size batch_size, containing tuples of (index_i, index_j) where:</span></span><br><span class="line"><span class="string">                - index_i is the indices of the selected predictions (in order)</span></span><br><span class="line"><span class="string">                - index_j is the indices of the corresponding selected targets (in order)</span></span><br><span class="line"><span class="string">            For each batch element, it holds:</span></span><br><span class="line"><span class="string">                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        bs, num_queries = outputs[<span class="string">"pred_logits"</span>].shape[:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We flatten to compute the cost matrices in a batch</span></span><br><span class="line">        out_prob = outputs[<span class="string">"pred_logits"</span>].flatten(<span class="number">0</span>, <span class="number">1</span>).softmax(<span class="number">-1</span>)  <span class="comment"># [batch_size * num_queries, num_classes]</span></span><br><span class="line">        out_bbox = outputs[<span class="string">"pred_boxes"</span>].flatten(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># [batch_size * num_queries, 4]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Also concat the target labels and boxes</span></span><br><span class="line">        tgt_ids = torch.cat([v[<span class="string">"labels"</span>] <span class="keyword">for</span> v <span class="keyword">in</span> targets])</span><br><span class="line">        tgt_bbox = torch.cat([v[<span class="string">"boxes"</span>] <span class="keyword">for</span> v <span class="keyword">in</span> targets])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the classification cost. Contrary to the loss, we don't use the NLL,</span></span><br><span class="line">        <span class="comment"># but approximate it in 1 - proba[target class].</span></span><br><span class="line">        <span class="comment"># The 1 is a constant that doesn't change the matching, it can be ommitted.</span></span><br><span class="line">        cost_class = -out_prob[:, tgt_ids]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the L1 cost between boxes</span></span><br><span class="line">        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the giou cost betwen boxes</span></span><br><span class="line">        cost_giou = -generalized_box_iou(box_cxcywh_to_xyxy(out_bbox), box_cxcywh_to_xyxy(tgt_bbox))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Final cost matrix</span></span><br><span class="line">        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou</span><br><span class="line">        C = C.view(bs, num_queries, <span class="number">-1</span>).cpu()</span><br><span class="line"></span><br><span class="line">        sizes = [len(v[<span class="string">"boxes"</span>]) <span class="keyword">for</span> v <span class="keyword">in</span> targets]</span><br><span class="line">        indices = [linear_sum_assignment(c[i]) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(C.split(sizes, <span class="number">-1</span>))]</span><br><span class="line">        <span class="keyword">return</span> [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) <span class="keyword">for</span> i, j <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_matcher</span><span class="params">(args)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> HungarianMatcher(cost_class=args.set_cost_class, cost_bbox=args.set_cost_bbox, cost_giou=args.set_cost_giou)</span><br></pre></td></tr></table></figure><p>这个函数的目标就是我们刚才讲到的第一步，用Hungarian Method求解最优匹配。</p><p>函数的返回值是最后的最优匹配结果：index_i 和index_j的匹配结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">A list of size batch_size, containing tuples of (index_i, index_j) where:</span><br><span class="line">                - index_i is the indices of the selected predictions (in order)</span><br><span class="line">                - index_j is the indices of the corresponding selected targets (in order)</span><br><span class="line">            For each batch element, it holds:</span><br><span class="line">                len(index_i) &#x3D; len(index_j) &#x3D; min(num_queries, num_target_boxes)</span><br></pre></td></tr></table></figure><p>因为这个过程是不参与反向传播的，是一个只有前向计算的过程，所以我们可以注意到</p><p>在前向计算中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, outputs, targets)</span>:</span></span><br></pre></td></tr></table></figure><p>直接用<code>@torch.no_grad()</code>取消了梯度的产生。</p><p>另外需要说明的是：<code>scipy.optimize.linear_sum_assignment(cost_matrix)</code>就是一个用匈牙利算法进行分配的API。Detr直接调用了这个库来求解匹配结果。</p><h2 id="permutation-invariant">Permutation Invariant</h2><p>我们先来讲一下permutation invariant的概念。置换不变性是什么意思呢？</p><p>举一个简单的例子：<strong>maxpooling就是一个对集合具有置换不变性的特性</strong>。 <span class="math display">\[max\left\{1,3,4,2,6\right\}=max\left\{4,6,3,2,1\right\}=6\]</span> 无论1,2,3,4,6这几个元素构成序列的位置顺序是怎么样的，取max的结果一定是6。</p><h3 id="positional-encoding">Positional Encoding</h3><p>Detr提到了可以用embedding或者position encoding来消除<code>置换不变性</code>，这是为什么呢？</p><p>因为，位置嵌入编码是可以影响<code>置换不变性</code>，使其变为<code>置换同变性(Permutation Variant)</code>。</p><p>比如，我们给max操作，引入乘以位置位置下标的编码方式：</p><p>即，位置编码为<span class="math inline">\(p_1,p_2,...,p_i=1,2,...,i\)</span> <span class="math display">\[max\left\{1*p_1,3*p_2,4*p_3,2*p_4,6*p_5\right\}=30\\max\left\{4*p_1,6*p_2,3*p_3,2*p_4,1*p_5\right\}=12\]</span></p><p>Detr把图像特征的<strong><code>[ Batchsize, Feature number, height, weight]</code></strong>的形状，展开成<strong><code>[Batchsize, height*weight, feature number]</code></strong>的形状，其2D空间结构消失了，这种信息的丢失就必然需要用位置编码的策略来表达原来完整的2D结构信息。Transformer的position encoding策略就可以达到满足这个要求。</p><h3 id="集合与序列">集合与序列</h3><p><code>Position encoding</code>让<code>集合</code>变成了<code>序列</code>，使得<code>置换不变性(permutation-invariance)</code>消失，<code>置换同变性(permutation-envariance)</code>产生。</p><h2 id="后言">后言</h2><blockquote><p>只关注性能的提升而忽视创新是学界的灾难；</p><p>我在想，把从2012年到2020年所有顶会提出的改良性能的方法或者技巧，都组合在一起，在COCO detection的性能上的mAP就真得可以刷到100%吗？或者说技巧们都是互不影响的增量式前进？</p><p>第一性原理可以促使我们不断去对一个问题的本质追根溯源，而优秀方法的强大力场，又会让我们活在前人思维框架的束缚当中。这几年目标检测方法，明显能感受到某些框架在束缚着它们。而Detr是一次追根溯源的思考，尽管有很多相似的影子，但它还是充满了活力。</p></blockquote><p>本文先介绍了【Hungarian Loss】和【Permutation Invariant】，【Transformer】部分且听下回分解。</p><section class="footnotes"><hr><ol><li id="fn1"><p>End-to-end people detection in crowed scene, In CVPR 2015<a href="#fnref1" class="footnote-back">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Detr （DEtection TRansformer） 是最近很受关注的一个工作。论文叫做「End-to-end object detection with Transformers」， Facebook Research目前把它投稿到了2020年的ECCV。&lt;/p&gt;
&lt;p&gt;鉴于网上有太多关于DETR的解读和评价，本文就不做太多的探讨，而致力于分析这两个概念：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set prediction and Hungarian Loss&lt;/li&gt;
&lt;li&gt;Permutation Invariance&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Hungarian Match" scheme="http://senyang-ml.github.io/tags/Hungarian-Match/"/>
    
      <category term="Object Detection" scheme="http://senyang-ml.github.io/tags/Object-Detection/"/>
    
      <category term="Transformer" scheme="http://senyang-ml.github.io/tags/Transformer/"/>
    
  </entry>
  
  <entry>
    <title>Bipartite Matching and Hungarian Algorithm</title>
    <link href="http://senyang-ml.github.io/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/"/>
    <id>http://senyang-ml.github.io/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/</id>
    <published>2020-06-01T10:53:41.000Z</published>
    <updated>2020-09-21T10:37:46.311Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">引言</h2><p>二分图匹配和匈牙利算法（Bipartite Matching and Hungarian Algorithm）在CV领域的后处理算法中是经常可以看到的，比如以下的一些论文：</p><ul><li>2017年的CVPR工作，OpenPose 利用bipartite matching 来进行，同关节类型的多个人体关键点分配到不同的隶属人体</li><li>2020年的End-to-end Object Detection with Transformer直接构造了一个Hungarian Loss，来解决预测目标与真实目标的分配问题</li><li>2020年CVPR Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation中多人人体3D距离匹配算法</li><li>多目标跟踪领域内的，前后相邻帧匹配问题</li><li>在Instance Segmentation领域，这也是很常见的</li></ul><p>顾名思义，二分图匹配是一个<code>分配</code>问题 (Assignment Problem)。</p><p>Hungarian Algorithm (匈牙利算法) 是在1955年提出的<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> .</p><p>如果想要彻底理解它，需要先掌握它所涉及到的一些概念，作为先验知识。如下所示： <a id="more"></a></p><ul><li><p>图</p></li><li><p>边与节点</p></li><li><p>增广路</p></li><li><p>匹配</p></li><li><p>最大匹配</p></li><li><p>完美匹配</p></li><li><p>二分图</p></li><li><p>匈牙利算法</p></li><li><p>KM算法</p></li><li><p><a href="https://www.jianshu.com/p/b06ea43b7c07" target="_blank" rel="noopener">BFS搜索与队列思想解决迷宫最短路径问题</a></p></li></ul><h2 id="二分图的最大匹配">二分图的最大匹配</h2><h3 id="名词解释">名词解释</h3><p>首先将我们遇到的问题转换成<strong>分配问题</strong>。我们的数据以一种<strong>二分图的数据结构</strong>呈现。</p><p>什么是二分图呢，它是一种特殊的图，即可以将图分为两组，两组内的节点之间没有连接边，两组之间节点存在连接关系。</p><p>我们的目标是寻找一个<strong>最大的匹配</strong>结果，使得匹配结果最合理。</p><p>比如说，我们想在一群男生和一群女生之间，根据他们各自喜欢的异性，找到一种能够成全更多对情侣的方案。又比如说，我们打算把工作项目，指派给员工们，希望用一种最大效率或者最低成本的方案。</p><p><strong>最大匹配</strong>，即，在任意一个节点最多只有一个边与之相连接的条件下，尽可能使得两组之间的匹配边数最多。这就是我们的最优匹配目标，那么，如何<strong>求解出一种匹配方式</strong>，能够达到最大匹配呢？</p><p>最粗暴的算法就是暴力搜索，然而它的时间复杂度是指数级O(n!)</p><p>那有没有可以降低复杂度，并能够确定找到最优解的算法呢？</p><p>有，<strong>匈牙利算法</strong>就是！</p><p>匈牙利算法是一个什么算法呢？一种<strong>搜索</strong>算法，怎么样去搜索解呢？</p><p>其中，引入一种<strong>增广路</strong>的概念，它能在暴力搜索的过程中能够用一种简单的策略来<strong>改进当前匹配</strong>结果，直到我们无法找到更优的匹配。</p><p>增广路是什么意思呢，就是一种特殊的<strong>交错路</strong>，它<strong>从一个未匹配点出发</strong>，经过未匹配边、已匹配边、未匹配边,...,直到<strong>到达另外一个未匹配点</strong>，那么这样交换交错路径（匹配边和未匹配边），就总能改进匹配，<strong>因为增广路上的未匹配边总比匹配边多1个，交换就意味着改进</strong>，因为我们想要最多的匹配边。</p><h3 id="术语理解">术语理解</h3><p>基本概念讲完了。求解最大匹配问题的一个算法是<strong>匈牙利算法</strong>，下面讲的概念都为这个算法服务。</p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602094646170.png" alt="Fig 5" style="zoom:50%;"></p><p><strong>交替路</strong>：从一个未匹配点出发，依次经过非匹配边、匹配边、非匹配边…形成的路径叫交替路。</p><p><strong>增广路</strong>：从一个未匹配点出发，走交替路，如果途径另一个未匹配点（出发的点不算），则这条交替路称为增广路（agumenting path）。例如，图 5 中的一条增广路如图 6 所示（图中的匹配点均用红色标出）：</p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602094720047.png" alt="Fig6" style="zoom:50%;"></p><p>增广路有一个重要特点：非匹配边比匹配边多一条。因此，研究增广路的意义是<strong>改进匹配</strong>。只要把增广路中的匹配边和非匹配边的身份交换即可。由于中间的匹配节点不存在其他相连的匹配边，所以这样做不会破坏匹配的性质。交换后，图中的匹配边数目比原来多了 1 条。</p><p>我们可以通过不停地找增广路来增加匹配中的匹配边和匹配点。找不到增广路时，达到最大匹配（这是增广路定理）。匈牙利算法正是这么做的。在给出匈牙利算法 DFS 和 BFS 版本的代码之前，先讲一下匈牙利树。</p><p><strong>匈牙利树</strong>一般由 BFS 构造（类似于 BFS 树）。从一个未匹配点出发运行 BFS（唯一的限制是，必须走交替路），直到不能再扩展为止。例如，由图 7，可以得到如图 8 的一棵 BFS 树：</p><figure><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602094844320.png" alt="匈牙利树"><figcaption>匈牙利树</figcaption></figure><p>这棵树存在一个叶子节点为非匹配点（7 号），但是匈牙利树要求所有叶子节点均为匹配点，因此这不是一棵匈牙利树。如果原图中根本不含 7 号节点，那么从 2 号节点出发就会得到一棵匈牙利树。这种情况如图 9 所示（顺便说一句，图 8 中根节点 2 到非匹配叶子节点 7 显然是一条增广路，沿这条增广路扩充后将得到一个完美匹配）。</p><h2 id="kuhn-munkres算法二分图最大权匹配">Kuhn-<em>Munkres</em>算法(二分图最大权匹配)</h2><p>在进入加权二分图之前，我们默认已经掌握了图、交错路径、增广路径、最大匹配和完美匹配的概念</p><p>上面探讨的二分图匹配问题中，任意两个节点之间，要么是有边的，要么是没有边。在很多情况下，节点之间的候选连接是有强度的，带权值的。</p><p>在这种情况下，整体最优的匹配是，我们希望找到一种方案（依然是一点至多只有一个边的条件）使得我们的成本最少或者权重和最大。</p><h3 id="原理">原理</h3><p><strong>Kuhn-Munkres</strong> 算法的原理是什么呢？</p><h4 id="先一言以蔽之">先一言以蔽之</h4><blockquote><p>The KM theorem transforms the problem from an op- timization problem of finding a max-weight matching into a combinatorial one of finding a perfect match- ing. It combinatorializes the weights. This is a classic technique in combinatorial optimization. KM定理将问题从寻找最大权重匹配的优化问题转变为寻找完美匹配的组合问题。 它组合了权重。 这是组合优化中的经典技术。</p></blockquote><p><strong>如何转换的呢</strong>？</p><p>KM又引入了<code>Feasible Labelings</code>&amp; <code>Equality Graphs</code>这两个概念</p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200601182550299.png" alt="Feasible Labelings &amp; Equality Graphs" style="zoom:50%;"></p><ul><li><p>A vetex labeling is a function <span class="math inline">\(\ell: V \rightarrow \mathcal{R}\)</span></p></li><li><p>A feasible labeling is one such that <span class="math display">\[\ell(x)+\ell(y) \geq w(x, y), \quad \forall x \in X, y \in Y\]</span></p></li><li><p>the Equality Graph (with respect to <span class="math inline">\(\ell\)</span>) is <span class="math inline">\(G=\left(V, E_{\ell}\right)\)</span> where <span class="math display">\[E_{\ell}=\{(x, y): \ell(x)+\ell(y)=w(x, y)\}\]</span></p></li></ul><h3 id="理论与证明">理论与证明</h3><p><code>Theorem[Kuhn-Munkres]:</code> If <span class="math inline">\(\ell\)</span> is feasible and <span class="math inline">\(M\)</span> is a Perfect matching in <span class="math inline">\(E_{\ell}\)</span> then <span class="math inline">\(M\)</span> is a max-weight matching.</p><blockquote><p>如果能找到一个取值节点策略 <span class="math inline">\(\ell: V \rightarrow \mathcal{R}\)</span> ，使得满足Feasible Labelings &amp; Equality Graphs 两个条件，就可以找到最大匹配</p></blockquote><p>The KM theorem transforms the problem from an optimization problem of finding a max-weight matching into a combinatorial one of finding a perfect matching. It combinatorializes the weights. This is a classic technique in combinatorial optimization.</p><p>Notice that the proof of the KM theorem says that for any matching <span class="math inline">\(M\)</span> and any feasible labeling <span class="math inline">\(\ell\)</span> we have <span class="math display">\[w(M) \leq \sum_{v \in V} \ell(v)\]</span> This has very strong echos of the max-flow min-cut theorem.</p><p><code>Proof:</code></p><blockquote><p>只要证明：任意一个匹配解的权重和的上界都不大于 <span class="math inline">\(\sum_{v \in V} \ell(v)\)</span></p></blockquote><p>Denote edge <span class="math inline">\(e \in E\)</span> by <span class="math inline">\(e=\left(e_{x}, e_{y}\right)\)</span> Let <span class="math inline">\(M^{\prime}\)</span> be any PM in <span class="math inline">\(G\)</span> (not necessarily in in <span class="math inline">\(E_{\ell}\)</span> ). since every <span class="math inline">\(v \in \bar{V}\)</span> is covered exactly once by <span class="math inline">\(M\)</span> we have <span class="math display">\[w\left(M^{\prime}\right)=\sum_{e \in M^{\prime}} w(e) \leq \sum_{e \in M^{\prime}}\left(\ell\left(e_{x}\right)+\ell\left(e_{y}\right)\right)\leq \sum_{v \in V} \ell(v)\]</span> so <span class="math inline">\(\sum_{v \in V} \ell(v)\)</span> is an upper-bound on the cost of any perfect matching. Now let <span class="math inline">\(M\)</span> be a PM in <span class="math inline">\(E_{\ell}\)</span>. Then <span class="math inline">\(w(M)=\sum_{e \in M} w(e)=\sum_{v \in V} \ell(v)\)</span> So <span class="math inline">\(w\left(M^{\prime}\right) \leq w(M)\)</span> and <span class="math inline">\(M\)</span> is optimal.</p><p><code>算法思路</code></p><p>This algorithm will be to Start with any feasible labeling <span class="math inline">\(\ell\)</span> and some matching <span class="math inline">\(M\)</span> in <span class="math inline">\(E_{\ell}\)</span> While <span class="math inline">\(M\)</span> is not perfect repeat the following:</p><ol type="1"><li>Find an augmenting path for <span class="math inline">\(M\)</span> in <span class="math inline">\(E_{\ell}\)</span> this increases size of <span class="math inline">\(M\)</span></li><li>If no augmenting path exists, improve <span class="math inline">\(\ell\)</span> to <span class="math inline">\(\ell^{\prime}\)</span> such that <span class="math inline">\(E_{\ell} \subset E_{\ell^{\prime}}\)</span> Go to 1 Note that in each step of the loop we will either be increasing the size of <span class="math inline">\(M\)</span> or <span class="math inline">\(E_{\ell}\)</span> so this process must terminate.</li></ol><p>Furthermore, when the process terminates, <span class="math inline">\(M\)</span> will be a perfect matching in <span class="math inline">\(E_{\ell}\)</span> for some feasible labeling <span class="math inline">\(\ell\)</span> So, by the Kuhn-Munkres theorem, <span class="math inline">\(M\)</span> will be a maxweight matching.</p><h3 id="具体步骤">具体步骤</h3><p><code>1.Finding an Initial Feasible Labelling</code>--其中一边端点为0，另一边取节点连接边权重的最大值</p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602095707628.png" alt="image-20200602095707628" style="zoom: 25%;"></p><p>Finding an initial feasible labeling is simple. Just use:</p><p><span class="math display">\[\forall y \in Y, \ell(y)=0, \quad \forall x \in X, \ell(x)=\max _{y \in Y}\{w(x, y)\}\]</span></p><p>With this labelling it is obvious that</p><p><span class="math display">\[\forall x \in X, y \in Y, w(x) \leq \ell(x)+\ell(y)\]</span></p><p><code>2.Improving Labellings</code></p><blockquote><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602110856305.png" alt="equality graph" style="zoom: 25%;"></p><p>这是初始labeling产生的equality graph的<span class="math inline">\(E_{\ell}\)</span>，节点和边用红色标注了出来. Note: 这个时候红色的图，只能找到最大匹配，形成不了完美匹配</p></blockquote><h4 id="扩展e_ell-subset-e_ellprime-的技巧">扩展<span class="math inline">\(E_{\ell} \subset E_{\ell^{\prime}}\)</span> 的技巧</h4><p>Let <span class="math inline">\(\ell\)</span> be a feasible labeling. Define neighbor of <span class="math inline">\(u \in V\)</span> and <span class="math inline">\(\operatorname{set} S \subseteq V\)</span> to be <span class="math display">\[N_{\ell}(u)=\left\{v:(u, v) \in E_{\ell},\right\}, \quad N_{\ell}(S)=\cup_{u \in S} N_{\ell}(u)\]</span> Lemma: Let <span class="math inline">\(S \subseteq X\)</span> and <span class="math inline">\(T=N_{\ell}(S) \neq Y.\)</span> Set</p><blockquote><p><code>Sen Yang Note:</code> Bipartite graph：the neighbours of <span class="math inline">\(x\in S\)</span> are the $ yY$, so <span class="math inline">\(T \subseteq Y\)</span>.</p></blockquote><p><span class="math display">\[\alpha_{\ell}=\min _{x \in S, y \notin T}\{\ell(x)+\ell(y)-w(x, y)\}\]</span> and <span class="math display">\[\ell^{\prime}(v)=\left\{\begin{array}{ll}\ell(v)-\alpha_{\ell} &amp; \text { if } v \in S \\\ell(v)+\alpha_{\ell} &amp; \text { if } v \in T \\\ell(v) &amp; \text { otherwise }\end{array}\right.\]</span></p><blockquote><p><code>Sen Yang Note:</code> 比如拿此时equality graph为例，<span class="math inline">\(x \in S, y \notin T\)</span>的条件就是 <span class="math inline">\(Y_3\)</span>和<span class="math inline">\(X_2,X_3\)</span>,那么<span class="math inline">\(\alpha_{\ell}=\min_{X_2,X_3,Y_3}\{8+0-6,4+0-1 \}=2\)</span></p><p>然后上面构造<span class="math inline">\(\ell^{\prime}(v)\)</span>的逻辑是，因为<span class="math inline">\(S\cup T= E_{\ell}\)</span>，这样还能保证其中每个边对应labeling的和<span class="math inline">\(\ell(v_S)-\alpha_{\ell}+\ell(v_T)+\alpha_{\ell}=\ell^{\prime}(v_S)+\ell^{\prime}(v_T)\)</span>依然是保持不变，那么原来属于,并且<span class="math inline">\(Y\)</span>的label值增加，<span class="math inline">\(X\)</span>的label值减少</p></blockquote><p>Then <span class="math inline">\(\ell^{\prime}\)</span> is a feasible labeling and (i) If <span class="math inline">\((x, y) \in E_{\ell}\)</span> for <span class="math inline">\(x \in S, y \in T\)</span> then <span class="math inline">\((x, y) \in E_{\ell^{\prime}}\)</span> (ii) If <span class="math inline">\((x, y) \in E_{\ell}\)</span> for <span class="math inline">\(x \notin S, y \notin T\)</span> then <span class="math inline">\((x, y) \in E_{\ell^{\prime}}\)</span> (iii) There is some edge <span class="math inline">\((x, y) \in E_{\ell^{\prime}}\)</span> for <span class="math inline">\(x \in S, y \notin T\)</span></p><blockquote><p><code>Sen Yang Note:</code></p><p>(i)一定可以满足，（ii）不一定能找到这样的(x,y)比如当前的图中找不到一个<span class="math inline">\((x, y) \in E_{\ell}\)</span> for <span class="math inline">\(x \notin S, y \notin T\)</span></p><p>（iii）多产生了一个新边 <span class="math inline">\((X_2, Y_3) \in E_{\ell^{\prime}}\)</span>，如下图。为什么会产生，这是因为我们<span class="math inline">\(\alpha_{\ell}=\min_{X_2,X_3,Y_3}\{8+0-6,4+0-1 \}\)</span></p><p>我们可以知道了这个</p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602115818179.png" alt="image-20200602115818179" style="zoom: 25%;"></p></blockquote><p>上面我们展示了扩展<span class="math inline">\(E_{\ell} \subset E_{\ell^{\prime}}\)</span> 的技巧，请注意，它并不是实际KM算法（Hungarian ）中的步骤，下面是真正的算法过程</p><h2 id="km算法二分图加权最大匹配的匈牙利方法">KM算法：二分图加权最大匹配的匈牙利方法</h2><h3 id="整体思路">整体思路</h3><ol type="1"><li><p>Generate initial labelling <span class="math inline">\(\ell\)</span> and matching <span class="math inline">\(M\)</span> in <span class="math inline">\(E_{\ell}\)</span></p></li><li><ul><li><p>If <span class="math inline">\(M\)</span> perfect, stop.</p></li><li><p>Otherwise pick free vertex <span class="math inline">\(u \in X\)</span> <span class="math inline">\(\operatorname{Set} S=\{u\}, T=\emptyset\)</span></p></li></ul></li><li><p>If <span class="math inline">\(\left.N_{\ell}(S)=T, \text { update labels (forcing } N_{\ell}(S) \neq T\right)\)</span> <span class="math display">\[\begin{array}{l} \alpha_{\ell}=\min _{s \in S, y \notin T}\{\ell(x)+\ell(y)-w(x, y)\} \\ \ell^{\prime}(v)=\left\{\begin{array}{ll} \ell(v)-\alpha_{\ell} &amp; \text { if } v \in S \\ \ell(v)+\alpha_{\ell} &amp; \text { if } v \in T \\ \ell(v) &amp; \text { otherwise } \end{array}\right. \end{array}\]</span></p></li><li><p>If <span class="math inline">\(N_{\ell}(S) \neq T,\)</span> pick <span class="math inline">\(y \in N_{\ell}(S)-T\)</span></p></li></ol><ul><li>If $ y$ free, <span class="math inline">\(u-y\)</span> is augmenting path. Augment <span class="math inline">\(M\)</span> and go to 2. <code>Note that:增强路为什么会在这个时候产生？</code><a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li><li>If <span class="math inline">\(y\)</span> matched, say to<span class="math inline">\(z\)</span>, extend <code>alternating tree:</code> <span class="math inline">\(S=S \cup\{z\}, T=T \cup\{y\} .\)</span><a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a> Go to 3</li></ul><h3 id="实战">实战</h3><p>迭代扩展匹配M至完美匹配</p><p>迭代等价图<span class="math inline">\(E_l\)</span></p><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602151923976.png" alt="image-20200602151923976" style="zoom:50%;"></p><ul><li>Initial Graph, trivial labelling and associated Equality Graph</li><li>Initial matching: <span class="math inline">\(\left(x_{3}, y_{1}\right),\left(x_{2}, y_{2}\right)\)</span> <code>初始的任意一种匹配M'</code> <span class="math inline">\(S=\left\{x_{1}\right\}, T=\emptyset\)</span></li><li>since <span class="math inline">\(N_{\ell}(S) \neq T,\)</span> do step 4 Choose <span class="math inline">\(y_{2} \in N_{\ell}(S)-T\)</span></li><li><span class="math inline">\(y_{2}\)</span> is matched so grow tree by adding <span class="math inline">\(\left(y_{2}, x_{2}\right)\)</span> ¡.e., <span class="math inline">\(S=\left\{x_{1}, x_{2}\right\}, T=\left\{y_{2}\right\}\)</span></li><li>At this point <span class="math inline">\(N_{\ell}(S)=T,\)</span> so goto 3</li></ul><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602152240643.png" alt="image-20200602152240643" style="zoom:50%;"></p><ul><li><p><span class="math inline">\(S=\left\{x_{1}, x_{2}\right\}, T=\left\{y_{2}\right\}\)</span> and <span class="math inline">\(N_{\ell}(S)=T\)</span></p></li><li><p>Calculate <span class="math inline">\(\alpha_{\ell}\)</span> <span class="math display">\[\begin{aligned}\alpha_{\ell} &amp;=\min _{x \in S, y \notin T}\left\{\begin{array}{ll}6+0-1, &amp; \left(x_{1}, y_{1}\right) \\6+0-0, &amp; \left(x_{1}, y_{3}\right) \\8+0-0, &amp; \left(x_{2}, y_{1}\right) \\8+0-6, &amp; \left(x_{2}, y_{3}\right)\end{array}\right.\\&amp;=2\end{aligned}\]</span></p></li><li><p>Reduce labels of <span class="math inline">\(S\)</span> by 2</p><p>Increase labels of <span class="math inline">\(T\)</span> by 2</p></li><li><p>Now <span class="math inline">\(N_{\ell}(S)=\left\{y_{2}, y_{3}\right\} \neq\left\{y_{2}\right\}=T\)</span></p></li></ul><p><img src="/2020/06/01/Bipartite-Matching-and-Hungarian-Algorithm/image-20200602152520026.png" alt="image-20200602152520026" style="zoom:50%;"></p><ul><li><p><span class="math inline">\(\boldsymbol{S}=\left\{x_{1}, x_{2}\right\}, N_{\ell}(S)=\left\{y_{2}, y_{3}\right\}, T=\left\{y_{2}\right\}\)</span></p></li><li>Choose <span class="math inline">\(y_{3} \in N_{\ell}(S)-T\)</span> and add it to <span class="math inline">\(T\)</span></li><li><span class="math inline">\(y_{3}\)</span> is not matched in <span class="math inline">\(M\)</span> so we have just found an alternating path <span class="math inline">\(x_{1}, y_{2}, x_{2}, y_{3}\)</span> with two free endpoints. We can therefore augment <span class="math inline">\(M\)</span> to get a larger matching in the new equality graph. This matching is perfect, so it must be optimal.</li><li><p>Note that matching <span class="math inline">\(\left(x_{1}, y_{2}\right),\left(x_{2}, y_{3}\right),\left(x_{3}, y_{1}\right)\)</span> has cost <span class="math inline">\(6+6+4=16\)</span> which is exactly the sum of the labels in our final feasible labelling.</p></li></ul><p>至此，我们从名词解释，到术语概念，到问题转换目标，到问题区分（二分图匹配和二分图加权最大匹配），到KM算法的原理与证明，再到算法步骤，最后举一个实际例分析，完整介绍了匈牙利算法。</p><p>后续，我们还会进一步研究匈牙利算法如何和一些CV任务进行结合。</p><h4 id="实现-python">实现-python</h4><p><a href="https://github.com/bmc/munkres/blob/master/munkres.py" target="_blank" rel="noopener"><strong>from</strong> munkres <strong>import</strong> Munkres</a></p><p><a href="https://github.com/scipy/scipy/blob/67146a96e6c227a85658322429f402ceab91875b/scipy/optimize/_lsap.py" target="_blank" rel="noopener">from scipy.optimize import linear_sum_assignment</a></p><h4 id="参考文献">参考文献</h4><p><a href="https://econweb.ucsd.edu/~v2crawford/hungar.pdf" target="_blank" rel="noopener">pdf 1960年手稿</a></p><p><a href="https://luzhijun.github.io/2016/10/10/匈牙利算法详解/" target="_blank" rel="noopener">匈牙利算法详解博客</a></p><p><a href="https://www.renfei.org/blog/bipartite-matching.html" target="_blank" rel="noopener">二分图的最大匹配、完美匹配和匈牙利算法</a></p><p><a href="https://github.com/tdedecko/hungarian-algorithm" target="_blank" rel="noopener">python算法实现 Hungarain</a></p><p><a href="https://blog.csdn.net/xuguangsoft/article/details/7861988" target="_blank" rel="noopener">匈牙利算法求二分图的最大匹配</a></p><p><a href="http://csclab.murraystate.edu/~bob.pilgrim/445/munkres.html" target="_blank" rel="noopener">Munkres' Assignment Algorithm /Modified for Rectangular Matrices</a></p><p><a href="http://hungarianalgorithm.com/examplehungarianalgorithm.php" target="_blank" rel="noopener">Weighted Hungarian Algorithm</a></p><p><a href="https://www.iteye.com/blog/philoscience-1754498" target="_blank" rel="noopener">博主对KM的理解</a></p><p><code>建议阅读下面两个文档，从数学和算法的角度分析了加权二分图的最大权匹配问题</code></p><ul><li><p><a href="http://www.cse.ust.hk/~golin/COMP572/Notes/Matching.pdf" target="_blank" rel="noopener">Bipartite Matching &amp; the Hungarian Metho</a></p></li><li><p><a href="https://sites.cs.ucsb.edu/~suri/cs231/Matching.pdf" target="_blank" rel="noopener">Matching</a></p></li></ul><p>-<a href="https://sites.cs.ucsb.edu/~suri/cs231/231.html" target="_blank" rel="noopener">数据结构与算法、组合优化的课程网站</a><!----></p><section class="footnotes"><hr><ol><li id="fn1"><p>Kuhn, H.W.: The hungarian method for the assignment problem (1955)<a href="#fnref1" class="footnote-back">↩</a></p></li><li id="fn2"><p>因为初始的<span class="math inline">\(\operatorname{Set} S=\{u\}, T=\emptyset\)</span> <span class="math inline">\(u\)</span>是一个未匹配（free, 即不在<span class="math inline">\(M\)</span>内）的一个顶点，而<span class="math inline">\(y \in N_{\ell}(S)-T\)</span>表示此时M匹配（）的一个末端顶点又连接到的一个未匹配顶点，增广路形成了！增广路形成可以让我们的<span class="math inline">\(M\)</span>匹配走向完美匹配！真的是太妙了！一定要注意到这一点。<a href="#fnref2" class="footnote-back">↩</a></p></li><li id="fn3"><p>扩展交替路的一种方法。添加的<span class="math inline">\(S=S \cup\{z\}, T=T \cup\{y\},(z,y)\)</span>是一个匹配边<a href="#fnref3" class="footnote-back">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;二分图匹配和匈牙利算法（Bipartite Matching and Hungarian Algorithm）在CV领域的后处理算法中是经常可以看到的，比如以下的一些论文：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2017年的CVPR工作，OpenPose 利用bipartite matching 来进行，同关节类型的多个人体关键点分配到不同的隶属人体&lt;/li&gt;
&lt;li&gt;2020年的End-to-end Object Detection with Transformer直接构造了一个Hungarian Loss，来解决预测目标与真实目标的分配问题&lt;/li&gt;
&lt;li&gt;2020年CVPR Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation中多人人体3D距离匹配算法&lt;/li&gt;
&lt;li&gt;多目标跟踪领域内的，前后相邻帧匹配问题&lt;/li&gt;
&lt;li&gt;在Instance Segmentation领域，这也是很常见的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;顾名思义，二分图匹配是一个&lt;code&gt;分配&lt;/code&gt;问题 (Assignment Problem)。&lt;/p&gt;
&lt;p&gt;Hungarian Algorithm (匈牙利算法) 是在1955年提出的&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; .&lt;/p&gt;
&lt;p&gt;如果想要彻底理解它，需要先掌握它所涉及到的一些概念，作为先验知识。如下所示：&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Algorithm" scheme="http://senyang-ml.github.io/tags/Algorithm/"/>
    
      <category term="Hungarian Match" scheme="http://senyang-ml.github.io/tags/Hungarian-Match/"/>
    
  </entry>
  
  <entry>
    <title>Enhance Hexo Next</title>
    <link href="http://senyang-ml.github.io/2020/05/24/enhance-hexo-next/"/>
    <id>http://senyang-ml.github.io/2020/05/24/enhance-hexo-next/</id>
    <published>2020-05-24T07:08:20.000Z</published>
    <updated>2020-05-31T08:44:40.993Z</updated>
    
    <content type="html"><![CDATA[<p>目标：希望让hexo的next主题，能达到vscode markdown enhanced插件的markdown和MathJax的渲染效果, 并同时提高写博客效率</p><p>方法：</p><ul><li>修改Next相关配置文件</li><li>利用Mac的截图和Typora的快速插图动作</li><li>利用edge浏览器的集锦功能</li></ul><a id="more"></a><p>在我寻找方案的过程中，我利用了新版的Mircosoft的Edge浏览器搜索相关资料。它完美衔接google的Chromium内核，增加了很独特的集锦功能，可以把整理的网页以及笔记放在一起。如下图所示，右侧简洁地记下了所有过程。</p><figure><img src="/2020/05/24/enhance-hexo-next/image-20200524153534179.png" alt="image-20200524153534179"><figcaption>image-20200524153534179</figcaption></figure><p><a href="https://blog.csdn.net/littlehaes/article/details/81503455" target="_blank" rel="noopener">hexo博客迁移到另一台电脑_json_littlehaes的博客-CSDN博客</a></p><p><a href="https://www.jianshu.com/p/0ee8b976ceab" target="_blank" rel="noopener">多个github帐号更新多个hexo博客 - 简书</a></p><p>修改Mathjax的渲染引擎，用了新的渲染链接</p><p><a href="http://docs.mathjax.org/en/v2.7-latest/configuration.html?highlight=cdn#using-a-local-configuration-file-with-a-cdn" target="_blank" rel="noopener">Loading and Configuring MathJax — MathJax 2.7 documentation</a></p><p>修改Next/source中的相关文件</p><p><a href="https://blog.csdn.net/qw8880000/article/details/80235648" target="_blank" rel="noopener">修改文章内链接样式 ｜ hexo_好好编程的博客-CSDN博客</a></p><p>最后，直接把集锦里的所有链接，都复制到你的markdown里面就可以里</p><p><strong>快速插图</strong></p><p>在markdown插入图片时，有一个非常方便的操作，前提是你使用了<strong>Tpyora</strong>这个完美的markdown工具</p><p>你打开偏好设置，点击图像，选择以下图中的<code>插入图片时```复制到指定路径为./filename</code>, 那么</p><p>你在直接复制图片到markdown的内容里时，图片会自动保存在hexo-assert-image自动生成的文件里，比如这里的enhance hexo next.md对应的文件夹<code>enhance hexo next</code> 下面。这样就可以快速地生成有效的图片路径</p><figure><img src="/2020/05/24/enhance-hexo-next/image-20200524152842014.png" alt="image-20200524152842014"><figcaption>image-20200524152842014</figcaption></figure><p>从windows写博客，到Macbook Pro写博客，效率和体验确实有挺大的提升。</p><h4 id="彩蛋----网页风格配色">彩蛋 -- 网页风格配色</h4><p>本网站的配色，完全手工挑选。怎么可以做到呢？很简单: 1. 可以点击你当前博客的某个要素，右击查看该元素的样式(css style)，现在的浏览器都有这个调试功能， 2. 在浏览器的一侧下方的样式中，出现了 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-tags</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline-block;</span><br><span class="line">    <span class="attribute">margin-right</span>: <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">13px</span>;</span><br><span class="line">    <span class="attribute">text-decoration</span>: none;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#658a3a</span>;</span><br><span class="line">    <span class="attribute">border-bottom</span>: none;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 3. 复制<code>.posts-expand .post-tags</code> 在 vscode中搜索相关的<code>.styl</code>的文件，找到<code>color、background</code>等属性的位置，可以稍微学习一下语法 4. 在浏览器中调试好颜色后，直接复制粘贴色号到你的相关的<code>.styl</code>的文件的相关位置就可以了！</p><p>本次的分享到这里结束啦～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目标：希望让hexo的next主题，能达到vscode markdown enhanced插件的markdown和MathJax的渲染效果, 并同时提高写博客效率&lt;/p&gt;
&lt;p&gt;方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;修改Next相关配置文件&lt;/li&gt;
&lt;li&gt;利用Mac的截图和Typora的快速插图动作&lt;/li&gt;
&lt;li&gt;利用edge浏览器的集锦功能&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Mac" scheme="http://senyang-ml.github.io/tags/Mac/"/>
    
      <category term="Hexo" scheme="http://senyang-ml.github.io/tags/Hexo/"/>
    
      <category term="Typora" scheme="http://senyang-ml.github.io/tags/Typora/"/>
    
  </entry>
  
  <entry>
    <title>Stacked Capsule Autoencoders-堆叠的胶囊自编码器</title>
    <link href="http://senyang-ml.github.io/2020/02/11/stacked-capsule-autoencoders/"/>
    <id>http://senyang-ml.github.io/2020/02/11/stacked-capsule-autoencoders/</id>
    <published>2020-02-11T11:18:17.000Z</published>
    <updated>2020-05-29T09:43:24.496Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">1. 引言</h2><p>《stacked capsule autoencoders》使用无监督的方式达到了98.5%的MNIST分类准确率。 <a href="https://arxiv.org/abs/1906.06818" target="_blank" rel="noopener">Stacked Capsule Autoencoders</a> 发表在 NeurIPS-2019，作者团队阵容豪华。可以说是官方capsule的第3个版本。前两个版本的是：</p><ul><li>Dynamic Routing Between Capsules<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li><li>Matrix capsule with EM routing<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li></ul><a id="more"></a><p>当然还有最早的<a href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf" target="_blank" rel="noopener">Transforming Auto-encoders</a>，发表在2011年ICANN，论文第一次引入“capsule”的概念。值得一提的是，这篇论文的作者是Hinton、Alex Krizhevsky等人，对，是AlexNet的Alex。原来Alex本人在2012年发表AlexNet之前在研究这种“奇怪”的东西。2011年的他可能没想到，第二年的他们，为了参与ImageNet大规模数据集图像识别挑战赛而设计的一款基于的传统CNN的AlexNet，引爆了接下来已经持续7年之久的“Deep Learning”潮流，现如今CVPR 2020投稿量都过10000了，是谁惹得“祸“的还不清楚吗？</p><h2 id="概念">2. 概念</h2><p>从2017年开始， Hinton等人研究的Capsule Network得到了深度学习社区的大量关注。可以说Capsule Network在反思CNN的一些固有偏见，比如CNN的学习过分强调不变性（invariant ）特征的学习，数据增强也服务于这一目的。而这样做，实际上，忽略了一个真实世界中的事实：</p><p>1）<strong>物体-部件 关系（Object-Part-relationship）是视角不变的（viewpoint invariant）</strong>， 2）<strong>物体-观察者（Object-Viewer-relationship） 是视角同变性（viewpoint equivariant）的</strong>。</p><p>equivariant：<span class="math inline">\(\forall_{T \in \mathcal{T}} Tf(\mathbf{x}) = f(T\mathbf{x})\)</span> invariant： <span class="math inline">\(\forall_{T \in \mathcal{T}} Tf(\mathbf{x}) = f(T\mathbf{x})\)</span></p><p>并且Capsule强调物体的存在是因为当部件以合理的关系组合才得以存在，所以进一步引出了routing的机制，来发掘part-whole关系。</p><p>Matrix Capsule的那篇论文中提出，用混合高斯模型来学习这些关系，并提出了EM routing的算法，它从GMM和EM的角度，解释为更低层capsule与更高层capsule之间的自聚类算法。然而这种计算一旦放在前向传播会大大增加计算量，这也是其模型理论受限的部分， 所以其实验数据集主打还是MNIST, SmallNorb，如果换做更大一点的ImageNet, COCO，不知道Capsule的精深理念能不能发扬光大。可能Hinton深知自己的先驱身份所以他应该相信后面会有人帮他填满这些实验？</p><p>不过这种Gaussian Mixture的建模方式是非常合理的，一是GMM作为生成模型，自身就具备很强的解释性，二是这种参数学习以极大似然估计的方式，不再过分依赖梯度回传的更新机制，所以GMM的思想继续用在了Stacked Capsule Autoencoder里面。</p><blockquote><p>这里想说的是: 如果有人问，当前深度学习的核心理念是什么，我个人觉得，一个比较好的回答就是，学习目标形式化进而转换为参数的梯度学习。然而这样的同质研究越来越多，又越来越难以深入其内部，DL社区就开始自我反思。而Capsule的理念里面，就尝试去摆脱D中L对梯度回传的过分依赖，对卷积结构的过分依赖，所以Capsule Network本身将一些自编码器、重构、混合高斯、注意力等机制引入其中。</p></blockquote><p>其实读这篇论文会感觉作者用到的技术太多了，很容易忽略它背后的动机。我个人对它背后动机的理解为: 将图像中的实例的部件及属性从像素二维空间中以像素重建的方式抽取出出来；再用重构的方式解释部件与整体的关系。这也是SACE的两个主要构成环节。</p><p>接下来，本文希望通过简洁的语言来描述该论文的主要思路，所以跳过了论文对Toy Setup的描述，直接总结了SCAE的两个主要模块。</p><h2 id="stacked-capsule-autoencoders-scae">3. Stacked Capsule Autoencoders (SCAE)</h2><h3 id="scae-pcae-ocae">SCAE = PCAE +OCAE</h3><p>(Part Capsule Autoencoder + Object Capsule Autoencoder)</p><p>以这幅官方给出的这幅示意图为例</p><figure><img src="/2020/02/11/stacked-capsule-autoencoders/scae.jpg" alt="SCAE"><figcaption>SCAE</figcaption></figure><h4 id="pace">PACE</h4><p><strong>目标</strong>：将 <span class="math inline">\(h\times w \times c\)</span> 的图像，编码成 <span class="math inline">\(M\)</span> 个part capsules，每个part capsule能够对应图像中的一种部件part的所有属性，用多个属性构成的一个向量 <span class="math inline">\(X \in R^{6+1+z_m}\)</span> 来表示一个part实例，这个实例属性中包括6维的姿态，1维的存在概率和 <span class="math inline">\(z_m\)</span> 维的自身独特性特征。</p><p><strong>编码方法</strong>：采用CNN+Attention Pooling 方式:</p><p><span class="math display">\[h\times w\times c \Rightarrow M \times X \]</span></p><p><strong>解码方法</strong>：可学习的<span class="math inline">\(M\)</span>个 <span class="math inline">\(11\times 11\)</span>大小的模板+仿射变换（6维姿态导出）</p><p><strong>学习训练方法</strong>：用可学习的模板，进行高斯混合来重构原始图像+所有位置上的像素数据的极大似然估计</p><h4 id="oace">OACE</h4><p><strong>目标</strong>：把 part capsules当成 <span class="math inline">\(M\)</span> 个 <span class="math inline">\(X\)</span> 构成的集合，使用 Set Transformer学习集合中元素与元素之间的成对关系以及高阶的关系，来预测 <span class="math inline">\(K\)</span> 个object capsules，每个object capsule能够对应图像中的一个物体的所有属性，用多个属性构成的一个向量 <span class="math inline">\(Y \in R^{9+1+z_k}\)</span> 来表示一个part实例，这个实例属性中包括9维的object-viewer-matrix，1维的存在概率和 <span class="math inline">\(z_K\)</span> 维的自身独特性特征。然后对每个object capsule 使用MLP解码出，每个object capsule与所有part capsule之间的隶属关系，使用高斯混合模型来建模，即part capsule m的姿态 <span class="math inline">\(x_m\)</span>的可以看成是多个object capsule的贡献的高斯混合, 那么第 <span class="math inline">\(k\)</span> 个object capsule 对第 <span class="math inline">\(m\)</span> 个part capsule的贡献为 <span class="math inline">\(p(x_m|k,m)\)</span></p><p><strong>编码方法</strong>：Set Transformer 学习集合中各个元素的pair-wise关系以及高阶的关系，是permutation-invariant的模型。输出 <span class="math inline">\(K\)</span> 个object capsules实例</p><p><strong>解码方法</strong>：用 <span class="math inline">\(K\)</span> 个MLP预测高斯混合模型（详见论文中的公式）的候选投票，一个是方差，然后计算出高斯混合模型每个高斯成分的均值和方差，这样就可以计算出 <span class="math inline">\(p(x_m|k,m)\)</span>。</p><p><strong>学习训练方法</strong>：部件part-capsule及其属性，被物体object-capsule整体下的高斯混合来进行重构解释的极大似然估计</p><h2 id="aaai-2020-hinton发表了关于stacked-capsule-autoencoder的演讲">4. AAAI 2020 Hinton发表了关于Stacked Capsule AutoEncoder的演讲</h2><p>演讲地址在：<a href="https://www.bilibili.com/video/av88128940" target="_blank" rel="noopener">Geoffrey Hinton：Stacked Capsule Autoencoders(堆叠胶囊自编码器) (AAAI 2020)_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></p><p><strong>矩阵具备很好的同变性（equivariant）的性质，仿射矩阵可以发挥出出奇的作用。</strong></p><p>他还提到了一个很有意思的概念：Coincidence Fitering</p><p>他指出了现在的CNN的问题 并没有很好的利用--偶然性过滤，但是Transformer的注意力机制中体现出了偶然性过滤</p><p>我比较喜欢，这种用恰到好处的语言来描述提炼出一个抽象的解释性名词，既形象生动，又直指核心问题</p><h4 id="什么叫偶然性过滤">什么叫偶然性过滤？</h4><p>我个人的直觉理解是，CNN主要做的是卷积核的权重与激活值的矩阵乘法（或者说是向量的内积），那么由于输出目标监督是稀疏的（groudtruth往往是稀疏的，无论是分类还是回归，坐标还是热图），就会让中间的激活也倾向于是稀疏的，也就是说，大部分的权重矩阵与激活向量的乘法运算时对应元素的取值倾向，是无关的，或者说他们的对应取值是偶然的，这样的值是不显著的，容易会被不激活，但是问题是，这种稀疏激活模式，并不是发生在两个高维激活向量之间，而是可学习的权重和神经元的激活值之间，而真正的偶然性过滤需要发生在两个高维向量之间的某些对应元素上，比如两个10000维的向量在第2437位上具备相似的取值，而其他元素对应不存在什么关系，这种两个偶然性中出现的一致是不平凡的，那么其他平凡的偶然性就会被过滤掉。Transformer中的注意力机制就是高维向量间的内积，就会形成一种covariance structure，它会过滤掉大部分偶然的输入 (非目标输入)，只有契合该covariance structure的输入才能形成明显的激活！</p><p>(上述是我的直觉理解，描述不一定准确, 期待有更多的讨论和解读)</p><p><img src="/2020/02/11/stacked-capsule-autoencoders/coincidence-filtering.jpg" alt="SCAE"> &gt; [!NOTE] &gt;（~而且有时候，我们的实验结果和结论都有可能是“偶然的”）</p><p>演讲地址在：<a href="https://www.bilibili.com/video/av88128940" target="_blank" rel="noopener">Geoffrey Hinton：Stacked Capsule Autoencoders(堆叠胶囊自编码器) (AAAI 2020)_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a> 另外，我也整理出了关于 Dynamic Routing Between Capsules，Matrix capsule with EM routing 和 Stacked Capsule Autoencoders 的PPT。百度云盘地址：https://pan.baidu.com/s/1BCdJiNWGqD-SNao7Lmv4sw 提取码：e58t</p><p>Hinton联合了很多的学者在不断迭代着Capsule的概念和技术，不断地引入新鲜的东西到深度学习中。一方面我觉着我们需要相信大佬的直觉，另一方面我们也不能盲目地追捧，还是要尽量抛开作者光环，去探讨论文研究中是不是拥有着奇思妙想或者醍醐灌顶的东西存在。</p><section class="footnotes"><hr><ol><li id="fn1"><p><a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="noopener">Dynamic Routing Between Capsules</a><a href="#fnref1" class="footnote-back">↩</a></p></li><li id="fn2"><p><a href="https://openreview.net/pdf?id=HJWLfGWRb" target="_blank" rel="noopener">Matrix capsule with EM routing</a><a href="#fnref2" class="footnote-back">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;1. 引言&lt;/h2&gt;
&lt;p&gt;《stacked capsule autoencoders》使用无监督的方式达到了98.5%的MNIST分类准确率。 &lt;a href=&quot;https://arxiv.org/abs/1906.06818&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stacked Capsule Autoencoders&lt;/a&gt; 发表在 NeurIPS-2019，作者团队阵容豪华。可以说是官方capsule的第3个版本。前两个版本的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic Routing Between Capsules&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Matrix capsule with EM routing&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Capsule Network" scheme="http://senyang-ml.github.io/tags/Capsule-Network/"/>
    
      <category term="Unsupervised Learning" scheme="http://senyang-ml.github.io/tags/Unsupervised-Learning/"/>
    
      <category term="PPT" scheme="http://senyang-ml.github.io/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
    <link href="http://senyang-ml.github.io/2019/11/02/distribution-aware/"/>
    <id>http://senyang-ml.github.io/2019/11/02/distribution-aware/</id>
    <published>2019-11-02T03:52:16.000Z</published>
    <updated>2020-05-29T04:57:07.945Z</updated>
    
    <content type="html"><![CDATA[<p>在arxiv上看到了这篇<a href="https://arxiv.org/abs/1910.06278" target="_blank" rel="noopener">论文</a> ,个人认为这是一个很有意思的工作, 利用用heatmap上的最大值以及其对应位置m, 来估计真实高斯分布均值位置μ. 这样的量化误差（下采样导致的量化最小单位误差）能够得到最大程度上的减轻．</p><p>论文实验验证了该方法比经验上的估计方法更准确.（取峰值到次峰值的１／４偏移处的位置，这个估计其实也是近似符合高斯分布了）.</p><a id="more"></a><p><strong>公式６一阶导</strong></p><p><span class="math display">\[\left.\mathcal{D}^{\prime}(\boldsymbol{x})\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=\left.\frac{\partial \mathcal{P}^{T}}{\partial \boldsymbol{x}}\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=-\left.\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=0\]</span></p><p>那么<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{x})\)</span> 是一个和<span class="math inline">\(\boldsymbol{x}\)</span> 形状一样的向量, 然而在公式(7)对向量<span class="math inline">\(\boldsymbol{\mu}\)</span>泰勒展开:</p><p><strong>公式７,高斯分布均值<span class="math inline">\(\mu\)</span>处关于<span class="math inline">\(ｍ\)</span>位置的二阶泰勒展开</strong></p><p><span class="math display">\[\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{T} \mathcal{D}^{\prime \prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\]</span></p><p>中的第二项<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\)</span> 中的<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})\)</span> 是不是应该加上转置,才能得到标量? 即 <span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})^T(\boldsymbol{\mu}-\boldsymbol{m})\)</span></p><h2 id="推导">推导</h2><p>泰勒展开公式</p><p><span class="math display">\[\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{T} \mathcal{D}^{\prime \prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\]</span></p><p>代入<span class="math inline">\(P(\mu)\)</span>和<span class="math inline">\(P(m)\)</span>的高斯分布公式，即，将<span class="math inline">\(\mu,m\)</span>代入下面的式子，约掉常数项</p><p><span class="math display">\[\begin{aligned} \mathcal{P}(\boldsymbol{x}; \boldsymbol{\mu}, \Sigma)=\ln (\mathcal{G})=&amp;-\ln (2 \pi)-\frac{1}{2} \ln (|\Sigma|)\\ &amp;-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \end{aligned}\]</span></p><p>可以得到</p><p><span class="math display">\[\begin{aligned}0&amp;=-\frac{1}{2}(m-\mu)^{\top} \Sigma^{-1}(m-\mu) +D^{\prime}(m)^{\top}(\mu-m)+\frac{1}{2}(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top}(\mu-m)&amp;=(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top} &amp;=(\mu-m)^{\top} D^{\prime \prime}(m)\\-D^{\prime}(m)^{\top} D^{\prime \prime}(m)^{-1} &amp;=(\mu-m)^{\top} \\-D^{\prime\prime}(m)^{-\top} D^{\prime}(m) &amp;=\mu-m \\ \mu &amp;=m-D^{\prime \prime}(m)^{-\top} D^{\prime}(m) \end{aligned}\]</span></p><p><strong>因为<span class="math inline">\(D^{\prime \prime}(m)＝－ \Sigma^{-1}\)</span>，在论文中方差矩阵假设为对角阵(可逆)<span class="math inline">\(\Sigma=\left[\begin{array}{ll}{\sigma^{2}} &amp; {0} \\ {0} &amp; {\sigma^{2}}\end{array}\right]\)</span> (因为ｘｙ方向独立),　这意味着<span class="math inline">\(D^{\prime \prime}(m)=D^{\prime \prime}(m)^T\)</span>, 所以</strong></p><p><span class="math display">\[\begin{aligned}\mu &amp;=m-D^{\prime \prime}(m)^{-\top} D^{\prime}(m) \\ \mu&amp;=m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\end{aligned}\]</span></p><p>补充一个细节:</p><p>上面的推导, 在第三个等式约掉(<span class="math inline">\(\mu-m\)</span>)的条件是假设<span class="math inline">\(\mu\)</span>不等于<span class="math inline">\(m\)</span>, 所以下面的等式是更完备的推导: <span class="math display">\[\begin{aligned}0&amp;=-\frac{1}{2}(m-\mu)^{\top} \Sigma^{-1}(m-\mu) +D^{\prime}(m)^{\top}(\mu-m)+\frac{1}{2}(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top}(\mu-m)&amp;=(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m) \\-D^{\prime}(m)^{\top} D^{\prime \prime}(m)^{-1} (\mu-m)&amp;=(\mu-m)^{\top}(\mu-m)  \\  0 &amp;=\left[\mu-m+D^{\prime \prime}(m)^{-\top} D^{\prime}(m)\right] (\mu-m) \\  0 &amp;=[\mu-m+D^{\prime \prime}(m)^{-1} D^{\prime}(m)] (\mu-m)\end{aligned}\]</span></p><p>这个推导的建立在两个假设上面: (1) 下采样后得到的heatmap上面的取值, 被假设为服从真实关键点位置的高斯分布 (2) 二阶泰勒展开的近似</p><p>那么 <span class="math inline">\(\mu =m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\)</span> 也包含了<span class="math inline">\(\mu=m\)</span>的可能, 因为</p><p><span class="math inline">\(D^{\prime}(m)=0\Leftrightarrow m\)</span>在高斯分布的均值位置<span class="math inline">\(\Leftrightarrow \mu=m\)</span></p><p>所以 <span class="math inline">\(\mu =m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\)</span>是完备的</p><p>在ilovepose网站(赞!)上, 原作者也给出了关于公式的解释: <a href="http://www.ilovepose.cn/t/99" target="_blank" rel="noopener" class="uri">http://www.ilovepose.cn/t/99</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在arxiv上看到了这篇&lt;a href=&quot;https://arxiv.org/abs/1910.06278&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文&lt;/a&gt; ,个人认为这是一个很有意思的工作, 利用用heatmap上的最大值以及其对应位置m, 来估计真实高斯分布均值位置μ. 这样的量化误差（下采样导致的量化最小单位误差）能够得到最大程度上的减轻．&lt;/p&gt;
&lt;p&gt;论文实验验证了该方法比经验上的估计方法更准确.（取峰值到次峰值的１／４偏移处的位置，这个估计其实也是近似符合高斯分布了）.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Distribution" scheme="http://senyang-ml.github.io/tags/Distribution/"/>
    
  </entry>
  
  <entry>
    <title>Reproduce PersonLab (2)</title>
    <link href="http://senyang-ml.github.io/2019/09/02/personlab-2/"/>
    <id>http://senyang-ml.github.io/2019/09/02/personlab-2/</id>
    <published>2019-09-02T08:17:42.000Z</published>
    <updated>2020-05-24T05:43:37.750Z</updated>
    
    <content type="html"><![CDATA[<p>Bilinear interpolation kernel + Hough voting + Greedy Algorithm</p><p>个人认为： - PersonLab中最给人启发的是：构造 geometric embedding: short-range offsets, mid-range offsets and long-range offsets 几何信息来表示人体姿态, 以此监督神经网络的预测，并根据预测结果，施以贪婪算法，关联出所有人的姿态和分割区域。</p><ul><li>PersonLab中最值得琢磨的数学部分是：如何将表示keypoints大致位置的heatmaps和short-offsets maps通过双线性插值核，然后进行Hough Vote得到精确位置的Hough Score Maps的过程。</li></ul><a id="more"></a><p>后者是我复现过程中遇到的一个难题。其实构造Hough Score maps在Google的上一篇论文 <code>G-RMI</code> 中就出现了，然而因为G-RMI没有开源的代码，所以我对其提到的双线性插值核函数<span class="math inline">\(G(\cdot)\)</span>的理解仍然还是很模糊。另外，如果按照论文公式给出的Hough Voting公式，去遍历图像的每一个像素位置进行计算的话，计算的时间复杂度会很高，所以，作者一定有什么巧妙的方式可以处理它，可是PersonLab也没有开源。</p><p>后来我发现，github上有一个<a href="https://github.com/octiapp/KerasPersonLab" target="_blank" rel="noopener">KerasPersonLab</a>的实现，论文的所有细节都几乎实现了，所以来看看他的关于构造Hough Score Maps的。</p><blockquote><p>不禁佩服这个项目的开发者，为什么他们就可以有这个能力把论文中没有提到的细节完美地实现出来，这一定是他们的功底比我强很多，或者他们知道了一些我不知道的先验知识。对两人的了解后，我发现他们是<a href="http://octi.tv/" target="_blank" rel="noopener"><code>Octi</code></a>公司的两位算法工程师, 我的猜测是开发APP需要将PersonLab的算法移植到手机上面，所以激发了两个人完成了实现，PersonLab的ArXiv发布时间是2018年三月底，这个项目的发布时间是2018年5月份，真的是厉害了。GitHub上其他的Personlab实现版本都是在此之后，不少repo借鉴了他们的这个项目。</p></blockquote><p>关于构造Hough Score Map 有这样的一段代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accumulate_votes</span><span class="params">(votes, shape)</span>:</span></span><br><span class="line">    xs = votes[:,<span class="number">0</span>]</span><br><span class="line">    ys = votes[:,<span class="number">1</span>]</span><br><span class="line">    ps = votes[:,<span class="number">2</span>]</span><br><span class="line">    tl = [np.floor(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    tr = [np.floor(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    bl = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    br = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    dx = xs - tl[<span class="number">1</span>]</span><br><span class="line">    dy = ys - tl[<span class="number">0</span>]</span><br><span class="line">    tl_vals = ps*(<span class="number">1.</span>-dx)*(<span class="number">1.</span>-dy)</span><br><span class="line">    tr_vals = ps*dx*(<span class="number">1.</span>-dy)</span><br><span class="line">    bl_vals = ps*dy*(<span class="number">1.</span>-dx)</span><br><span class="line">    br_vals = ps*dy*dx</span><br><span class="line">    data = np.concatenate([tl_vals, tr_vals, bl_vals, br_vals])</span><br><span class="line">    I = np.concatenate([tl[<span class="number">0</span>], tr[<span class="number">0</span>], bl[<span class="number">0</span>], br[<span class="number">0</span>]])</span><br><span class="line">    J = np.concatenate([tl[<span class="number">1</span>], tr[<span class="number">1</span>], bl[<span class="number">1</span>], br[<span class="number">1</span>]])</span><br><span class="line">    good_inds = np.logical_and(I &gt;= <span class="number">0</span>, I &lt; shape[<span class="number">0</span>])</span><br><span class="line">    good_inds = np.logical_and(good_inds, np.logical_and(J &gt;= <span class="number">0</span>, J &lt; shape[<span class="number">1</span>]))</span><br><span class="line">    heatmap = np.asarray(coo_matrix( (data[good_inds], (I[good_inds],J[good_inds])), shape=shape ).todense())</span><br><span class="line">    <span class="keyword">return</span> heatmap</span><br></pre></td></tr></table></figure></p><p>当我刚看到这些的时候是无法理解的，我认为这里面有着论文Hough voting最核心的部分。我想Accumulate——vote的出现一定和Hough Transform有一定的关联，另外，为什么会出现<code>np.ceil</code>和<code>np.floor</code>?这个<code>tl_vals = ps*(1.-dx)*(1.-dy)</code>又是什么鬼？似乎和双线性有着一点点的联系。我要慢慢摸清楚，顺藤摸瓜。</p><h2 id="hough-transform">Hough Transform</h2><p>回顾一下最经典的Hough transform 算法，那就以Hough Line Transform 为例子。OpenCV关于<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a>，有一个比较好的<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">解释文档</a>.</p><p>我想简洁地概括一下Hough Line Transform：</p><p>在二维数据空间坐标中，线其上的点，<span class="math inline">\((x,y)\)</span>的满足 <span class="math inline">\(y = mx+c\)</span>，一条直线就确定了一个唯一的参数坐标 <span class="math inline">\((m,c)\)</span></p><p>换一种视角去理解。</p><p>在二维参数空间坐标中，“线”其上的“点”，<span class="math inline">\((m,c)\)</span>满足于<span class="math inline">\(c=-xm+y\)</span>, 一条参数空间上的“线”就确定了二维平面中的唯一数据坐标<span class="math inline">\((x,y)\)</span> (参数空间上的一条“线”，对应了数据空间中某个点的直线簇)</p><p>这里的参数空间就是，hough space</p><h3 id="什么是hough-voting呢">什么是Hough Voting呢？</h3><p>举直线（可以进一步泛化，称之为模型A）的例子</p><p>我们可以把数据空间上的数据点，想象成是由其模型A--直线生成的，那么数据空间中的一个点，在直线模型的参数空间（hough Space）上，可能属于很多个直线（点的直线簇）。我们用一个2维数组来记录直线的参数，构造出Hough Space。比如<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a>给出的参数形式 <span class="math inline">\(\rho = x \cos \theta + y \sin \theta\)</span> 中的 <span class="math inline">\((\rho,\theta)\)</span> 来构造一个二维Hough Space, 然后我们需要选择一个量化等级，来控制其中一个参数增减的最小单位。这样的话，拿到一个点，我们就可以给其在Hough Space中可能属于的所有模型各投一次票。如果对数据空间中的所有数据点进行上述操作，并用Accumulator累积Hough Space各个模型的投票总数，我们就可以获得不同模型的得分，那么根据得分最大的模型对应的参数，就能够刻画出数据空间中数据点们共同依赖的那个模型！</p><p>根据这种思想我们可以将直线模型泛化到更一般的圆模型、椭圆甚至无规则的形状！！</p><p>这就是Hough Transform 和hough Voting的基本要义了。</p><p>然后我研究了一下<code>coo_matrix</code>函数，它能快速地构造一个稀疏矩阵，并且注意到，代码给出的格式满足： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">coo_matrix((data, (i, j)), [shape&#x3D;(M, N)])</span><br><span class="line">to construct from three arrays:</span><br><span class="line">    1. data[:]   the entries of the matrix, in any order</span><br><span class="line">    2. i[:]      the row indices of the matrix entries</span><br><span class="line">    3. j[:]      the column indices of the matrix entries</span><br><span class="line"></span><br><span class="line">Where &#96;&#96;A[i[k], j[k]] &#x3D; data[k]&#96;&#96;.  When shape is not</span><br><span class="line">specified, it is inferred from the index arrays</span><br></pre></td></tr></table></figure> 并且它具有累积功能，而就是它，完成了hough voting的功能</p><h3 id="bilinear-interpolation">bilinear interpolation</h3><p>再去分析一下，与np.ceil和np.floor作差以及连乘的意义 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">xs = votes[:,<span class="number">0</span>]</span><br><span class="line">ys = votes[:,<span class="number">1</span>]</span><br><span class="line">ps = votes[:,<span class="number">2</span>]</span><br><span class="line">tl = [np.floor(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">tr = [np.floor(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">bl = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">br = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">dx = xs - tl[<span class="number">1</span>]</span><br><span class="line">dy = ys - tl[<span class="number">0</span>]</span><br><span class="line">tl_vals = ps*(<span class="number">1.</span>-dx)*(<span class="number">1.</span>-dy)</span><br><span class="line">tr_vals = ps*dx*(<span class="number">1.</span>-dy)</span><br><span class="line">bl_vals = ps*dy*(<span class="number">1.</span>-dx)</span><br><span class="line">br_vals = ps*dy*dx</span><br></pre></td></tr></table></figure> 其功能相当于在进行双线性插值：</p><p>f(dx,dy)=f(0,0)(1-dx)(1-dy)+f(1,0)x(1-dy)+f(0,1)(1-dx)dy+f(1,1)dxdy</p><p>针对，每个位置上预测的<strong>小数点精度</strong>上（如果预测为整数，仅有<code>tl_vals</code>不为0）的偏量，以双线性的方式，并乘以预测置信度，累积到相邻的<strong>整数</strong>像素位置（这样可以不会牺牲任何量化上的精度），进而可以得到精确的预测位置，即反应在Hough Score Maps的极大值位置上</p><p>对于Hough Score Maps对应的每一个关键点类型，最后施以： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_heatmaps</span><span class="params">(kp_maps, short_offsets,radius, kpts_num)</span>:</span></span><br><span class="line">    heatmaps = []</span><br><span class="line">    map_shape = kp_maps.shape[:<span class="number">2</span>]</span><br><span class="line">    idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(kpts_num):</span><br><span class="line">        this_kp_map = kp_maps[:,:,i:i+<span class="number">1</span>]</span><br><span class="line">        votes = idx + short_offsets[:,:,i]</span><br><span class="line">        votes = np.reshape(np.concatenate([votes, this_kp_map], axis=<span class="number">-1</span>), (<span class="number">-1</span>, <span class="number">3</span>))</span><br><span class="line">        heatmaps.append(accumulate_votes(votes, shape=map_shape) / (np.pi*radius**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.stack(heatmaps, axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure> 即得到了Hough Score Maps.</p><h2 id="greedy-decoding-和-assign-pixels-to-person-skeleton-instance">Greedy Decoding 和 Assign pixels to person skeleton instance</h2><ul><li>PersonLab中巧妙的算法是：利用树结构，通过<code>Greedy Decoding</code>将偏移向量连通成独立的人体骨架；利用mask中像素与keypoints的偏移向量，以聚类的方式形成人体intance mask。</li></ul><p>关于<code>Greedy Decoding</code>部分，可以参考<a href="https://senyang-ml.github.io/2019/07/17/pifpaf/#Fast-greedy-decoding">我的另一篇博客</a>。</p><p>关于长向量<code>long-range-offset</code>的预测部分,从直观上看，可以发现，偏移量向内指向人体的关节，可以形成了论文提到的<code>basins of attraction</code>. 体现了一种区域分割的思想。</p><p>更多请参考如下：</p><ul><li><a href="https://github.com/octiapp/KerasPersonLab" target="_blank" rel="noopener">KerasPersonLab</a></li><li><a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a></li><li><a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">PersonLab</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bilinear interpolation kernel + Hough voting + Greedy Algorithm&lt;/p&gt;
&lt;p&gt;个人认为： - PersonLab中最给人启发的是：构造 geometric embedding: short-range offsets, mid-range offsets and long-range offsets 几何信息来表示人体姿态, 以此监督神经网络的预测，并根据预测结果，施以贪婪算法，关联出所有人的姿态和分割区域。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PersonLab中最值得琢磨的数学部分是：如何将表示keypoints大致位置的heatmaps和short-offsets maps通过双线性插值核，然后进行Hough Vote得到精确位置的Hough Score Maps的过程。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Algorithm" scheme="http://senyang-ml.github.io/tags/Algorithm/"/>
    
      <category term="Hough Voting" scheme="http://senyang-ml.github.io/tags/Hough-Voting/"/>
    
  </entry>
  
  <entry>
    <title>Pose Neural Fabrics Search</title>
    <link href="http://senyang-ml.github.io/2019/08/26/Pose-Neural-Fabrics-Search/"/>
    <id>http://senyang-ml.github.io/2019/08/26/Pose-Neural-Fabrics-Search/</id>
    <published>2019-08-26T08:23:53.000Z</published>
    <updated>2020-05-29T02:11:55.947Z</updated>
    
    <content type="html"><![CDATA[<p>Neural Architecture Search (NAS) for Human Pose Estimation.</p><center class="half"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/cell.jpg" width="50%"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/cell-based_fabric.jpg" width="40%"></center><p>Search part-specific Cell-based Neural Fabrics (CNFs) with the guide of prior knowledge of human body structure.</p><center class="half"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/pnfs_framework.jpg"></center><p><a href="https://arxiv.org/abs/1909.07068" target="_blank" rel="noopener">ArXiv</a> <a href="https://github.com/yangsenius/PoseNFS" target="_blank" rel="noopener">Code</a> <a href="https://yangsenius.github.io/PoseNFS/" target="_blank" rel="noopener">Project Page</a></p><a id="more"></a><h2 id="introduction">Introduction</h2><p>Neural Architecture Search (NAS), the process of learning the structure of neural network, can play a potential role at automatically designing network architectures. Current methods mainly take image classification as a basic task and only search for a micro cell to build a chain-like structure, thus the neural search space is still at the limit of a micro search space. However, when applying NAS to dense prediction tasks such as semantic segmentation and human pose estimation, the micro search space is no longer able to generate more complex architectures. Therefore, it become a necessity to artificially design the macro search space allowing identifying hierarchical structure upon cells for these tasks. In addition, existing works focus on discovering an alternative to the human-designed module in a common pipeline. Such practice actually decouples the automating architecture engineering from tasks, and is thus unable to take advantage of the domain knowledge of a specific task.</p><p>In this work, we study how to search neural architectures with the guide of prior knowledge for human pose estimation task and propose a framework named Pose Neural Fabrics Search. We notice that modern methods conducting human pose estimation based on deep CNNs, regardless of top-down or bottom-up pipeline, convert it into pixel-wise prediction problem; they usually focus on two aspects: <strong>neural architecture design</strong> and <strong>pose representation</strong>. Next, we will discuss our motivations from these two aspects. ...</p><p><a href="https://arxiv.org/pdf/1909.07068.pdf" target="_blank" rel="noopener">arXiv:1909.07068</a> <a href="2019-pose_neural_fabrics_search.pdf">newly updated</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Neural Architecture Search (NAS) for Human Pose Estimation.&lt;/p&gt;
&lt;center class=&quot;half&quot;&gt;
&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/cell.jpg&quot; width=&quot;50%&quot;&gt;&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/cell-based_fabric.jpg&quot; width=&quot;40%&quot;&gt;
&lt;/center&gt;
&lt;p&gt;Search part-specific Cell-based Neural Fabrics (CNFs) with the guide of prior knowledge of human body structure.&lt;/p&gt;
&lt;center class=&quot;half&quot;&gt;
&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/pnfs_framework.jpg&quot;&gt;
&lt;/center&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.07068&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ArXiv&lt;/a&gt; &lt;a href=&quot;https://github.com/yangsenius/PoseNFS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt; &lt;a href=&quot;https://yangsenius.github.io/PoseNFS/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Neural Architecture Search" scheme="http://senyang-ml.github.io/tags/Neural-Architecture-Search/"/>
    
  </entry>
  
  <entry>
    <title>Reproduce PersonLab (1)</title>
    <link href="http://senyang-ml.github.io/2019/08/19/Reproduce-PersonLab/"/>
    <id>http://senyang-ml.github.io/2019/08/19/Reproduce-PersonLab/</id>
    <published>2019-08-19T08:57:32.000Z</published>
    <updated>2020-03-13T11:06:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="personlab复现过程">PersonLab复现过程</h1><blockquote><p>我打算记录整个复现的心路历程,和代码实践遇到的问题 正在写完这一行字的我, 什么代码都没写~ 所以以下的文字是一篇<code>记叙文</code>, 默认的叙述手法是<code>顺序</code> <a id="more"></a></p></blockquote><p>选个复现思路: - 自上而下: 从<code>train.py</code>的<code>main()</code>,逐渐构建所需要的函数以及其子函数 - 自下而上: 从每个最小的函数(如从读入数据集,预处理图像,构建标签)开始,逐渐向上封装函数</p><p>我倾向于后者:</p><p>我先想清楚了整个项目需要哪些模块, 然后创建了如下的几个空文件:</p><p>backbone_network.py, label_constrction.py, data_augmentation.py , data_iteration.py, loss.py, evaluate.py, greedy_decoding.py, instance_association.py, model.py , train.py</p><p>空文件先创建好,刺激你去复现,后面遇到问题再改</p><h1 id="从label_construction.py-开始">从<code>label_construction.py</code> 开始</h1><p>PersonLab 最精华的部分应该是<strong>如何利用COCO给定的标签信息, 利用人类的直觉和知识重新加工成几何上的监督信息</strong>, 即<code>heatmaps</code>,<code>short-range offset</code>,<code>hough_socre_maps</code>,<code>mid-range pairwise offset</code>, <code>long-range offset</code>,<code>persons_mask</code>. 所以此部分也是复现的关键步骤.</p><p><strong>对了</strong>, 有一点需要强调, <strong>复现前要仔细阅读论文的实验部分(Experiment)的描述.我读了一些关键的描述语言, 后面如果涉及到再详细说明</strong></p><p><strong>继续</strong></p><p><code>coco keypoint detection task</code> 数据集提供的标签格式是<code>.json</code>, 举个例子,<code>person_keypoints_val2017.josn</code>其中包含<code>info</code>和<code>annotation</code>,</p><ul><li><code>info</code>负责提供数据集中每张图像的<code>image_id</code>,<code>file_name</code>,<code>height</code>,<code>width</code></li><li><code>annotation</code>负责提供标注信息,其中的多个样本可能来自于同一张图像(多人问题嘛),每个样本包括如下: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;segmentation&quot;: [[76,46.53,省略,31.03,99,省略,46.03]],</span><br><span class="line">&quot;num_keypoints&quot;: 15,</span><br><span class="line">&quot;area&quot;: 2404.375,</span><br><span class="line">&quot;iscrowd&quot;: 0,</span><br><span class="line">&quot;keypoints&quot;: [102,50,1,0,0,0,101,46,2,0,0,0,97,46,2,82,44,2,91,49,2,97,43,2,109,66,2,112,43,2,128,73,2,71,74,2,76,79,2,94,65,2,110,81,2,84,90,2,129,99,2],&quot;image_id&quot;: 149770,</span><br><span class="line">&quot;bbox&quot;: [65,31.03,81,77.5],</span><br><span class="line">&quot;category_id&quot;: 1,</span><br><span class="line">&quot;id&quot;: 427983&#125;</span><br></pre></td></tr></table></figure> 我们的目标就是找到每张图像中所有样本,并且主要根据他们的<code>keypoints</code>,<code>segmentation</code>的坐标信息来构造上面提到的监督信号,<code>bbox</code>的坐标其实就不需要了.</li></ul><p><strong>以下论文提到的两点请注意</strong>:</p><p>在利用<code>segmentation</code>的时候,需要注意,作者提到了关于处理特殊情况的操作: &gt; we back-propagate across the full image, only excluding areas that contain people that have not been fully annotated with keypoints (person crowd areas and small scale person segments in the COCO dataset)</p><p>这意味着我们要考虑<code>iscrowd==1</code>的情况,我们在进行loss计算时,要把<code>iscrowd==1</code> 的区域mask掉.</p><p>此外, 在论文的<code>Imputing missing keypoint annotations</code>章节,作者说明: &gt;The standard COCO dataset does not contain keypoint annotations in the training set for the small person instances, and ignores them during model evaluation.However, it contains segmentation annotations and evaluates mask predictions for those small instances. Since training our geometric embeddings requires keypoint annotations for training, we have run the single-person pose estimator of [G-RMI] (trained on COCO data alone) in the COCO training set on image crops around the ground truth box annotations of those small person instances to impute those missing keypoint annotations.</p><p>因为暂时不打算用另外一个模型预测小尺寸图像的keypoints,这里我们直接忽略掉小尺寸的instance segmentation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_coco_annotations</span><span class="params">(ann_dir,img_dir,mode=<span class="string">'val'</span>)</span>:</span></span><br><span class="line">    ann_file = os.path.join(ann_dir,<span class="string">'person_keypoints_&#123;&#125;2017.json'</span>.format(mode))</span><br><span class="line">    coco = COCO(ann_file)</span><br><span class="line">    image_id_list = coco.getImgIds()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img_id <span class="keyword">in</span> image_id_list:</span><br><span class="line"></span><br><span class="line">        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))</span><br><span class="line">        <span class="keyword">if</span> len(anns)==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        file_name = coco.imgs[img_id][<span class="string">'file_name'</span>]</span><br><span class="line">        file_path = os.path.join(img_dir,mode+<span class="string">'2017'</span>,file_name)</span><br><span class="line">        img = cv2.imread(file_path)</span><br><span class="line">        h, w, c = img.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment">#crowd_mask = np.zeros((h,w),dtype='bool')</span></span><br><span class="line">        instance_masks = []</span><br><span class="line">        keypoints_skeletons = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ann <span class="keyword">in</span> anns:</span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'area'</span>] ==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mask = coco.annToMask(ann)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'iscrowd'</span>] ==<span class="number">1</span>:</span><br><span class="line">                <span class="comment"># IGNORE CROWD IN PAPAER: TODO      </span></span><br><span class="line">                <span class="keyword">continue</span>       </span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'num_keypoints'</span>] ==<span class="number">0</span>:</span><br><span class="line">                <span class="comment"># IGNORE: TODO   </span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            keypoints_skeletons.append(ann[<span class="string">'keypoints'</span>])</span><br><span class="line">            instance_masks.append(mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> img, keypoints_skeletons, instance_masks</span><br><span class="line">    </span><br><span class="line">ann_dir = <span class="string">'/data/dataset/coco/annotations/'</span></span><br><span class="line">img_dir = <span class="string">'/data/dataset/coco/images/'</span></span><br><span class="line"></span><br><span class="line">c = get_coco_annotations(ann_dir,img_dir)</span><br><span class="line"></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><pre><code>&lt;generator object get_coco_annotations at 0x0000024759D61518&gt;</code></pre><p>上面是构造了一个生成器, 迭代产生(img, keypoints_skeletons, instance_masks) 这样的元组.</p><p>接下来要做的事,就是如何把最原始的标签重新加工编码成一个更加几何化的监督表示.</p><p>我们要明确一点, 论文中构造的偏移向量的表示都是在原图像尺寸的量化精度下的, 而不是神经网络直接输出的feature map的尺寸, 因为network的输出已经被降采样了, 所以不论是预测出的<code>heatmap</code>,还是<code>offsets</code>都被上采样,来还原到原图的尺寸精度上.</p><p>所以我们编码用来监督的表示时,需要参考原图的高度和宽度.</p><h2 id="根据关键点的位置构造以关键点位置为中心半径为r的圆形disk区域区域内的取值为1区域外的取值为0">根据关键点的位置构造以关键点位置为中心，半径为r的圆形disk区域，区域内的取值为1，区域外的取值为0</h2><p>最直接的做法是，遍历heatmap的每个位置，通过计算距离来判断是否落在某个关键点的半径范围内，但这样的时间复杂度为O(H*W) 而通过数学的直观角度，大部分的区域都是disk外的，有没有更快捷的方法，这里我采用的是：</p><p><code>引入高斯核的技巧，即在每个关键点的位置生成高斯分布的函数，其分布满足中心对称，那么通过设定阈值（半径处取值），大于阈值设置为1，小于阈值设置为0</code></p><blockquote><p>高斯核的技巧，避免了heatmap上所有位置的遍历，此处我用了opencv带的cv2.GaussianBlur()函数，这个实际上也是个滤波器，也包含遍历，但其复杂度为<code>O(H*W)</code>但其函数通过openc库实现。（是否需要考证一下这种方法的计算速度？毕竟滤波器遍历了整个heatmap。有没有更直接的指定位置插入高斯核的现成的函数？）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">disk_mask_heatmap</span><span class="params">(one_hot_heatmap,radius)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> radius % <span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        radius = radius - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># odd number for kernel size</span></span><br><span class="line">    heatmap = cv2.GaussianBlur(one_hot_heatmap, ksize= (radius,radius), sigmaX=<span class="number">1</span>, sigmaY=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_threshold_value</span><span class="params">(ksize)</span>:</span></span><br><span class="line">        <span class="comment"># In order to get a circle(disk ) mask, </span></span><br><span class="line">        <span class="comment"># we need to find a threshold value `t` in Gaussian kernel size, v=1 if v&gt;t else 0 </span></span><br><span class="line">        <span class="comment"># sample value in position: circle center + radius//2  </span></span><br><span class="line">        <span class="comment"># to get a approximate disk of radius</span></span><br><span class="line">        square = np.zeros(shape=(ksize[<span class="number">0</span>]*<span class="number">2</span>,ksize[<span class="number">1</span>]*<span class="number">2</span>))</span><br><span class="line">        center = square.shape[<span class="number">0</span>]//<span class="number">2</span>, square.shape[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line">        square[center[<span class="number">0</span>],center[<span class="number">1</span>]]=<span class="number">1</span></span><br><span class="line">        gaussian = cv2.GaussianBlur(square, ksize, sigmaX=<span class="number">1</span>, sigmaY=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        circle_border_value = gaussian[center[<span class="number">0</span>],center[<span class="number">1</span>]+ksize[<span class="number">1</span>]//<span class="number">2</span>] <span class="comment"># border of circle</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> circle_border_value</span><br><span class="line"></span><br><span class="line">    threshold = get_threshold_value(ksize=(radius,radius))</span><br><span class="line">    heatmap = (heatmap &gt;= threshold)</span><br><span class="line">    <span class="keyword">return</span> heatmap</span><br></pre></td></tr></table></figure><p>上述方法可以解决了生成disk的难题，但是接下考虑构造short-range offset时，必须找到在关键点周围半径内的位置计算偏移。这种功能要求代码，必须给定一个关键点位置，就可以获得其周围对应的位置。这个需求也就是：<code>在产生指定圆的时候，同时记录其圆内所有像素的位置和像素相对于圆心的偏移。</code></p><p>但是上面的代码不是逐个处理关键点的方法，而是一次性生成的，所以不能够实现上面的需求</p><p>获取heatmap上位置索引的技巧，比如，给定一个HxW大小的heatmaps</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">H,W =<span class="number">3</span>,<span class="number">3</span> </span><br><span class="line">map_shape = (H, W) </span><br><span class="line">idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)) </span><br><span class="line">print(idx.shape)</span><br><span class="line">print(idx)</span><br><span class="line">print(idx[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]<span class="number">-2</span>)</span><br></pre></td></tr></table></figure><pre><code>(3, 3, 2)[[[0 0]  [1 0]  [2 0]] [[0 1]  [1 1]  [2 1]] [[0 2]  [1 2]  [2 2]]]-1</code></pre><p>这就可以得到了heatmap每个位置的地址索引. 继续考虑如何构造disk区域。 ## 获取每个关键点影响周围的disk区域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_keypoint_discs</span><span class="params">(all_keypoints,map_shape,K=<span class="number">17</span>,radius=<span class="number">4</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    discs = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(all_keypoints))]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        </span><br><span class="line">        centers = [keypoints[i,:<span class="number">2</span>] <span class="keyword">for</span> keypoints <span class="keyword">in</span> all_keypoints <span class="keyword">if</span> keypoints[i,<span class="number">2</span>] &gt; <span class="number">0</span>]</span><br><span class="line">        dists = np.zeros(map_shape+(len(centers),))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">            dists[:,:,k] = np.sqrt(np.square(center-idx).sum(axis=<span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">if</span> len(centers) &gt; <span class="number">0</span>:</span><br><span class="line">            inst_id = dists.argmin(axis=<span class="number">-1</span>)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(all_keypoints)):</span><br><span class="line">            <span class="keyword">if</span> all_keypoints[j][i,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                discs[j].append(np.logical_and(inst_id==count, dists[:,:,count]&lt;=radius))</span><br><span class="line">                count +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                discs[j].append(np.array([]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># discs.shape N*K*[indices of the specified keypoint]</span></span><br><span class="line">    discs = np.array(discs)</span><br><span class="line">    <span class="keyword">return</span> discs</span><br></pre></td></tr></table></figure><p>返回的discs，包含K个heatmap，其中每个heatmap对应一种类型的人体关键点的所有人体的位置，每个位置的周围disk内的所有像素的位置的索引。进而达到了功能的需求。</p><p>进而我们可以设计这样的函数,根据所有人体的所有关键点的位置集合,</p><ul><li>获取每个关键点位置周围的disk区域,进行赋值,得到heatmaps</li><li>获取每个关键点位置周围的disk区域内每个像素的位置索引,与中心位置在x,y方向上作差,得到short offsets</li><li>获取起始关键点位置周围的disk区域内每个像素的位置索引,用终点关键点的位置于这些像素位置的索引作差，得到mid-range offsets</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_keypoint_discs</span><span class="params">(all_keypoints,map_shape,kpts_num=<span class="number">17</span>,radius=<span class="number">4</span>)</span>:</span></span><br><span class="line">print(all_keypoints)</span><br><span class="line"></span><br><span class="line">idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">discs = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(all_keypoints))]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"></span><br><span class="line">centers = [keypoints[i,:<span class="number">2</span>] <span class="keyword">for</span> keypoints <span class="keyword">in</span> all_keypoints <span class="keyword">if</span> keypoints[i,<span class="number">2</span>] &gt; <span class="number">0</span>]</span><br><span class="line">dists = np.zeros(map_shape+(len(centers),))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">dists[:,:,k] = np.sqrt(np.square(center-idx).sum(axis=<span class="number">-1</span>))</span><br><span class="line"><span class="keyword">if</span> len(centers) &gt; <span class="number">0</span>:</span><br><span class="line">inst_id = dists.argmin(axis=<span class="number">-1</span>)</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(all_keypoints)):</span><br><span class="line"><span class="keyword">if</span> all_keypoints[j][i,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">discs[j].append(np.logical_and(inst_id==count, dists[:,:,count]&lt;=radius))</span><br><span class="line">count +=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">discs[j].append(np.array([]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line">discs = np.array(discs)</span><br><span class="line"><span class="keyword">return</span> discs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kpts_maps</span><span class="params">(keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line">kpts_maps = np.zeros(map_shape+(kpts_num,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"><span class="comment">#print(keypoints_skeletons[n,k,2])</span></span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,k,<span class="number">2</span>] &gt; <span class="number">0.</span>:</span><br><span class="line"></span><br><span class="line">disk_indices = discs[n][k]</span><br><span class="line">kpts_maps[disk_indices,k] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> kpts_maps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">short_offsets</span><span class="params">(keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line"><span class="comment">#disk_mask_map = np.zeros(shape=map_shape+(kpts_num,),dtype='bool')</span></span><br><span class="line">short_offsets = np.zeros(map_shape+(kpts_num,<span class="number">2</span>,))</span><br><span class="line"><span class="comment">#map_shape = (H, W)</span></span><br><span class="line"><span class="comment"># [H,W,2]  for each pixel's (x,y) position index</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,k,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">disk_indices = discs[n][k]</span><br><span class="line"></span><br><span class="line">kpt_x = keypoints_skeletons[n][k,<span class="number">0</span>]</span><br><span class="line">kpt_y = keypoints_skeletons[n][k,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">short_offsets[disk_indices,k,<span class="number">0</span>] = pixels_indices[disk_indices,<span class="number">0</span>] - kpt_x</span><br><span class="line">short_offsets[disk_indices,k,<span class="number">1</span>] = pixels_indices[disk_indices,<span class="number">1</span>] - kpt_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> short_offsets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mid_range_offsets</span><span class="params">(pair_wise_kpts,keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">pair_wisd_kpts: for COCO, edges of tree structures of body is `kpts_num-1`</span></span><br><span class="line"><span class="string">such as [[kpt_shoulder_r,kpt_ankle_r],[kpt_shoulder_r,nose],..,[]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">discs.shape Number of people*kpts_num*[disk indices of the specified keypoint]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">return [H,W,2*(kpts_num-1),2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">directed_edges = []</span><br><span class="line"><span class="keyword">for</span> edge <span class="keyword">in</span> pair_wise_kpts:</span><br><span class="line">directed_edges.append(edge)</span><br><span class="line"><span class="comment">#directed_edges.append(edge[::-1])</span></span><br><span class="line"></span><br><span class="line">mid_offsets = np.zeros(map_shape+(len(directed_edges),<span class="number">2</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [H,W,2]  for each pixel's (x,y) position index</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> edge_id, direct_edge <span class="keyword">in</span> enumerate(directed_edges):</span><br><span class="line">begin_kpt_id, end_kpt_id = direct_edge</span><br><span class="line">begin_kpt = keypoints_skeletons[n][begin_kpt_id]</span><br><span class="line">end_kpt = keypoints_skeletons[n][end_kpt_id]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> begin_kpt[<span class="number">2</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> end_kpt[<span class="number">2</span>] &gt;<span class="number">0</span>:</span><br><span class="line">disk_indices = discs[n][begin_kpt_id]</span><br><span class="line"><span class="comment">#print(end_kpt[0],pixels_indices[disk_indices])</span></span><br><span class="line">mid_offsets[disk_indices,edge_id,<span class="number">0</span>] = end_kpt[<span class="number">0</span>] - pixels_indices[disk_indices,<span class="number">0</span>]</span><br><span class="line">mid_offsets[disk_indices,edge_id,<span class="number">1</span>] = end_kpt[<span class="number">1</span>] - pixels_indices[disk_indices,<span class="number">1</span>]</span><br><span class="line"><span class="comment">#print(mid_offsets[disk_indices,edge_id])</span></span><br><span class="line"><span class="keyword">return</span> mid_offsets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">()</span>:</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#keypoints_skeletons1 = np.array([[[28,28,1],[61,61,1],[155,155,1]],[[20,20,1],[55,55,1],[6,6,1]]])</span></span><br><span class="line">keypoints_skeletons = np.array(</span><br><span class="line">[</span><br><span class="line">[[<span class="number">28</span>,<span class="number">228</span>,<span class="number">1</span>],[<span class="number">161</span>,<span class="number">361</span>,<span class="number">1</span>],[<span class="number">55</span>,<span class="number">455</span>,<span class="number">1</span>],[<span class="number">64</span>,<span class="number">54</span>,<span class="number">1</span>],[<span class="number">368</span>,<span class="number">550</span>,<span class="number">1</span>]],</span><br><span class="line"><span class="comment">#[[20,120,1],[55,55,1],[6,6,1]]</span></span><br><span class="line">]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">pair_wise_kpts = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">print(<span class="string">"keypoints coordiantes:\n"</span>,keypoints_skeletons,<span class="string">"\npair wise keypoints:\n"</span>,pair_wise_kpts)</span><br><span class="line"></span><br><span class="line">map_shape = (<span class="number">600</span>,<span class="number">600</span>)</span><br><span class="line">kpts_num = <span class="number">5</span></span><br><span class="line">radius = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">discs = get_keypoint_discs(keypoints_skeletons,map_shape,kpts_num,radius)</span><br><span class="line"></span><br><span class="line">kpts_heatmaps = kpts_maps(keypoints_skeletons,discs,map_shape,kpts_num)</span><br><span class="line">visual_kpts_heatmaps = np.amax(kpts_heatmaps,axis=<span class="number">-1</span>)</span><br><span class="line">short = short_offsets(keypoints_skeletons,discs,map_shape,kpts_num)</span><br><span class="line"></span><br><span class="line">offsets_magnitude = np.sqrt(np.square(short).sum(axis=<span class="number">-1</span>))</span><br><span class="line">visual_offsets_magnitude=np.max(offsets_magnitude,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show mid_range_offset</span></span><br><span class="line">mid_offsets = mid_range_offsets(pair_wise_kpts,</span><br><span class="line">keypoints_skeletons,</span><br><span class="line">discs,</span><br><span class="line">map_shape,</span><br><span class="line">kpts_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mid_offsets_edge = mid_offsets[:,:,0::2,:] # directed edges</span></span><br><span class="line">mid_offsets_edge = mid_offsets   <span class="comment"># undirected edges</span></span><br><span class="line"></span><br><span class="line">mid_offsets_edge = mid_offsets_edge.astype(<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (h,w,2)</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># canvs</span></span><br><span class="line">background = np.zeros(map_shape+(<span class="number">3</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> edge_id, edge <span class="keyword">in</span> enumerate(pair_wise_kpts):</span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,edge[<span class="number">0</span>],<span class="number">2</span>]&gt;<span class="number">0</span> <span class="keyword">and</span> keypoints_skeletons[n,edge[<span class="number">1</span>],<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">begin_disk_indices = discs[n][edge[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x,y) <span class="keyword">in</span> pixels_indices[begin_disk_indices]:</span><br><span class="line"><span class="comment"># sparse disk setting for better visualization</span></span><br><span class="line"><span class="keyword">if</span> x %<span class="number">8</span>==<span class="number">0</span> <span class="keyword">and</span> y%<span class="number">8</span> ==<span class="number">0</span>:</span><br><span class="line"><span class="comment">#continue</span></span><br><span class="line">begin_kpt = (x,y)</span><br><span class="line">                        <span class="comment"># note: mid_offsets_edge[y,x,edge_id,0] not: mid_offsets_edge[x,y,edge_id,0]</span></span><br><span class="line">end_kpt = (x + mid_offsets_edge[y,x,edge_id,<span class="number">0</span>], y + mid_offsets_edge[y,x,edge[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">cv2.line(background,begin_kpt,end_kpt,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),thickness=<span class="number">2</span>)</span><br><span class="line">                        </span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">plt.imshow(visual_kpts_heatmaps )</span><br><span class="line">plt.title(<span class="string">"heatmaps of keypoints"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">plt.imshow(visual_offsets_magnitude)</span><br><span class="line">plt.title(<span class="string">"the magnitude of short offsets"</span>)</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">plt.imshow(background)</span><br><span class="line">plt.title(<span class="string">"mid_range_offsets of keypoints"</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'visualization.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">visualize()</span><br></pre></td></tr></table></figure><pre><code>keypoints coordiantes: [[[ 28 228   1]  [161 361   1]  [ 55 455   1]  [ 64  54   1]  [368 550   1]]] pair wise keypoints: [[0, 1], [1, 2], [2, 4], [3, 4]][[[ 28 228   1]  [161 361   1]  [ 55 455   1]  [ 64  54   1]  [368 550   1]]]Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</code></pre><p><img src="/2019/08/19/Reproduce-PersonLab/output_9_2.png" alt="visualization"> <!-- <img src="/2019/08/19/Reproduce-PersonLab/output_9_2.png" class="" title="[visualization]"> --&gt;</p><p>经过了很多的BUG和debug的过程，终于可视化出了自己期望的效果</p><p>会发现，我们可以得到了和论文Ｇ－ＲＭＩ和 PersonLab 中一样的disk的效果： short-offsets指向圆心，其模长的分布为：中心为０，向外靠近ｄｉｓｋ边缘时增大，边缘最大，然后陡然减为0 （是不是可以考虑优化一下？） mid-offsets: 从起点关键点的disk内的像素点指向，终点的关键点位置的向量场</p><p>待续 ...</p>--></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;personlab复现过程&quot;&gt;PersonLab复现过程&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;我打算记录整个复现的心路历程,和代码实践遇到的问题 正在写完这一行字的我, 什么代码都没写~ 所以以下的文字是一篇&lt;code&gt;记叙文&lt;/code&gt;, 默认的叙述手法是&lt;code&gt;顺序&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Reproduce" scheme="http://senyang-ml.github.io/tags/Reproduce/"/>
    
  </entry>
  
  <entry>
    <title>Chelsea Finn</title>
    <link href="http://senyang-ml.github.io/2019/07/21/Chelsea-Finn/"/>
    <id>http://senyang-ml.github.io/2019/07/21/Chelsea-Finn/</id>
    <published>2019-07-21T15:55:16.000Z</published>
    <updated>2020-03-13T11:00:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Chelsea Finn 这个人很厉害~</p><a id="more"></a><p>了解她的学术经历发现，她2018年毕业于 University of California, Berkeley, Berkeley CA，2014年开始读博，也就是4年博士毕业。</p><p>往前追溯，她从2010年到2014年在Massachusetts Institute ofTechnology, Cambridge MA读完本科，拿到学士学位，也就是说，她当初选择了直博，或者是硕博连读。</p><p>为什么说她厉害，主要是因为她17年在ICML上发表了<code>Model-Agnostic Meta-Learning for Fast Adaptation of DeepNetworks</code>（简称为<code>MAML</code>）。这篇论文之后在<code>meta-learning</code>相关领域很有影响力，其实就连最近流行起来的可导神经网络架构搜索<code>DARTS</code>的二阶近似也在一定程度上借鉴了这篇的元学习算法。你能想到这是她在博二或者（博三）时的发的？如果按照中国正常上学的年龄计算，发<code>MAML</code>时候的她应该是25岁吧。</p><p>附： MAML: https://arxiv.org/abs/1703.03400 (icmL 2017) Chelsea Finn: https://people.eecs.berkeley.edu/~cbfinn/</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Chelsea Finn 这个人很厉害~&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用torch.nn.DataParallel进行多GPU训练的一个BUG，已解决</title>
    <link href="http://senyang-ml.github.io/2019/07/20/pytorch-multigpu/"/>
    <id>http://senyang-ml.github.io/2019/07/20/pytorch-multigpu/</id>
    <published>2019-07-20T10:42:12.000Z</published>
    <updated>2020-03-13T10:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>解决了PyTorch 使用torch.nn.DataParallel 进行多GPU训练的一个BUG:</p><p><strong>模型(参数)和数据不在相同设备上</strong> <a id="more"></a> 我使用<code>torch.nn.DataParallel</code>进行多GPU训练时出现了一个BUG，困扰了我许久：</p><p><code>RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</code></p><p>这个错误表明, <code>input</code>数据在<code>device 1</code> 上, 而模型的参数在<code>device 0</code> 上 (暗示数据是被拆分到了各个GPU上,但是BUG出现位置的此处参数可能没有成功复制到其他GPU上, 或者说, 还是调用了复制前的那个参数地址)</p><p>因为模型比较复杂，继承与调用太多，之前调试了好久, 也没有解决掉, 在Github上有一个issue和我的问题很像: https://github.com/pytorch/pytorch/issues/8637 但是我还是没有找到自己的bug在哪里.</p><h4 id="今天-我又准备再此尝试解决它">今天, 我又准备再此尝试解决它</h4><p>经过6个小时的<code>print调试法</code>以及后面关键的VScode的<code>Debug</code>功能, 我大功告成,找到了问题所在,原来</p><p>我的<code>A(nn.Module)</code>类的<code>forward</code> 前向计算函数里面, 有一处调用了一个该类的列表<code>self.cell_fabrics</code>, 其列表的元素是通过<code>self.cell_fabrics = [self.cell_1, self.cell_2,...,self.cell_n]</code> 来赋值的,其中每个<code>self.cell</code>也是<code>nn.Module</code>类</p><p>即用<code>self.cell_fabrics = [self.cell_0_0, self.cell_0_1, … , self.cell_3_0, self.cell_3_1,…, self.cell_5_0]</code> 这样的方式,将所有的<code>cell类</code>放到<code>A类</code>的一个<code>列表属性</code>中, 而当整个<code>A类</code>通过<code>torch.nn.DataParallel</code>被复制了一份放到设备<code>cuda:1</code>上以后, 且在预计在设备<code>cuda:1</code>上执行下面代码段时:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span></span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">layer</span> <span class="title">in</span> <span class="title">self</span>.<span class="title">cell_fabrics</span>:</span></span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> layer:</span><br><span class="line">           y = cell (x,)</span><br></pre></td></tr></table></figure><p>经验证，<code>x</code>是在设备<code>cuda:1</code> 上面, 但是 <code>cell</code> 中的参数却明显都在 <code>cuda:0</code>上</p><p>也就是说:</p><p><strong>此时 <code>self.cell_fabrics</code> 的列表中保存的各个对象 (<code>self.cell</code>) 的地址，还是指向在没有进行<code>torch.nn.DataParallel</code>之前的<code>nn.Module</code> 的那些<code>self.cell</code>, 而<code>nn.DataParallel</code>类的<code>nn.Module</code>的参数都默认存放在<code>device(type='cuda',index=0)</code>上 .</strong></p><p><code>torch.nn.DataParallel(model,device_ids=[range(len(gpus))])</code>的机制是, 将属于<code>nn.Module</code>类的<code>model</code>以及其广播的所有<code>nn.Module</code>子类的上的所有参数,复制成<code>len(gpus)</code>份,送到各个GPU上. 这种广播机制的范围是注册(registered)成为其属性的<code>nn.Module</code>子类, <strong>属性为列表list中的各个对象是不会被复制的, 所以其list中的对象还是存放在默认设备<code>device 0</code>上</strong></p><p>所以 在使用<code>torch.nn.DataParallel</code>进行多GPU训练的时候, 请注意：所有属于模型参数的模块以及其子模块必须以<code>nn.Module</code>的类型注册为模型的属性, 如果需要一个列表来批量存放子模块或者参数的话, 请采用<code>nn.ModuleList</code>或者<code>nn.ModuleDict</code>这样的继承了<code>nn.Module</code>的类来进行定义, 并且在<code>forward(self,)</code>前向传播的过程中，需要直接调用属于 <code>nn.Module</code>,<code>nn.ModuleList</code>或者<code>nn.ModuleDict</code> 这样的属性。</p><p>那么<code>torch.nn.DataParallel</code>将会正常地将模型参数准确复制到多个GPU上, 并根据数据的<code>batchsize</code>的大小平分成GPU的数量分别送到相应的GPU设备上,</p><p>然后运用<strong>多线程</strong>的方式, 同时对这些数据进行加工处理, 然后收集各个GPU上最终产生对模型的各参数的梯度, 最后汇总到一起更新原模型的参数!</p><p>参考: 1. https://github.com/pytorch/pytorch/issues/8637 2. https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;解决了PyTorch 使用torch.nn.DataParallel 进行多GPU训练的一个BUG:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型(参数)和数据不在相同设备上&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="PyTorch" scheme="http://senyang-ml.github.io/tags/PyTorch/"/>
    
      <category term="Multi-gpus" scheme="http://senyang-ml.github.io/tags/Multi-gpus/"/>
    
      <category term="Bug" scheme="http://senyang-ml.github.io/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title>G-RMI-&gt; PersonLab -&gt; PifPaf  -- Human Pose Estimation</title>
    <link href="http://senyang-ml.github.io/2019/07/17/pifpaf/"/>
    <id>http://senyang-ml.github.io/2019/07/17/pifpaf/</id>
    <published>2019-07-17T03:35:08.000Z</published>
    <updated>2020-03-13T10:58:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="g-rmi--personlab---pifpaf-composite-fields-for-human-pose-estimation---cvpr-2019-论文解读">G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读</h3><blockquote><p>博客地址：https://yangsenius.github.io/blog/2-pifpaf/</p></blockquote><p>arxiv地址: https://arxiv.org/abs/1903.06593 github地址: https://github.com/vita-epfl/openpifpaf</p><p>今年的CVPR19的论文最近已经在CVF Openaccess 网站上放出来了。 <a id="more"></a></p><p>还记得去年18CVPR论文出来的时候，我把所有有关的人体姿态估计的论文的题目和概要大致都看了，得出的一个浅显的结论就是：3D姿态估计、密集姿态估计要流行起来了。这是因为在去年CVPR18的论文中，出现了大量的3D有关的论文而少有2D姿态估计研究（比如在MPII, COCO keypoint数据集上的方法挺少，可能2d姿态的都去发了ECCV18）。</p><p>而今年19CVPR的姿态估计好像又呈现出一次小爆发</p><p>COCO数据集上的性能又来到了一次新高：似乎74mAP已经被突破了（HRNet, 0.770 mAP, ECSI, 0.746 mAP）。。。</p><p>各位研究者们，是不是感觉到了精度上、性能上的压力。。。深度调参还是方法革新，这是个问题.</p><p>众多论文中，我先阅读了这篇，OpenPIFPAF。 因为它奇怪的名字好像是茫茫论文海中出现的那个与众不同的一篇，吸引我去一探Ta的全貌与究竟</p><p>读完后，我觉得OpenPifpaf继承了几篇姿态估计论文的工作：</p><ul><li>openpose</li><li>G-RMI</li><li>PersonLAB (应该说大部分核心的想法来自于PersonLab)</li></ul><p>并致力于解决几个棘手的问题： - Bottom-up的多人姿态解析问题 - 自动驾驶中，图像中小尺寸人体的问题</p><p>其实很有必要介绍一下先前的工作</p><h2 id="g-rmi">G-RMI</h2><p>G-RMI 是google的一篇自上而下处理姿态估计问题的开篇</p><p>通过Faster-RCNN检测得到包含单个人体的bounding box，然后再进行单人姿态估计</p><figure><img src="https://img-blog.csdnimg.cn/20190322204206204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><p>本论文在预测<span class="math inline">\(K\)</span>个表示置信度的heatmaps之外，又引入了offset fields的方法，用<span class="math inline">\(2\times K\)</span>个heatmaps表示，即每个heatmap的位置上预测一个<span class="math inline">\(F_k(x_{i})=l_k-x_{i}\)</span>的位移偏量，用<span class="math inline">\(l_k\)</span>来表示真实位置，其中<span class="math inline">\(x_i,k \in \mathbb{Z}_+^2\)</span> <span class="math inline">\(i,k\)</span>表示位置索引和关键点类型。</p><p><span class="math display">\[h_{k}\left(x_{i}\right)=1 \text { if }\left\|x_{i}-l_{k}\right\| \leq R\]</span></p><p><span class="math display">\[F_k(x_{i})=l_k-x_{i}\]</span> After generating the heatmaps and offsets, we aggregate them to produce highly localized activation maps <span class="math inline">\(f_{k}\left(x_{i}\right)\)</span> as follows: <span class="math display">\[f_{k}\left(x_{i}\right)=\sum_{j} \frac{1}{\pi R^{2}} G\left(x_{j}+F_{k}\left(x_{j}\right)-x_{i}\right) h_{k}\left(x_{j}\right)\]</span></p><p>其中第三个公式中的<span class="math inline">\(G(\cdot)\)</span>论文中说它是双线性插值核，并用霍夫投票的形式。在今年19CVPR的openpifpaf论文，又再次利用这个公式，不过用一个高斯核来代替了<span class="math inline">\(G(\cdot)\)</span>函数，我从中推断出这是起到了平滑取值的作用，就像我们在构造产生grountruth heatmaps那样的做法。下面的<span class="math inline">\(\pi R^{2}​\)</span>是一个归一化，和高斯核那样类似。</p><p>注：这个<span class="math inline">\(G(\cdot)\)</span>函数其实是很多人理解这篇论文的绊脚石。实际上,这就是对上采样后的heatmaps再次进行一次平滑.</p><blockquote><p>A different approach addressing this issue would be to predict activation maps,as in [27], which allow for multiple predictions of the same keypoint. However, the size of the activation maps, and thus the localization precision, is limited by the size of the net’s output feature maps, which is a fraction of the input imagesize, due to the use of max-pooling with decimation.In order to address the above limitations, we adopt acombined classification and regression approach. For each spatial position, we first classify whether it is in the vicin-ity of each of the K keypoints or not (which we call a “heatmap”), then predict a 2-D local offset vector to get amore precise estimate of the corresponding keypoint loca-tion. Note that this approach is inspired by work on object detection, where a similar setup is used to predict bounding boxes, e.g. [14, 37]. Figure 2 illustrates these three output channels per keypoint. <img src="https://img-blog.csdnimg.cn/20190322204225926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote><p>训练loss是：</p><p><span class="math display">\[L(\theta)=\lambda_{h} L_{h}(\theta)+\lambda_{o} L_{o}(\theta) \]</span></p><p><span class="math inline">\(\lambda_{h}=4\)</span> and <span class="math inline">\(\lambda_{o}=1\)</span> is a scalar factor to balance.</p><p>We use a single ResNet model with two convolutional output heads. The output of the firshead passes through a sigmoid function to yield the heatmap probabilities <span class="math inline">\(h_{k}\left(x_{i}\right)\)</span> for each position <span class="math inline">\(x_{i}\)</span> and each keypoint <span class="math inline">\(k\)</span> . The training target <span class="math inline">\(\overline{h}_{k}\left(x_{i}\right)\)</span> is a map of zeros and ones, with <span class="math inline">\(\overline{h}_{k}\left(x_{i}\right)=1\)</span> if <span class="math inline">\(\left\|x_{i}-l_{k}\right\| \leq R\)</span> and 0 otherwise. The corresponding loss function <span class="math inline">\(L_{h}(\theta)\)</span> is the sum of logistic losses for each position and keypoint separately.</p><p><span class="math display">\[L_{o}(\theta)=\sum_{k=1 : K} \sum_{i :\left\|l_{k}-x_{i}\right\| \leq R} H\left(\left\|F_{k}\left(x_{i}\right)-\left(l_{k}-x_{i}\right)\right\|\right)\]</span></p><p>where <span class="math inline">\(H(u)\)</span> is the Huber robust loss, <span class="math inline">\(l_{k}\)</span> is the position of the <span class="math inline">\(k\)</span> -th keypoint, and we only compute the loss for positions <span class="math inline">\(x_{i}\)</span> within a disk of radius <span class="math inline">\(R\)</span> from each keypoint.</p><p>Huber robust loss的函数图像为：</p><figure><img src="https://img-blog.csdn.net/20151229152931179" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="g-rmi的开创行思路keypoints-location-disk-mask-logistic-classication-and-short-range-offset解决了下采样导致对量化误差问题">G-RMI的开创行思路:keypoints location disk mask logistic classication and short-range offset解决了下采样导致对量化误差问题!!</h2><p><strong>作者构造出<code>0,1</code>取值构成的<code>kepoints location  masks heatmaps</code>和<code>某关键点对应对heatmap的每个grid位置相对于其真实位置的偏移</code>的<code>分类</code>加<code>回归</code>预测方法!!</strong></p><p>这一点<code>PersonLab</code>和<code>PifPaf</code>都沿袭了这一思路</p><p>而<code>PersonLab</code>在此基础上,为了解决多人关联肢体的算法设计问题,又继续引入了<code>mid-range pairwise offset</code>来针对<code>instance association</code>这一问题, 可以说将将<code>G-RMI</code>的方法拓展到多人问题上.</p><h1 id="personlab">PersonLab</h1><p>PersonLab在构造监督标签和网络预测表示上面下了不少功夫。 公式总览：</p><p><span class="math inline">\(\textbf{K heatmaps: } \qquad\qquad\qquad\qquad\qquad p_{k}(x)=1 \text { if } x \in \mathcal{D}_{R}\left(y_{j, k}\right), \mathcal{D}_{R}(y)=\{x :\|x -y\| \leq R\}\)</span></p><p><span class="math inline">\(\textbf{K short-range 2-D offset fields: } \qquad S_{k}(x)=y_{j, k}-x\)</span></p><p><span class="math inline">\(\textbf{K Hough score maps: }\qquad \qquad \qquad h_{k}(x)=\frac{1}{\pi R^{2}} \sum_{i=1 : N} p_{k}\left(x_{i}\right) B\left(x_{i}+S_{k}\left(x_{i}\right)-x\right)\)</span></p><p><span class="math inline">\(\textbf{2(K-1) mid-range 2-D offset fields: }\quad M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}_{R}\left(y_{j, k}\right)\right], \text{from k-th to l-th}\)</span></p><p><span class="math inline">\(\textbf{K long-range 2-D offset fields: }\qquad \qquad L_{k}(x)=y_{j, k}-x\)</span></p><p><img src="https://img-blog.csdnimg.cn/20190714160727112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在Personlab中,<span class="math inline">\(h_k(x)\)</span>,即hough投票后对高精度score maps并不提供实例相关的信息, 而需要一种机制来<code>group together the keypoints belonging to each individual instance</code>. 因此, 作者继续构造了<code>mid-range pairwise offeset</code> 负责<code>connect the dots</code>.</p><p><code>mid-range pairwise offeset</code> 也是 2D的offset fields: <span class="math inline">\(M_{k, l}(x)\)</span> &gt; We compute 2<span class="math inline">\((K-1)\)</span> such offset fields, one for each directededge connecting pairs <span class="math inline">\((k, l)\)</span> of keypoints which are adjacent to each other in a tree-structured kinematic graph of the person, see Figs <img src="https://img-blog.csdnimg.cn/20190714162530980.png" alt="在这里插入图片描述"></p><p>这里的2<span class="math inline">\((K-1)=2\times16\)</span> 个向量场指的是 上图16种肢体连接的正反2种方向:<span class="math inline">\((k,l)和(l,k)\)</span>的偏移向量场 <span class="math inline">\(M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}_{R}\left(y_{j, k}\right)\right]\)</span> , 这个向量场还是在disk对半径范围内的.</p><p><code>Recurrent offset refinement</code>： 作者在预测一些大尺寸人体时,有时候mid-range pairwise offsets很长,精度可能不准,所以用了<code>Recurrent offset refinement</code>:</p><p><span class="math display">\[M_{k, l}(x) \leftarrow x^{\prime}+S_{l}\left(x^{\prime}\right), \text { where } x^{\prime}=M_{k, l}(x),\]</span> 来进一步提高精度,迭代2次上述对公式. 其中,<span class="math inline">\(S_{l}\left(x^{\prime}\right)\)</span>是short-range offset.</p><h3 id="fast-greedy-decoding">Fast greedy decoding</h3><p>有了所有人体的关键点的预测位置和每个关键点的<code>mid-range pairwise offset</code>, 接下来要做的就是 进行将属于同一个人体的关键点组合成一个实例的机制。</p><p><code>PersonLab</code> 构造出一个<code>优先级的队列</code>，根据Hough score maps <span class="math inline">\(h_{k}(x)\)</span>上的<code>局部最大值</code>的位置（这里强调局部最大值，是因为可能会有false positive 的位置）以及其score高于一定的阈值的（实验取0.01） 来<code>按得分大小顺序放入队列</code>，这些放入队列中的关键点应该被称为了<code>seed</code>点（pifpaf实际上没有解释清楚这个seed点），从最高响应值的seed点位置开始，以此不断找到其在<code>tree-structure</code>上的连接关键点。</p><p>在这个算法迭代的每一步中，如果发现当前的<code>seed</code>关键点落入了已经关联到先前某个人体<span class="math inline">\(j^{\prime},\)</span>的某个<span class="math inline">\(k\)</span>类型对seed关键点的 <span class="math inline">\(\mathcal{D}_{r}\left(y_{j^{\prime}, k}\right)\)</span> 半径内，那么就意味着，这个半径区域内有可能有两个同样类型的关键点，那么我们就可以开辟一个新的人体实例<span class="math inline">\(j\)</span>,作为另外一个人体的<span class="math inline">\(k\)</span>类型关键点作为<code>seed</code>. 其中,通过<code>seed</code>点计算与其<code>adjacent</code>的关键点的公式是:<span class="math inline">\(y_{j, l}=y_{j, k}+M_{k, l}\left(y_{j, k}\right)\)</span>.</p><p>这种机制对于所有关键点的是公平的,即 根据高得分的关键点位置作为起始seed,(我想起了Associative Embedding 对待起始点是从头部开始), 然而实际中容易检测的点往往是起始位置点, 这种方法能够一定程度处理遮挡问题.</p><blockquote><p>the position <span class="math inline">\(x_{i}\)</span> of the current candidate detection seed of type <span class="math inline">\(k\)</span> is within a disk <span class="math inline">\(\mathcal{D}_{r}\left(y_{j^{\prime}, k}\right)\)</span> of the corresponding keypoint of previously detected person instances <span class="math inline">\(j^{\prime},\)</span> then we reject it;</p></blockquote><p>Personlab的decoding 大致代码如下: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"> config.EDGES = [</span><br><span class="line">        (<span class="number">0</span>, <span class="number">14</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">13</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">4</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="number">14</span>, <span class="number">16</span>),</span><br><span class="line">        (<span class="number">13</span>, <span class="number">15</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">7</span>),</span><br><span class="line">        (<span class="number">10</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">8</span>),</span><br><span class="line">        (<span class="number">11</span>, <span class="number">12</span>),</span><br><span class="line">        (<span class="number">8</span>, <span class="number">9</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">6</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_skeletons</span><span class="params">(keypoints, mid_offsets)</span>:</span></span><br><span class="line">    <span class="comment"># keypoints 是hough score maps 所有局部最大值位置产生对所有候选关键点  </span></span><br><span class="line">    <span class="comment"># keypoints 数组每个元素为&#123;'xy':[x,y],'id':k,'conf':score&#125;</span></span><br><span class="line">    keypoints.sort(key=(<span class="keyword">lambda</span> kp: kp[<span class="string">'conf'</span>]), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 按照从大到小排序</span></span><br><span class="line">    skeletons = []</span><br><span class="line">    <span class="comment"># skeletons 表示多个人体骨架,每个元素是单人的骨架坐标集合</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># config.EDGES 表示16个单向的骨架连接边,dir_edges 构造出双向的连接边</span></span><br><span class="line">    dir_edges = config.EDGES + [edge[::<span class="number">-1</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> config.EDGES]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为每个关键点生成跟它相连的关键点的集合,比如 左肘部有[左肩膀,左手腕]</span></span><br><span class="line">    skeleton_graph = &#123;i:[] <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP)&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">            <span class="keyword">if</span> (i,j) <span class="keyword">in</span> config.EDGES <span class="keyword">or</span> (j,i) <span class="keyword">in</span> config.EDGES:</span><br><span class="line">                skeleton_graph[i].append(j)</span><br><span class="line">                skeleton_graph[j].append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优先级队列</span></span><br><span class="line">    <span class="keyword">while</span> len(keypoints) &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># pop出最大置信度的候选关键点</span></span><br><span class="line">        kp = keypoints.pop(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 判断该类型(根据id)关键点有没有落入了之前的某个骨架的同类关键点上</span></span><br><span class="line"><span class="comment"># 计算距离是否在r=10(论文里面提到)内</span></span><br><span class="line">        <span class="keyword">if</span> any([np.linalg.norm(kp[<span class="string">'xy'</span>]-s[kp[<span class="string">'id'</span>], :<span class="number">2</span>]) &lt;= <span class="number">10</span> <span class="keyword">for</span> s <span class="keyword">in</span> skeletons]):</span><br><span class="line">            <span class="comment"># 如果是,则抑制该关键点(非极大值抑制)</span></span><br><span class="line">            <span class="comment">#　如果否，则认为该类型关键点属于另外一个人体，那么就开辟新的实例</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment">#　构造新的骨架</span></span><br><span class="line">        this_skel = np.zeros((config.NUM_KP, <span class="number">3</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将该类型关键点的坐标和置信度赋值给该骨架</span></span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], :<span class="number">2</span>] = kp[<span class="string">'xy'</span>]</span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], <span class="number">2</span>] = kp[<span class="string">'conf'</span>]</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 此处 引入该函数</span></span><br><span class="line">    <span class="comment">###########################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterative_bfs</span><span class="params">(graph, start, path=[])</span>:</span></span><br><span class="line">    <span class="string">'''iterative breadth first search from start'''</span></span><br><span class="line">    <span class="comment"># 此处的graph是所有的每个关键点与它相连的关键点的构成的多叉树结构</span></span><br><span class="line">    <span class="comment"># start 表示关键点的类型</span></span><br><span class="line">    <span class="comment"># 构造队列</span></span><br><span class="line">    q=[(<span class="literal">None</span>,start)]</span><br><span class="line">    </span><br><span class="line">    visited = []</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        <span class="comment"># 拿出第一个元素v (关键点l（子点）,关键点k（父点）) 初始 关键点l=None</span></span><br><span class="line">        v=q.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 如果其对应的关键点k之前没访问过</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> v[<span class="number">1</span>] <span class="keyword">in</span> visited:</span><br><span class="line">            <span class="comment"># 记录访问</span></span><br><span class="line">            visited.append(v[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 记录路径</span></span><br><span class="line">            path=path+[v]</span><br><span class="line">            <span class="comment"># 将在图结构graph中,找到关键点k直接相连的所有子关键点k_1,k_2,...</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 放到队列中[(关键点l_1关键点k_1),(关键点l_2,关键点k_2),...]</span></span><br><span class="line">            q=q+[(v[<span class="number">1</span>], w) <span class="keyword">for</span> w <span class="keyword">in</span> graph[v[<span class="number">1</span>]]]</span><br><span class="line">        <span class="comment">#  然后子点变成父点，下一次迭代生出更多的子点</span></span><br><span class="line">        <span class="comment"># 循环,这样的话,不论初始点是什么,我们都可以找到一个包含完整人体树结构tree-structure的路径,</span></span><br><span class="line">        <span class="comment"># 包含所有 [(None,关键点k),(关键点k,关键点k_1),(关键点1_x,关键点k_2),...,(关键点16_x,关键点16)] 一共会有17个path, 然而就会多余出1个连接,就是初始连接.</span></span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">    <span class="comment">#假如是Lwrist 为起始点的话: path=[('Lwrist', 'Lelbow'), ('Lelbow', 'Lshoulder'), ('Lshoulder', 'nose'), ('Lshoulder', 'Lhip'), ('nose', 'Rshoulder'), ('nose', 'Reye'), ('nose', 'Leye'), ('Lhip', 'Lknee'), ('Rshoulder', 'Relbow'), ('Rshoulder', 'Rhip'), ('Reye', 'Rear'), ('Leye', 'Lear'), ('Lknee', 'Lankle'), ('Relbow', 'Rwrist'), ('Rhip', 'Rknee'), ('Rknee', 'Rankle')]</span></span><br><span class="line"><span class="comment">###########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处对skeleton_graph是每个关键点与它相连的关键点的图结构</span></span><br><span class="line">        path = iterative_bfs(skeleton_graph, kp[<span class="string">'id'</span>])[<span class="number">1</span>:] <span class="comment"># 此处1:开始,即去掉多余的连接</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> path:</span><br><span class="line">            <span class="comment"># 判断第一条边的起始关键点的置信度是不是为0, 因为已经this_skel[kp['id'], 2] = kp['conf'], 所以第一次肯定不为0;但是如果第一次之后没有找到新的关键点的话,那么以后的循环都要continue</span></span><br><span class="line">            <span class="keyword">if</span> this_skel[edge[<span class="number">0</span>],<span class="number">2</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># dir——edges 索引0-31</span></span><br><span class="line">            mid_idx = dir_edges.index(edge)</span><br><span class="line">            <span class="comment"># mid_offsets shape=[h,w,32x2] 输出feature map的每个位置</span></span><br><span class="line">            offsets = mid_offsets[:,:,<span class="number">2</span>*mid_idx:<span class="number">2</span>*mid_idx+<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 计算当前给定的关键点基于grid的位置</span></span><br><span class="line">            from_kp = tuple(np.round(this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>]).astype(<span class="string">'int32'</span>))</span><br><span class="line">            <span class="comment"># 计算当前关键点的位置，加上，此位置针对该类型有向连接的预测的偏移x,y向量，得到的候选位置，</span></span><br><span class="line">            <span class="comment"># 比如我们当前的关键点类型为左肘部，有向连接为（左肘，左手腕），那么根据该位置的mid-offset预测的左手腕的位置就知道了，但是这是根据mid-offset预测得到的位置，如果我们在hough score maps上也同样在该附近位置预测到了左手腕的位置，那么就说明mid-offset的预测也是合理的。</span></span><br><span class="line">            proposal = this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>] + offsets[from_kp[<span class="number">1</span>], from_kp[<span class="number">0</span>], :]</span><br><span class="line"><span class="comment"># 所以接下的matches，是在优先级队列中找到候选的所有左手腕（假设）的关键点，并记录其在优先级队列中的位置i。（找到最佳匹配点后，需要把它从队列中pop出）</span></span><br><span class="line">            matches = [(i, keypoints[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keypoints)) <span class="keyword">if</span> keypoints[i][<span class="string">'id'</span>] == edge[<span class="number">1</span>]] <span class="comment"># edge[1]表示有向线段的末端点（左手腕）</span></span><br><span class="line"><span class="comment"># 通过计算mid-offset预测出的有向线段末端点位置，与score maps 上该类型的位置的距离，我们可以得到很有可能的匹配关键点（&lt;32）</span></span><br><span class="line">            matches = [match <span class="keyword">for</span> match <span class="keyword">in</span> matches <span class="keyword">if</span> np.linalg.norm(proposal-match[<span class="number">1</span>][<span class="string">'xy'</span>]) &lt;= <span class="number">32</span>]</span><br><span class="line">            <span class="comment"># 找不到匹配点的话，就队列中的下一个关键点</span></span><br><span class="line">            <span class="keyword">if</span> len(matches) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 根据匹配到的该类型关键点与预测的距离进行排序，距离越小，排序靠前</span></span><br><span class="line">            matches.sort(key=<span class="keyword">lambda</span> m: np.linalg.norm(m[<span class="number">1</span>][<span class="string">'xy'</span>]-proposal))</span><br><span class="line"><span class="comment"># 排在最前面的关键点，即matches[0]，的坐标作为grid上的位置</span></span><br><span class="line">            to_kp = np.round(matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'xy'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 记录其confidence</span></span><br><span class="line">            to_kp_conf = matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'conf'</span>]</span><br><span class="line">            <span class="comment"># 根据其在队列中的位置，将其pop出来</span></span><br><span class="line">            keypoints.pop(matches[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 把该匹配到的点的位置记录到骨架该处有向线段的末端位置上</span></span><br><span class="line">            </span><br><span class="line">            this_skel[edge[<span class="number">1</span>],:<span class="number">2</span>] = to_kp</span><br><span class="line">            this_skel[edge[<span class="number">1</span>], <span class="number">2</span>] = to_kp_conf</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 此处我们进行path路径上的顺寻，进入下一个有向连接，</span></span><br><span class="line">            <span class="comment"># 这里的path = iterative_bfs(skeleton_graph, kp['id']) 函数是个非常巧妙的扩散方式，</span></span><br><span class="line">            <span class="comment">#它从骨架上的任意一点出发，按照固定的顺序散播到骨架上的所有16个有向连接上（上游点（父），下游点（子））。</span></span><br><span class="line">            <span class="comment">#那么根据起始的任意一种人体关键点，这一个算法就可以在优先级队列中将很有可能属于该人体的所有关键点的候选点group到该人体上。</span></span><br><span class="line"><span class="comment"># 但是,如果考虑到有一种枢纽的关键点，没有找到合适的点，那么不完整的人体骨架将会产生，</span></span><br><span class="line">            <span class="comment"># 但是在队列后面属于人体的关键点还会产生一个候选骨架，那么非极大值抑制skeleton就是必要的一项了</span></span><br><span class="line">        skeletons.append(this_skel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> skeletons</span><br><span class="line"></span><br><span class="line"> <span class="comment">## https://github.com/octiapp/KerasPersonLab/blob/master/post_proc.py</span></span><br><span class="line"> <span class="comment">## https://github.com/senyang-ml/OKS-NMS</span></span><br></pre></td></tr></table></figure> # OpenPIFPAF</p><p>在G-RMI、PersonLab的基础上，引入了PAF和PIF 复合结构，实际上具备显式含义的向量场。</p><p>即在图像每个location的像素位置，寄托更多的复合含义，编码具有直观含义的向量</p><p>PIF针对每一种类型的关键点，PAF针对每一种关联肢体（两个有关part的连接连线）</p><p>对于COCO，有17个关键点，19个连接（论文默认设置）</p><p><strong>PIF和PAF是训练Encoder网络用的监督标签，如何构造这两种标签，来指导监督Encoder网络训练，是本论文很关键的部分，后面的decoder 部分完全依赖于PIF和PAF的预测值。本文的PIF和PAF设计，可谓是将人工先验知识发挥到了极致！</strong></p><h2 id="pif">PIF</h2><p>PIF是个<span class="math inline">\(K\times H \times W \times 5\)</span>的结构， K表示关键点的数量，COCO为17个</p><p>They are composed of a scalar component for confidence, a vector component that points to the closest body part of the particular type and another scalar component for the size of the joint. More formally, at every output location spread <span class="math inline">\(b​\)</span> (details in Section 3.4<span class="math inline">\()​\)</span> and a scale <span class="math inline">\(\sigma​\)</span> and can be written as</p><p><span class="math display">\[\mathbf{p}^{i j}=\left\{p_{c}^{i j}, p_{x}^{i j}, p_{y}^{i j}, p_{b}^{i j}, p_{\sigma}^{i j}\right\}​\]</span></p><p>因为作者主要针对小尺寸人体图片，那么得到的置信度图 confidence map 是非常粗糙的，为了进一步地提升confidence map 的定位精度，作者使用偏量位移maps 来提升confidence map 的分辨率，得到一个<code>高分辨率的confidence map</code>（这个高分辨率的置信图发挥着<code>产生姿态种子点</code>和<code>评价候选点得分</code>的作用），如下公式： <span class="math display">\[f(x, y)=\sum_{i j} p_{c}^{i j} \mathcal{N}\left(x, y | p_{x}^{i j}, p_{y}^{i j}, p_{\sigma}^{i j}\right)\]</span> 这个公式我发现，很大程度上借鉴了G-RMI中的上述公式。用一个未归一化的高斯核，以及可学习的范围因子<span class="math inline">\(\sigma\)</span>来代替G-RMI中的双线性插值核以及归一化的分母，通过上述公式计算一个高分辨率的图（这里的高分辨率尺寸应该是原图尺寸，因为关键点的坐标标签真实值是基于原图的像素大小等级的）的响应值，我个人理解为是一种利用预测值的高斯上采样插值法（<span class="math inline">\(p_{x}^{i j}, p_{y}^{i j}\)</span>是预测出的小尺寸置信图每个位置<span class="math inline">\((i,j)\)</span>基于其自身grid位置<span class="math inline">\((i,j)\)</span>的偏移量，<span class="math inline">\(p_{\sigma}^{i j}\)</span>应该是高分辨图中每个位置的得分受到周围多大范围的预测值的影响，这部分应关注源码）。</p><p>这么做的缘故是，我认为是，<code>想保证不论在何种尺寸（量化等级下）都能克服量化误差的影响，因为heatmap是基于grid的，离散的取值，而真实的位置是不基于grid，并且是连续的位置，我通过预测真实位置与grid位置的偏移、以及grid上的置信度，就能进而获知真实的精确位置</code>。（我个人理解这样的好处就是，定位精度是float级别的，而不是int级别的，这个实际上在小尺寸的图像上是非常重要的一种策略。这种思想源自于G-RMI, 我认为这是一个解决量化误差问题的非常好的方式, 像SimpleBaseline,CPN运用取1/4偏移的方式,是一种人为的假设.）</p><figure><img src="https://img-blog.csdnimg.cn/20190704131509721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="paf">PAF</h2><p>PAF是个<span class="math inline">\(N\times H \times W \times 7\)</span>的结构， N表示关联肢体的数量，默认为19，</p><p>作者使用的bottom-up的方法，必然要解决：关联检测的关键点的位置形成隶属的人体的这一问题，就必须用一定的表示手段和策略来实现。</p><p>作者提出了PAF，来将关键点连接一起形成姿态。</p><p>在输出的每个位置，PAFs预测一个置信度、两个分别指向关联一起的两个part的向量、两个宽度。用下面来表示： <span class="math display">\[\mathbf{a}^{i j}=\left\{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right\}\]</span></p><p>作者接下来说了这样一句话，</p><blockquote><p>Both endpoints are localized with regressions that do not suffer from discretizations as they occur in grid- based methods. This helps to resolve joint locations of close-by persons precisely and to resolve them into distinct annotations。 我目前的理解是，两个端点定位的回归，不再受困于 grid-based方法中出现的离散化问题！这就帮助对于离得很近的关键点精确位置，并区分它们的标注。</p></blockquote><p>在COCO数据集，一共有19个连接关联两种类型的关键点。算法在每个feature map的位置，构造PAFs成分时，采用了两步：</p><p>首先，找到关联的两个关键点中最近的那一个的位置，来决定其向量成分中的一个。</p><p>然后，groundtruth pose决定了另外一个向量成分。第二个点不必是最近的，也可以是很远的。</p><blockquote><p>一开始我没有，怎么理解这么做的含义。后来意识到，这样就相当于，对于每一种类型的关联肢体，比如左肩膀和左屁股连接。对应的PAF中，每个位置都会优先确定理它最近的关键点的位置（考虑多个人体的情况下），然后指向另外一端的向量就自然得到了。</p></blockquote><p>并且在训练的时候，向量成分所指向的parts对必须是相关联的，每个向量的x，y方向必须指向同一个关键点的。</p><figure><img src="https://img-blog.csdnimg.cn/20190704131528767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="adaptive-regression-loss">Adaptive Regression Loss</h2><p>定位偏差可能对于大尺寸人体来讲，是小的影响，但是对于小尺寸人体，这个偏差就会成为主要的问题。本研究通过引入尺度依赖到<span class="math inline">\(L_1 - type​\)</span>的loss函数里，</p><h2 id="greedy-decoding">Greedy Decoding</h2><p>通过PIF和PAF来得到poses。这个快速贪心的算法过程和PersonLab中的相似。</p><p>一个姿态由一个种子点(高分辨率PIF的最高响应位置)开始，一旦一个关键点的估计完成，决策就是最终不变的了。（贪心）</p><p>A new pose is seeded by PIF vectors with the highest values in the high resolution confidence map <span class="math inline">\(f(x, y)\)</span> defined in equation <span class="math inline">\(1 .\)</span> Starting from a seed, connections to other joints are added with the help of PAF fields. The algorithm is fast and greedy. Once a connection to a new joint has been made, this decision is final.</p><p>Multiple PAF associations can form connections between the current and the next joint. Given the loca- tion of a starting joint <span class="math inline">\(\vec{x},\)</span> the scores <span class="math inline">\(s\)</span> of PAF associations a are calculated with</p><p><span class="math display">\[s(\mathbf{a}, \vec{x})=a_{c} \quad \exp \left(-\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\right) f_{2}\left(a_{x 2}, a_{y 2}\right)\]</span></p><p>这个<span class="math inline">\(s(\mathbf{a},\vec{x})\)</span>表示每个location属于part association的得分，得分越高，代表这个更有可能是part association区域部分那么,如果<span class="math inline">\(s(\mathbf{a},\vec{x})\)</span>越大,那么就期望<span class="math inline">\(a_c\)</span>越大,<span class="math inline">\(\left(-\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\right)\)</span>越大,<span class="math inline">\(\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\)</span>越小,那么就期望PAF某位置的<span class="math inline">\(\mathbf{a}\)</span> 对应的<span class="math inline">\(\mathbf{a}=\left\{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right\}\)</span>向量中, 其指向的端点1和当前种子点距离最近, 并且期望该位置指向的另外一个端点2的置信度响应高, 这些期望和该位置是属于这两个关键点(端点)连接肢体的期望是一致的. 一旦我们的初始种子点确立后,我们就可以根据预测的PAF找到其关联的肢体区域和另外一个关键点位置,作为下一次的寻找的种子点.然后,重复这个过程,直到该种子点对应的人体全部找到.(这实际运用了人体躯干的连通性的潜在知识). 作者提倒:</p><blockquote><p><code>To confirm the proposed position of the new joint, we run reverse matching. This process is repeated until a full pose is obtained. We apply non-maximum suppression at the keypoint level as in [34]. The suppression radius is dynamic and based on the predicted scale component ofthe PIF field. We do not refine any fields neither during training nor test time.</code></p></blockquote><p>这个设计是巧妙的,<strong>因为我们在构造PAF的时候,请注意到,<span class="math inline">\((a_{x1},a_{y1})​\)</span> 是PAF输出map的某位置<span class="math inline">\(\mathbf{a}\)</span>最近的关键点的位置（请看Figure 4b），以此来判断离该位置<span class="math inline">\(\mathbf{a}\)</span>最近的关键点是不是<span class="math inline">\(\vec{x}\)</span></strong>。如果当前<span class="math inline">\(\vec{x}​\)</span>和<span class="math inline">\((a_{x1},a_{y1})​\)</span>的距离就可以作为当前位置是不是指向<span class="math inline">\(\vec{x}​\)</span>的判断,因为如果两点重合的话,距离为0,指数取值为最大值1. 并且该位置对应的另外一个端点的取值具有高响应, 那么这就意味着:</p><p><strong><span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>的髙得分位置,很有可能处在指向<span class="math inline">\(\vec{x}\)</span>端点的肢体关联部分的区域！</strong></p><p><strong>换句话说：</strong></p><p><span class="math inline">\(PIF\)</span>是计算得到的<code>高分辨率置信度图</code>负责提供候选的关键点。<span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>得分公式，利用<span class="math inline">\(PAF\)</span>预测值计算在其输出feature map每一个位置的得分，来判断两种关键点之间的连接（如左肘部和左手腕），因为涉及到多人，（参考OpenPose，对于单个人体的单个肢体连接，只有一种连接是合理的），论文提到的To confirm the proposed position of the new joint, we run reverse matching，我认为就是来确定某人体的某个肢体连接的唯一性、合理性的手段，具体还是要看源码。</p><p><del>找到<span class="math inline">\((a_{x2},a_{y2})\)</span>的位置(通过髙响应<span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>)的位置?还是通过PIF,PAF的预测值得到?这个目前有待考证，我在后面会阅读实现源码,继续更新博客)</del>，<code>高分辨率置信度图</code>负责提供候选的关键点的位置。</p><p>那么，通过这样的一个贪心的快速算法, 我们根据初始的某个关键点就能同时确立多个人体位置,</p><!-- ## 占位符。。。。。。## 占位符。。。。。。 --><h4 id="思考">思考</h4><p>注：可以看出这一系列的论文（GRMI，PersonLab，Openpifpaf，part-based）相比与针对网络结构进行改进（SimpleBaseline，HRNET）的文章看，更加关注几何关系上的问题以及网络的输出表示形式。PersonLab，Openpifpaf面对更加有挑战性的BBOX-FREE方法，以及小尺寸，遮挡问题进行处理，确确实实能给人持续往下深入的启示和实际应用的潜力。针对改网络结构的文章，譬如HRNET，SEU-POSE，SIMPLEBASELINE，CPN等等，致力于寻找最有的卷积结构设计，而不怎么关注一些棘手的问题（用模型本身的能力来克服），为姿态估计行业引领性能的标准，并不断去探索神经网络结构可能发挥的极限。前者更适合去研究新方法，突破现有检测器约束的姿态估计框架，去挑战多人姿态估计的难题，后者给我们提供了，固有框架内可以进一步提升性能的很多实用的经验和技巧，让我们更加洞察神经网络的结构的特性，并充分利用神经网络结构设计的潜在能力。</p><p>哪个才能更好解决人体姿态估计问题的手段呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;g-rmi--personlab---pifpaf-composite-fields-for-human-pose-estimation---cvpr-2019-论文解读&quot;&gt;G-RMI-&amp;gt; PersonLab -&amp;gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;博客地址：https://yangsenius.github.io/blog/2-pifpaf/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;arxiv地址: https://arxiv.org/abs/1903.06593 github地址: https://github.com/vita-epfl/openpifpaf&lt;/p&gt;
&lt;p&gt;今年的CVPR19的论文最近已经在CVF Openaccess 网站上放出来了。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
  </entry>
  
  <entry>
    <title>Hello, 这是我的新博客</title>
    <link href="http://senyang-ml.github.io/2019/07/17/hello-world/"/>
    <id>http://senyang-ml.github.io/2019/07/17/hello-world/</id>
    <published>2019-07-17T03:28:25.000Z</published>
    <updated>2019-07-17T03:28:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>原来的 https://yangsenius.github.io 网站的所有源码完全是我手工设计的： - 整个网页css的风格来自于DeepMind (https://deepmind.com/) 网站的风格 (0.0) - 用 head.js, body.js, foot.js 制作了每个博客的模板 - html5文件是我用vscode从markdown文本格式转化过来的 - 然后将head.js,body.js,foot.js的文件放到每个html5的指定位置 - 手工设计的博客悄然完成</p><p>然而，网站的风格和博客模板有了，但是维护起来不是特别容易，因为随着博文的增多，分类和分页功能我自己还没开发出来, 另外每次自动修改html源代码的步骤还需要我手动重复操作</p><p>“工欲善其事必先利其器”，既然hexo (https://hexo.io/zh-cn/) 提供了自动设计博客的功能，能够解放我的机械式的博客加工过程，何乐而不为呢？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原来的 https://yangsenius.github.io 网站的所有源码完全是我手工设计的： - 整个网页css的风格来自于DeepMind (https://deepmind.com/) 网站的风格 (0.0) - 用 head.js, body.js, foot
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Rethinking Human Pose Estimation  重新思考人体姿态估计</title>
    <link href="http://senyang-ml.github.io/2019/07/13/rethinking_human_pose/"/>
    <id>http://senyang-ml.github.io/2019/07/13/rethinking_human_pose/</id>
    <published>2019-07-13T01:52:16.000Z</published>
    <updated>2020-03-13T10:58:45.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>更多内容在知乎文章: <a href="https://zhuanlan.zhihu.com/p/72561165" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72561165/</a></strong></p><p>浅谈：2D人体姿态估计基本任务、研究问题、意义、应用、研究趋势以及未来方向</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1000017/1emeto7o6r.gif"></p><a id="more"></a><h4 id="基本定义从单张rgb图像中精确地识别出多个人体的以及其骨架的的稀疏的关键点位置">1.基本定义：从单张RGB图像中，精确地识别出多个人体的以及其骨架的的稀疏的关键点位置。</h4><p><img src="https://tse3-mm.cn.bing.net/th?id=OIP.u3JSsrIZJMhgjdxWK443vwHaHZ&w=221&h=211&c=7&o=5&pid=1.7"> <img src="https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/media/keypoints_pose_18.png" width="20%"></p><h4 id="基本任务给定一张rgb图像定位图像中人体的关键点位置并确定其隶属的人体">2.基本任务：给定一张RGB图像，定位图像中人体的关键点位置，并确定其隶属的人体。</h4><blockquote><p>按照人的直观视觉理解的话，主要会涉及到以下问题： - 关键点及周围的局部特征是什么样的？ - 关键点之间、人体肢体的空间约束关系是什么样的，以及层级的人体部件关系是什么样的？ - 不同人体之间的交互关系是什么样的，人体与外界环境之间的交互关系是什么？</p></blockquote><blockquote><p>基于Deep CNN的方法的试图通过神经网络的拟合能力，建立一种隐式的预测模型来避开上述的显式问题： - 基于去显式分析人体姿态问题的方法是有的，传统的Pictorial Structure是其中一个较为经典的算法思路，目前也有少数方法用part-based的层级树结构建立人体姿态模型并利用CNN，来进行学习与预测。 - 当下多数深度CNN回归的方式, 试图用模型强大的拟合能力去回避以上的显式问题，而从大量的图像数据和标签监督信息中用神经网络去学习图像数据与构建的标签信息之间的映射。</p></blockquote><h4 id="当前主流研究的基础问题和难点">3.当前主流研究的基础问题和难点：</h4><ul><li><p>神经网络结构的设计是个永远（当下）都会伴随的问题（假如深度学习的热潮没有退去的话）</p></li><li><p>Top-down：先检测人体，再做单人姿态估计两阶段的方法。</p><ul><li><p>必然受到了目标检测任务的制约。</p></li><li><p>基于bounding box的单人姿态估计问题，在面对遮挡问题容易受到挫折。</p></li><li><p>精度虽然髙实时性能较差</p></li><li><p>小尺寸图像与计算资源限制</p></li></ul></li><li><p>Bottom-up：针对整副图像的多人关键点检测，检测所有关键点候选位置的同时，关联相关人体</p><ul><li>精度不如单人估计的更加精准，但实时性能较好</li><li>面对拥挤问题、遮挡问题仍然容易受到挫折</li><li>小尺寸图像问题</li></ul></li></ul><h4 id="方法分类">4.方法分类：</h4><ul><li><p>标准1：:Top-Down和Bottom-up的方法。</p></li><li><p>标准2：全局的长距离关系的隐式学习问题（大多数）和基于part的中短距离关系（ECCV-18 PersonLab，ECCV-18 Deeply learned compositional models）的学习问题</p></li><li><p>标准3：heatmap回归（大多数），直接坐标回归方法（CVPR-14-DeepPose，ECCV-18的Integral Pose），向量场嵌入（CVPR-17 G-RMI，ECCV-18 PersonLab，CVPR-19 PIFPAF）的方法等等</p></li></ul><h5 id="近几年的代表作">5.近几年的代表作</h5><ul><li>发迹于2014年, CVPR: Google的DeepPose，同年出现了MPII数据集（Max-Planck ）以及MS-COCO数据集。</li><li>16年: CVPR：CMU的Convolutional Pose Machine (CPM)和德国的马克斯普朗克研究所Deepcut以及Stacked Hourglass 网络结构设计的出现。</li><li>17年: CVPR：Google的G-RMI开启基于目标检测的人体姿态估计方法。CMU的OpenPose系统出现，致力于打造实时姿态估计系统。Deepcut的改进版DeeperCut出现。同年ICCV上，Mask RCNN、上海交通大学的RMPE以及随后的AlphaPose崭露头角, NeurIPS17也出现了 Associative Embedding 以新的端到端的方式来避免人体姿态估计多阶段不连续学习的问题。</li><li>18年：CVPR上出现了旷世的CPN拿下了17年COCO挑战赛的冠军, ECCV上微软亚洲研究院的SimpleBaseline用自上而下的方法为姿态估计打造最简单的baseline，并刷新了COCO数据集的新高。ECCV上还出现了来自中东技术大学的Muhammed Kocabas提出了MultiPoseNet，以及Google的自下而上多任务的新作PersonLab, 值得一提的是还有一些开辟新的研究角度的方法如ECCV上美国西北大学part-based的姿态估计方法Deeply learned compositional models 。18年的另外一个趋势就是，新问题新任务的出现，比如CVPR18的DensePose标志着密集关键点人体姿态估计任务的出现, 2D pose track 任务(CVPR18 PoseTrack数据集)的提出, 以及3D 姿态估计问题的兴起......</li><li>19年CVPR, 姿态估计再次呈现一个小爆发（HRNET，PIFPAF，Seu-ByteDance Pose ，Related Parts Help，Crowded Pose , Fast Human Pose，Pose2Reg等等），并且出现了大量新的方向探讨姿态估计问题, 以及 3D 姿态估计成为主流。 当然, 2D姿态估计任务仍然是值得去深入探讨的问题, 因为一些本质上的难题目前还没有完全的洞察和有效的解决方案, 比如严重遮挡,多人重叠问题等等。另外， 数据集MPII, COCO数据集上的&quot;刷性能&quot; 也依然是大家孜孜不倦的追求，性能再次来到了新高。</li></ul><h4 id="研究意义">6.研究意义：</h4><ul><li><p>3D人体姿态估计的铺垫、3维人体重建的必备技术</p></li><li><p>人体关键点的视频追踪问题的基础（从静态到动态）</p></li><li><p>动作识别的信息来源（从关键点的时序空间特征映射到动作语义问题）</p></li></ul><h4 id="应用">7.应用：</h4><ul><li>自动驾驶行业：自动驾驶道路街景中行人的检测以及姿态估计、动作预测等问题</li><li>娱乐产业：动作特效的增加。快手、抖音、微视等视频软件</li><li>安全领域：行人再识别问题，以及特殊场景的特定动作监控，婴儿、老人的照顾。</li><li>影视产业：拍电影特效（复仇者联盟）</li><li>人机交互：AR，VR，以及未来的人机交互方式</li></ul><h4 id="研究趋势的变化以及扩展">8.研究趋势的变化以及扩展：</h4><ul><li>稀疏关键点到密集关键点（CVPR-18 FaceBook DensePose）</li><li>静态图像到视频追踪 （CVPR-18 PoseTrack）</li><li>从关键点定位到肢体的分割预测 （pose parsing,CVPR-19 Pose2Reg）</li><li>从监督学习到弱监督 、半监督，甚至无监督有可能（如, ICLR2019 unsupervised discovery, parts, structure and dynamics）</li><li>当然：神经网络结构的设计（ECCV-18 SimpleBaseline，CVPR-18 CPN， CVPR-19 HRNet，CVPR-19 Enhanced Channel-wise and Spatial Information，ICCV FPN-POSE等等）是个永远都会伴随的问题（假如深度学习的热潮没有退去的话）</li></ul><h4 id="个人思考">个人思考</h4><p>当前所有的姿态估计方法几乎都使用了深度卷积神经网络的强大功能，但个人认为神经网络设计绝不是解决该问题的核心，用力搔靴和脱掉鞋子，哪个才是更好的止痒手段呢？</p><p>人体姿态估计是一个综合的问题，有很多的切入点和难题值得去研究，并且它是一个尚未实际落地的计算机视觉技术。在这个层面上，AI的产品经理们和投机者们应该想想这项技术怎么能更好地服务大众，并带来市场和利润。</p><p>作为科学研究者，赚钱的考虑或应该暂时放到明天。我想讨论的是: 当我们面对一项任务和难题， 我们是应该忽略固有的困难和问题，提出新的问题，给出问题方案，去探索新的研究趋势呢？还是强行深入当前的固有问题，解决当下的难题呢？ 是不是有一些的问题是超前式的，也许放到以后才会有更加合适的方案和角度来解决？</p><p>或者说，我们还可以用另一种粗暴的方案：把这一问题黑箱化或者半黑箱化，然后从神经网络结构设计、数据处理、增强以及其他机器学习数学方法去暴力式的解决。这样的解决方式实际上是，摒弃了人类本身做姿态估计的直观思路（上面所述），而是从更加“机器学习”的角度去处理这个问题。假如，我们寻找到一个“完美”结构的神经网络，让它去达到１００％或者近似１００％的准确率！这样以来，似乎预测问题被完完全全地解决了，但是问题是，我们不知道能不能找到这样的结构或者技术，或者说一旦找到了以后能不能解释性地理解这一技术? 这就又引出了大家探讨争论许久的可解释性问题、显式推理问题。其实今年CVPR19 的PifPaf的工作值得我们去思考，它继续引入复合场(Composite Field)的概念，预测人为得设计好的高维度向量来处理人体姿态预测问题，让模型预测更加巧妙的监督信息。该方法能降低量化误差，设计的关联肢体得分公式巧妙保持了期望的一致性，再加之一个快速贪心算法，利用人体的连通特性就能得到多人姿态。这样的设计与算法，尽管性能比那些注重网络结构设计的略差一些，但却遵循合理的直觉，并且可解释性强，启发性强，是不是需要我们更多的关注？</p><p>另外，今年ICLR2019上，有学者甚至提出了无监督的方式处理人体部件。 我认为这是一种可以去探讨的问题, 因为人体姿态本身其实可以看成图像中的特征簇, 其视觉上的连通特性本身就具备了高维特征上的独特性。那么靠聚类手段、生成模型、无监督学习在直觉上是可行的, 如果再加上视频，光流等辅助信息, 那么就可以从大量无标签的图像数据中, 准确构建人体部件的特征、部件到整体的结构特征以及人体姿态的运动时序特征, 这可能又会是一个新的思路和解决人体姿态估计任务的新手段吗？</p><blockquote><p>知乎文章: <a href="https://zhuanlan.zhihu.com/p/72561165" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72561165/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;更多内容在知乎文章: &lt;a href=&quot;https://zhuanlan.zhihu.com/p/72561165&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/72561165/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;浅谈：2D人体姿态估计基本任务、研究问题、意义、应用、研究趋势以及未来方向&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/yehe-1000017/1emeto7o6r.gif&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Object Detection" scheme="http://senyang-ml.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>MetaAnchor - Learning to Detect Objects with Customized Anchors - 2018 NeurIPS 解读</title>
    <link href="http://senyang-ml.github.io/2018/12/20/metaanchor/"/>
    <id>http://senyang-ml.github.io/2018/12/20/metaanchor/</id>
    <published>2018-12-20T01:52:16.000Z</published>
    <updated>2020-03-13T10:59:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors" target="_blank" rel="noopener">NeuIPS 2018</a> &gt; 原创博文 转载请注明<a href="https://yangsenius.github.io/blog/MetaAnchor/" target="_blank" rel="noopener" class="uri">https://yangsenius.github.io/blog/MetaAnchor/</a></p><p>一般目标检测方法中的Anchors的生成是来自于人类的先验知识:<span class="math inline">\(b_i\in \mathcal{B} \ which \ is \ predefined \ by \ human\)</span>（<span class="math inline">\(\mathcal{B}\)</span>属于 <span class="math inline">\({prior}\)</span> <span class="math inline">\(i\)</span>代表网格或锚点），即</p><ul><li>通过固定锚点，或者划分网格，生成一定形状和尺寸的Anchor Bboxes 来作为候选检测区域,提取对应位置的图像特征，</li></ul><p>先验往往代表设计人员在构思最初的朴素想法，来源于直觉，并把这种直觉融合在设计者的实现过程与代码中。 <a id="more"></a> 下面举两个例子。</p><h3 id="在faster-rcnn中">在Faster Rcnn中</h3><p>对输出的(W,H,d)维Conv map进行滑动遍历，每个滑窗输出一个特征向量WxH个d维的特征向量</p><p>根据根据感受野中心不变的原理，每个滑窗中心对应原图的anchor锚点或者说anchor bboxes的中心。</p><p>每个锚点映射到原图，实际上对应着来自3x3(3种特定的尺度x3个特定的形状)个的anchor boxes，我们认为这9个anchor bboxes经过特征提取得到的具有尺度不变性的特征向量，这些anchor bboxes意味着proposals。</p><p>然后作者使用先验规定：proposal与GTbbox iou大于某个阈值（0.7）认为是正样本，小于某个阈值（0.3）为负样本，其余的不参与训练！即给这些proposals做标签！</p><p>然后把这些正负样本送入RPN进行训练。</p><p>loss由regression和classification两个loss构成，即预测proposal的中心位置和宽高，以及proposal属于前景or背景</p><p>注意：这里的regression loss包含三个坐标：预测bbox、anchor bboxes、GT——bboxes,loss函数的目标是，缩小 [预测bbox与anchor bboxes相对偏移] 和[gt_bbox与anchor bboxes相对偏移]之间的差距！</p><p>经过RPN筛选后的Proposal的特征图的尺寸大小是不一致的，经过ROIPOOling得到特征维度一致的特征，使用与RPN共享卷积的Fast Rcnn进行进一步的分类和回归。</p><h3 id="在yolo中">在yolo中</h3><p>对任意输入尺寸的图像划分为<span class="math inline">\(s*s\)</span>个网格</p><p>每个网格预测B个bbox的4个位置和1个置信度 - (confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息,object落在一个grid cell里，第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值)</p><p>每个网格同时预测C个类的类别信息(每个网格属于的某类别的条件概率)</p><p>即对于一个输入图像，其输出的张量为 <span class="math inline">\(S*S*（B*5+C）\)</span></p><h2 id="在这里有必要说明这里anchor先验的含义即要把anchor的设计位置尺寸类比蕴含在anchor-function的设计中而不能成为一个独立的模块">在这里，有必要说明，这里“Anchor先验”的含义，即：要把anchor的设计（位置、尺寸、类比）蕴含在anchor function的设计中，而不能成为一个独立的模块</h2><h4 id="作者总结了一个较为一般的形式">作者总结了一个较为一般的形式：</h4><p><span class="math display">\[\mathcal{F}_{b_i}(\mathbf{x};\theta_i)=\left(\mathcal{F}^{cls}_{b_i}(\mathbf{x};\theta^{cls}_i),\mathcal{F}^{reg}_{b_i}(\mathbf{x}; \theta^{reg}_i)\right)\]</span></p><p>判断： 1. 每个候选区域的与真实bbox（如果有）的相对位置 <span class="math inline">\(\mathcal{F}^{reg}_{b_i}(\mathord{\cdot})\)</span> 2. 每个候选区域的类别置信概率 <span class="math inline">\(\mathcal{F}^{cls}_{b_i}(\mathord{\cdot})\)</span></p><p>本篇文章，作者使用的Anchor Function 是从先验的<span class="math inline">\(b_i\)</span>动态生成的,通过如下函数：</p><p><span class="math display">\[\mathcal{F}_{b_i}=\mathcal{G}\left(b_i; w \right)(2)\]</span></p><blockquote><p><span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> is called <span class="math inline">\({anchor \ function \ generator}\)</span> which maps any bounding box prior <span class="math inline">\(b_i\)</span> to the corresponding anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>; and <span class="math inline">\(w\)</span> represents the parameters. Note that in MetaAnchor the prior set <span class="math inline">\(\mathcal{B}\)</span> is not necessarily predefined; instead, it works as a  manner -- during inference, users could specify any anchor boxes, generate the corresponding anchor functions and use the latter to predict object boxes.</p></blockquote><p>上面是作者的原话，我觉得这个想法还是非常具有启发性的。我的理解是：</p><p>我们不是先盲目地生成大量的Anchor来判断是否抛弃，而是根据后面<strong>推理时</strong>的需要，在对应的位置生成特定的anchor boxes，然后生成anchor function来预测物体bbox，这样就避免了大量无关的候选框？这是我的理解，不知道对不对，接着读论文~</p><ul><li><p>“default boxes” , “priors” or “grid cells” 经常作为一个默认的方法。很多任务需要你在设计achor的大小、尺寸、位置时需要小心谨慎，不同数据集之间的物体bbox分布也会影响anchor的选择，但是MetaAnchor的方法就不用考虑这个问题。</p></li><li><p>受到 Learning to learn、few shot learning 、transfer learning的启发：有时候，我们的权重预测不是通过模型本身来学习，而是通过另一个结构（模型）来取预测权重，比如（Learning to learn by gradient descent by gradient descent，hypernetworks等），作者还拿自己的方法和learning to segment everything 作了对比，作者的权重预测是为了生成anchor function。</p></li></ul><p>仿佛，论文最关键的就是如何生成anchor function了，也就是这个函数了：</p><p><span class="math display">\[\mathcal{F}_{b_i}=\mathcal{G}\left(b_i; w \right)\]</span></p><p>下面详细讨论这个机制。</p><h2 id="anchor-function-generator">Anchor Function Generator</h2><blockquote><p>In MetaAnchor framework, <span class="math inline">\({anchor \ function}\)</span> is dynamically generated from the customized box prior (or anchor box) <span class="math inline">\(b_i\)</span> rather than fixed function associated with predefined anchor box. So, <span class="math inline">\({anchor \ function \ generator}\)</span> <span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> (see Equ.2), which maps <span class="math inline">\(b_i\)</span> to the corresponding anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>, plays a key role in the framework.</p></blockquote><p>作者强调了从<span class="math inline">\(b_i\)</span>映射到anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>, 这种映射关系是因为<span class="math inline">\(b_i\)</span>是带着一种随机性</p><blockquote><p>In order to model <span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> with neural work, inspired by <a href>HyperNetworks</a>,<a href>Learning to segment everything</a>, first we assume that for different <span class="math inline">\(b_i\)</span> anchor functions <span class="math inline">\(\mathcal{F}_{b_i}\)</span> share the same formulation <span class="math inline">\(\mathcal{F}(\mathord{\cdot})\)</span> but have different parameters, which means:</p></blockquote><p><span class="math display">\[\mathcal{F}_{b_i}(\mathbf{x}; \theta_i) = \mathcal{F}(\mathbf{x}; \theta_{b_i})\]</span></p><p>作者写这个公式，似乎想给出 无论怎样选择<span class="math inline">\(b_i\)</span> 的anchor function的一般形式。为什么这么做呢？下标的变换有什么意义吗？</p><p>我根据后面的内容，猜测：一般anchor function在设计时是要考虑 anchor<span class="math inline">\(b_i\)</span>的预定义方式，也就是我们要根据不同的anchor先验，具体设计出相对应的anchor function。如果我们anchor function的设计能够独立于anchor<span class="math inline">\(b_i\)</span>的预定义方式，让anchor<span class="math inline">\(b_i\)</span>的设计变成一个函数的可学习的参数形式，那么就把问题转化为一般的超参数学习，或者Meta-learning 的方式。之前我研究Learning to learn by gradient descent by gradient descent，作者就是让人工干预设计的优化方式，变成了可以学习的参数，二者虽然面对的问题的不一样，但是都包含了一个共同的思想：</p><p>让人工设计的先验知识，转化成，可以通过另一个结构或模型学习的，参数形式：</p><p><strong><span class="math display">\[人工先验知识 \rightarrow  可学习的参数形式\]</span></strong></p><p>这个思想和我上一篇<a href="https://blog.csdn.net/senius/article/details/84483329" target="_blank" rel="noopener">博客:learning to learn</a> 所涉及的方法，在理念上不谋而合</p><p>接着看论文。</p><p>论文说道： &gt; each anchor function is distinguished only by its parameters <span class="math inline">\(\theta_{b_i}\)</span>, anchor function generator could be formulated to predict <span class="math inline">\(\theta_{b_i}\)</span> as follows:</p><p><span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; w) \\= \theta^* + \mathcal{R}(b_i; w)\]</span></p><p>就是说，每个anchor function 通过参数 <span class="math inline">\(\theta_{b_i}\)</span> 来唯一确定(我的理解应该没错)，其中<span class="math inline">\(\theta^*\)</span>代表共享参数（独立于<span class="math inline">\({b_i}\)</span>，并且可以学习），残差项<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>依赖于 anchor bbox <span class="math inline">\({b_i}\)</span></p><p>然后<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>使用一个简单的两层全连接网络来表示：</p><p><span class="math display">\[\mathcal{R}(b_i, w) = \mathrm{W}_2 \sigma \left( \mathrm{W}_1 b_i \right)\]</span></p><p>作者还考虑把图像特征引入到参数 <span class="math inline">\(\theta_{b_i}\)</span>的学习中：</p><p><span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; \mathbf{x}, w) \\    = \theta^* + \mathrm{W}_2 \sigma \left(    \mathrm{W}_{11} b_i + \mathrm{W}_{12} r(\mathbf{x})    \right)\]</span></p><p><span class="math inline">\(r(\mathord{\cdot})\)</span> 用来给 <span class="math inline">\(\mathbf{x}\)</span>降维;</p><p>以上就是论文的理论思想了！</p><figure><img src="https://img-blog.csdnimg.cn/2018121317400085.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="具体实施细节结合retinanet代码让我们来感受什么是prior什么是meta">具体实施细节，结合RetinaNet代码，让我们来感受什么是“Prior”？什么是“Meta”</h2><blockquote><p>作者没有公布自己的源码是一件令人头疼的事情，这样就不知道，作者是如何把可学习的参数<span class="math inline">\(\theta_{b_i}\)</span>如何融进anchor function，不过我后面会试图写一下。</p></blockquote><p>作者说，这个方法更实用于one-stage的检测方法如 RetinaNet，yolo等，two-stage方法精度似乎受到第二阶段（anchor 不再发挥作用）的学习的影响更大。</p><p>作者主要说明了MetaAnchor在RetinaNet上的使用，先来看看什么是RetianNet，放上一段简介的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RetinaNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    num_anchors = <span class="number">9</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">20</span>)</span>:</span></span><br><span class="line">        super(RetinaNet, self).__init__()</span><br><span class="line">        self.fpn = FPN50()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.reg_head = self._make_head(self.num_anchors*<span class="number">4</span>)</span><br><span class="line">        self.cls_head = self._make_head(self.num_anchors*self.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        fms = self.fpn(x)</span><br><span class="line">        reg_preds = []</span><br><span class="line">        cls_preds = []</span><br><span class="line">        <span class="keyword">for</span> fm <span class="keyword">in</span> fms:</span><br><span class="line">            loc_pred = self.loc_head(fm)</span><br><span class="line">            cls_pred = self.cls_head(fm)</span><br><span class="line">            loc_pred = loc_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,<span class="number">4</span>)                 <span class="comment"># [N, 9*4,H,W] -&gt; [N,H,W, 9*4] -&gt; [N,H*W*9, 4]</span></span><br><span class="line">            cls_pred = cls_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,self.num_classes)  <span class="comment"># [N,9*20,H,W] -&gt; [N,H,W,9*20] -&gt; [N,H*W*9,20]</span></span><br><span class="line">            loc_preds.append(loc_pred)</span><br><span class="line">            cls_preds.append(cls_pred)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(loc_preds,<span class="number">1</span>), torch.cat(cls_preds,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_head</span><span class="params">(self, out_planes)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            layers.append(nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">256</span>, out_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><blockquote><p>注： 以上代码来自于<a href="https://github.com/kuangliu/pytorch-retinanet/blob/master/retinanet.py" target="_blank" rel="noopener">kuangliu/pytorch-retinanet</a></p></blockquote><p>从以上代码</p><p>_make_head（self, out_planes)</p><p>函数中可以得知：我们必须把anchor的数量考虑并体现在RetinaNet最后一层卷积核的通道数量上。 那么作为RetinaNET网络结构的这个卷积核部分，就包含了我先验的一种设计（Anchor类型数为9）。</p><p>这样做的弊端就是：假如我换了anchor的种类或数量，那么就要重新改变这个卷积核的设计，进而影响了网络的结构和参数学习，那么这就意味着我先前学习的对于9个Anchor的RetinaNet不再具有一般性，不再具备迁移学习的能力。</p><p>如果我想，换一种数据集bbox的分布，或者换一种先验anchor的选择方式，网络依旧能够使用的话，就必须将anchor的先验从原来的设计中剥离出来作为一个独立的结构，从而不影响整体结构的设计，并且可以根据需求自定义不同的anchor设计，这也就是这篇论文要解决的问题，并冠以“MetaAnchor”的称号，并使用了一个<span class="math inline">\(\mathcal{G}(b_i; w)\)</span>的anchor function generator</p><p>在RetianNet 的原设计中，每个detection head模块最后一层，对于预定义的3x3中anchor bboxes ，anchor function中：</p><ul><li>cls模块用3x3x80（类别）=720个通道卷积核，生成720维的预测向量</li><li>reg模块有3x3x4=36个通道卷积核，生成36维的预测向量</li></ul><p>而在使用MetaAnchor后，就降成了：</p><ul><li>cls模块有80（类别）=80个通道卷积核，生成80维的预测向量</li><li>reg模块有4个通道卷积核，生成4维的预测向量</li></ul><p>这就就需要重新设计anchor function。根据自己定制（customized）的anchor bbox<span class="math inline">\({b_i}\)</span>首先，应该考虑如何编码<span class="math inline">\({b_i}\)</span>，它包含了位置、尺寸、类别信息，多亏了RetianNet的全卷积结构，位置坐标信息已经包含在Feature map 中，我们使用<span class="math inline">\(\mathcal{G}(\cdot)\)</span>来预测类别，那么<span class="math inline">\({b_i}\)</span>只需要包含尺寸信息：</p><p><span class="math display">\[b_i = \left(\log \frac{ah_i}{AH}, \log \frac{aw_i}{AW} \right)\]</span></p><p>在一个训练的mini-batch中，我们给定一个二维<span class="math inline">\(b_i\)</span>的数值，分别经过两层的全连接网络<span class="math inline">\(\mathcal{G}(b_i; w_{cls})\)</span>和<span class="math inline">\(\mathcal{G}(b_i; w_{reg})\)</span>的映射，得到一个<span class="math inline">\(W_{cls}\)</span>和<span class="math inline">\(W_{reg}\)</span>维度的参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span></p><p>论文里面没有给出这个参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span>如何写入到Loss function中，我根据作者思路猜测：</p><p>论文提到 <span class="math inline">\(\mathcal{G} \left(b_i, w\right)\)</span> 是一个低秩的子空间</p><p>不过根据论文的权重预测的思想，这里的参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span>应该在lossfunction中发挥权重的作用，在训练过程中，通过给定一个位置和尺度下的anchor bbox的输出和标签，乘以相应权重，来计算该anchor点对应的所有anchors总的loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Anchor_bbox_size</span><span class="params">(ah_i,aw_i,level)</span>:</span></span><br><span class="line">        minimum_size = <span class="number">20</span></span><br><span class="line">        AH,AW = minimum_size * np.pow(<span class="number">2</span>,level<span class="number">-1</span>)</span><br><span class="line">        b_i=(np.log(ah_i/AH),np.log(aw_i/AW))</span><br><span class="line">        <span class="keyword">return</span> b_i</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">anchor_bbox_generator</span><span class="params">(b_i,level=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''b_i = (log(ah_i/AH),log(aw_i/AW))</span></span><br><span class="line"><span class="string">       b_t = [N,2]     '''</span></span><br><span class="line">    </span><br><span class="line">    hidden_dim = <span class="number">5</span></span><br><span class="line">    theta_dim = <span class="number">10</span></span><br><span class="line">    theta_standard =torch.randn(theta_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## two -layer</span></span><br><span class="line">    Residual_theta =F.linear( F.relu (F.linear(bi,(<span class="number">2</span>,hidden_dim))) , (hidden_dim,theta_dim ) )</span><br><span class="line">    </span><br><span class="line">    theta_b_i = theta_standard + Residual_theta</span><br><span class="line">    </span><br><span class="line">    reutrn theta_b_i</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RetinaNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">20</span>)</span>:</span></span><br><span class="line">        super(RetinaNet, self).__init__()</span><br><span class="line">        self.fpn = FPN50()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.reg_head = self._make_head(<span class="number">4</span>)</span><br><span class="line">        self.cls_head = self._make_head(self.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        fms = self.fpn(x)</span><br><span class="line">        reg_preds = []</span><br><span class="line">        cls_preds = []</span><br><span class="line">        <span class="keyword">for</span> fm <span class="keyword">in</span> fms:</span><br><span class="line">            loc_pred = self.loc_head(fm)</span><br><span class="line">            cls_pred = self.cls_head(fm)</span><br><span class="line">            loc_pred = loc_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,<span class="number">4</span>)            <span class="comment"># [N, 4,H,W] -&gt; [N,H,W, 4] -&gt; [N,H*W, 4]</span></span><br><span class="line">            cls_pred = cls_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,self.num_classes)  <span class="comment"># [N,20,H,W] -&gt; [N,H,W,20] -&gt; [N,H*W,20]</span></span><br><span class="line">            loc_preds.append(loc_pred)</span><br><span class="line">            cls_preds.append(cls_pred)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(loc_preds,<span class="number">1</span>), torch.cat(cls_preds,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_head</span><span class="params">(self, out_planes)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            layers.append(nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">256</span>, out_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss_meta</span><span class="params">(bi,cls_pred,cls_label,reg_pred,reg_label)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    bi = [N,2]</span></span><br><span class="line"><span class="string">    cls_pred = [N,20]</span></span><br><span class="line"><span class="string">    cls_label = [N,]</span></span><br><span class="line"><span class="string">    reg_pred = [N,4]</span></span><br><span class="line"><span class="string">    reg_label = [N,4]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    alpha = <span class="number">0.25</span></span><br><span class="line">    gamma = <span class="number">2</span></span><br><span class="line">    num_classes = <span class="number">20</span></span><br><span class="line">    </span><br><span class="line">    t = torch.eye（num_classes+<span class="number">1</span>）(cls_label, ）  <span class="comment"># [N,21] 20+背景 </span></span><br><span class="line">    <span class="comment"># t is one-hot vector</span></span><br><span class="line">    t = t[:,<span class="number">1</span>:]  <span class="comment"># 去掉 background 【N，20】 </span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    p = F.logsigmoid(cls_pred)</span><br><span class="line">    pt = p*t + (<span class="number">1</span>-p)*(<span class="number">1</span>-t)         <span class="comment"># pt = p if t &gt; 0 else 1-p</span></span><br><span class="line">    m = alpha*t + (<span class="number">1</span>-alpha)*(<span class="number">1</span>-t)   </span><br><span class="line">    m = m * (<span class="number">1</span>-pt).pow(gamma)   <span class="comment"># focal loss 系数 解决样本不平衡</span></span><br><span class="line">    </span><br><span class="line">    weight = anchor_bbox_generator(bi,) <span class="comment"># [N,W] W维的θ参数，该怎么用？ 还是说这里W=1？？</span></span><br><span class="line">    </span><br><span class="line">    cls_loss = F.binary_cross_entropy_with_logits(x, t, m, size_average=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>以上代码仅代表个人对论文的局限理解</p><p>因为看不到论文的代码，目前我理解最模糊的就是这个θ参数如何与loss function相结合的地方了，还请网友多多交流，欢迎发表更多的见解~</p><p>以上基本就介绍了是论文最主要的想法：</p><ul><li>MetaAnchor对于anchor的设定和bbox的分布更加鲁棒</li><li>MetaAnchor可以缩减不同数据集bbox分布的差异的影响，即更具迁移学习的能力！</li></ul><p>论文的更多的实验细节，我会继续阅读并更新博客~</p><p>=========================================</p><p>上次博客中说道，我理解最模糊的就是这个θ参数如何与ancnhor 的 loss function相结合的地方了</p><p>我重新阅读了论文，作者提到了权重预测的主要受到<strong>HyperNetworks</strong>的启发,然后我找来这篇论文，刚读完摘要，就恍然大悟理解了MetaAnchor里预测权重的思想，即这个θ参数的内涵，<span class="math inline">\(\theta_{b_i}\)</span> 即 <span class="math inline">\(\mathcal{F}_{csl}\left(\cdot\right)\)</span> 和 <span class="math inline">\(\mathcal{F}_{reg}\left(\cdot\right)\)</span> 的中的参数，在RetinaNet中代表了最后一层卷积核的参数！</p><h4 id="原来我在这个点上理解困难的原因是头脑中少了hypernetworks的先验">原来我在这个点上理解困难的原因是头脑中少了“HyperNetworks”的先验！</h4><blockquote><p>看来很多情况下，我们理解的困难源于：少了某些“先验知识”</p></blockquote><p><a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="noopener">HyperNetwork</a> (ICLR2017)</p><p>HyperNetwork是什么呢，简言之：</p><p><strong>用一个网络(A-HyperNetwork)生成另外另一个网络(B-主体网络)的权重</strong></p><p>听起来很神奇，因为我们一般对于网络B的学习，通常经过梯度下降法产生梯度来更新参数。而这个工作可以直接用另一个网络的输出来预测。这样做的好处就是，我们可以将巨大参数量的权重学习，转换为一个小网络的参数学习，并可以通过端到端梯度优化的方法学习！</p><p>这篇论文分析了LSTM和CNN使用HyperNetwork的方法和效果，结合我们主要论述的MetaAnchor，我来简要介绍一下Static HyperNetwork在CNN中的应用</p><h2 id="通过一个两层全连接的小网络用一个layer-embedding来预测表征cnn的卷积核参数值">通过一个两层全连接的小网络，用一个layer embedding来预测（表征）CNN的卷积核参数值</h2><p>对于一个深度的卷积神经网络，其参数主要由卷积核构成</p><p>每个卷积核有 <span class="math inline">\(N_{in} \times N_{out}\)</span> 个滤波器 每个滤波器有 <span class="math inline">\(f_{size} \times f_{size}\)</span>.</p><p>假设这些参数存在一个矩阵 <span class="math inline">\(K^j \in \mathbb{R}^{N_{in}f_{size} \times N_{out}f_{size}}\)</span> for each layer <span class="math inline">\(j = 1,..,D\)</span>, 其中 <span class="math inline">\(D\)</span> 是卷积网络的深度</p><p>对于每一层 <span class="math inline">\(j\)</span>, hypernetwork 接受一个 a layer embedding <span class="math inline">\(z^j \in \mathbb{R}^{N_{z}}\)</span> 作为输入，并预测 <span class="math inline">\(K^j\)</span>, 可以写成:</p><p><span class="math display">\[ {K^j} = g( {z^j} ),\forall j = 1,..., D\]</span></p><p><span class="math display">\[{K} \in \mathbb{R}^{ N_{in}f_{size} \times N_{out}f_{size}}, {z} \in \mathbb{R}^{N_z}\]</span></p><figure><img src="https://img-blog.csdnimg.cn/20181214193114742.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><p>公式中，所有可学习的参数 <span class="math inline">\(W_i\)</span>, <span class="math inline">\(B_i\)</span>, <span class="math inline">\(W_{out}\)</span>, <span class="math inline">\(B_{out}\)</span> 对于所有 <span class="math inline">\(z^{j}\)</span>共享</p><p>在推理时, 模型仅仅将学习到的 the layer embeddings <span class="math inline">\(z^j\)</span> 来生成第 <span class="math inline">\(j\)</span> 层的卷积核权重参数</p><p>这就将可学习的参数量改变了:</p><p><span class="math display">\[D \times N_{in} \times f_{size} \times N_{out}\times f_{size}\]</span></p><p><span class="math display">\[\rightarrow\]</span></p><p><span class="math display">\[N_{z}\times D + d\times (N_z + 1)\times N_i + f_{size}\times N_{out}\times f_{size}\times (d+1)\]</span></p><h4 id="应用到metaanchor中theta_b_i即retinanet的最后一层卷积核的参数">应用到MetaAnchor中：<span class="math inline">\(\theta_{b_i}\)</span>即RetinaNet的最后一层卷积核的参数</h4><p>即，我们用自定义anchor设计<span class="math inline">\({b_i}\)</span>成二维向量，作为“layer embedding”，输入两层的网络，预测了RetinaNet的最后一层卷积核参数的残差，这样就降低了原RetinaNet的卷积核滤波器的数量，就像之前提到的。</p><p><strong>～～2019年6月25日更～～</strong> 最后，可能还需要再次提到<strong>关于<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>这两项的理解</strong>：</p><p>公式提到： <span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; w) \\= \theta^* + \mathcal{R}(b_i; w)\]</span></p><p>我们想要学习的是<span class="math inline">\(\theta_{b_i}\)</span> ，它有两部分构成：<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>，当进行学习的时候，梯度流到<span class="math inline">\(\theta_{b_i}\)</span> 节点，会产生<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>的梯度：<span class="math inline">\(\nabla\theta^*\)</span> 和<span class="math inline">\(\nabla\mathcal{R}(b_i; w)\)</span>, 而<span class="math inline">\(\nabla\theta^*\)</span>可以直接用来更新<span class="math inline">\(\theta^*\)</span> ，但<span class="math inline">\(\nabla\mathcal{R}(b_i; w)\)</span>进一步通过HyperNetwork网络流到其参数<span class="math inline">\(\nabla w\)</span>上，然后更新HyperNetwork的参数<span class="math inline">\(w\)</span>。以上是进一步的分析了，如果有问题欢迎提出～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NeuIPS 2018&lt;/a&gt; &amp;gt; 原创博文 转载请注明&lt;a href=&quot;https://yangsenius.github.io/blog/MetaAnchor/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://yangsenius.github.io/blog/MetaAnchor/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一般目标检测方法中的Anchors的生成是来自于人类的先验知识:&lt;span class=&quot;math inline&quot;&gt;\(b_i\in \mathcal{B} \ which \ is \ predefined \ by \ human\)&lt;/span&gt;（&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{B}\)&lt;/span&gt;属于 &lt;span class=&quot;math inline&quot;&gt;\({prior}\)&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;代表网格或锚点），即&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过固定锚点，或者划分网格，生成一定形状和尺寸的Anchor Bboxes 来作为候选检测区域,提取对应位置的图像特征，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先验往往代表设计人员在构思最初的朴素想法，来源于直觉，并把这种直觉融合在设计者的实现过程与代码中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
      <category term="Object Detection" scheme="http://senyang-ml.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Learning to learn by gradient descent by gradient descent-PyTorch实践</title>
    <link href="http://senyang-ml.github.io/2018/12/17/learning_to_learn/"/>
    <id>http://senyang-ml.github.io/2018/12/17/learning_to_learn/</id>
    <published>2018-12-17T01:52:16.000Z</published>
    <updated>2020-03-13T10:59:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创博文，转载请注明来源</p></blockquote><h2 id="引言">引言</h2><p>“浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！”</p><p>Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公式符号，初学者很容易看不下去，而比较好的方法就是先浏览目录，掌握一些简单的概念（回归与分类啊，监督与无监督啊），并在按顺序的阅读过程学会“前瞻”与“回顾”，进行快速学习。又比如在早期接受教育的学习阶段，盲目的“题海战术”或死记硬背的“知识灌输”如果不加上恰当的反思和总结，往往会耗时耗力，最后达到的效果却一般，这是因为在接触新东西，掌握新技能时，是需要“技巧性”的。</p><p>从学习知识到学习策略的层面上，总会有“最强王者”在告诉我们，“钻石的操作、黄铜的意识”也许并不能取胜，要“战略上最佳，战术上谨慎”才能更快更好地进步。</p><a id="more"></a><p>这跟本文要讲的内容有什么关系呢？进入正题。 &gt; &gt; 其实读者可以先回顾自己从初高中到大学甚至研究生的整个历程，是不是发现自己已经具备了“learning to learn”的能力？</p><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png" alt="在这里插入图片描述"> ## Learning to learn by gradient descent by gradient descent</p><p><strong>通过梯度下降来学习如何通过梯度下降学习</strong></p><blockquote><p>是否可以让优化器学会 &quot;为了更好地得到，要先去舍弃&quot; 这样的“策略”？</p></blockquote><p>本博客结合具体实践来解读《Learning to learn by gradient descent by gradient descent》，这是一篇Meta-learning（元学习）领域的<a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">论文</a>,发表在2016年的NIPS。类似“回文”结构的起名，让这篇论文变得有趣，是不是可以再套一层,&quot;Learning to learn to learn by gradient descent by gradient descent by gradient descent&quot;?再套一层？</p><p>首先别被论文题目给误导，==它不是求梯度的梯度，这里不涉及到二阶导的任何操作，而是跟如何学会更好的优化有关==，正确的断句方法为learning to (learn by gradient descent ) by gradient descent 。</p><p>第一次读完后，不禁惊叹作者巧妙的构思--使用LSTM（long short-term memory）优化器来替代传统优化器如（SGD，RMSProp，Adam等），然后使用梯度下降来优化优化器本身。</p><p>虽然明白了作者的出发点，但总感觉一些细节自己没有真正理解。然后就去看原作的代码实现，读起来也是很费劲。查阅了一些博客，但网上对这篇论文解读很少，停留于论文翻译理解上。再次揣摩论文后，打算做一些实验来理解。 在用PyTorch写代码的过程，才恍然大悟，作者的思路是如此简单巧妙，论文名字起的也很恰当，没在故弄玄虚，但是在实现的过程却费劲了周折！</p><h2 id="文章目录">文章目录</h2><p>[TOC]</p><p><strong>如果想看最终版代码和结果，可以直接跳到文档的最后！！</strong></p><p>下面写的一些文字与代码主要站在我自身的角度，记录自己在学习研究这篇论文和代码过程中的所有历程，如何想的，遇到了什么错误，问题在哪里，我把自己理解领悟“learning to learn”这篇论文的过程剖析了一下，也意味着我自己也在“learning to learn”！为了展现自己的心路历程，我基本保留了所有的痕迹，这意味着有些代码不够整洁，不过文档的最后是最终简洁完整版。 ==提醒：看完整个文档需要大量的耐心 : )==</p><p>我默认读者已经掌握了一些必要知识，也希望通过回顾这些经典研究给自己和一些读者带来切实的帮助和启发。</p><p>用Pytorch实现这篇论文想法其实很方便，但是论文作者来自DeepMind，他们用<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow写的项目</a>，读他们的代码你就会领教到最前沿的一线AI工程师们是如何进行工程实践的。</p><p>下面进入正题，我会按照最简单的思路，循序渐进地展开, &lt;0..0&gt;。</p><h2 id="优化问题">优化问题</h2><p>经典的机器学习问题，包括当下的深度学习相关问题，大多可以被表达成一个目标函数的优化问题：</p><p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p><p>一些优化方法可以求解上述问题，最常见的即梯度更新策略： <span class="math display">\[\theta_{t+1}=\theta_{t}-\alpha_{t}*\nabla f\left ( \theta_{t}\right )\]</span></p><p>早期的梯度下降会忽略梯度的二阶信息，而经典的优化技术通过加入<strong>曲率信息</strong>改变步长来纠正，比如Hessian矩阵的二阶偏导数。 <strong>Deep learning</strong>社区的壮大，演生出很多求解高维非凸的优化求解器，如 <strong>momentum</strong>[Nesterov, 1983, Tseng, 1998], <strong>Rprop</strong> [Riedmiller and Braun, 1993], <strong>Adagrad</strong> [Duchi et al., 2011], <strong>RMSprop</strong> [Tieleman and Hinton, 2012], and <strong>ADAM</strong> [Kingma and Ba, 2015].</p><p>目前用于大规模图像识别的模型往往使用卷积网络CNN通过定义一个代价函数来拟合数据与标签，其本质还是一个优化问题。</p><p>这里我们考虑一个简单的优化问题，比如求一个四次非凸函数的最小值点。对于更复杂的模型，下面的方法同样适用。 ### 定义要优化的目标函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">w = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(w,a=<span class="number">0.5</span>,b=<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="comment">#定义要优化的函数，求x的最优解</span></span><br><span class="line">    x= w*(x<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((x+<span class="number">1</span>)*(x+<span class="number">0.5</span>)*x*(x<span class="number">-1</span>)).sum()</span><br></pre></td></tr></table></figure><h3 id="定义常用的优化器如sgd-rmsprop-adam">定义常用的优化器如SGD, RMSProp, Adam。</h3><p>SGD仅仅只是给梯度乘以一个学习率。</p><p>RMSProp的方法是：</p><p><span class="math display">\[E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t\]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\]</span></p><p>当前时刻下，用当前梯度和历史梯度的平方加权和（越老的历史梯度，其权重越低）来重新调节学习率(如果历史梯度越低，“曲面更平坦”，那么学习率越大，梯度下降更“激进”一些，如果历史梯度越高，“曲面更陡峭”那么学习率越小，梯度下降更“谨慎”一些)，来更快更好地朝着全局最优解收敛。</p><p>Adam是RMSProp的变体： <span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><p><span class="math display">\[\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\ \hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}\]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</span> 即通过估计当前梯度的一阶矩估计和二阶矩估计来代替，梯度和梯度的平方，然后更新策略和RMSProp一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.1</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">    </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br></pre></td></tr></table></figure><p>这里的Adam优化器直接用了Pytorch里定义的。然后我们通过优化器来求解极小值x，通过梯度下降的过程，我们期望的函数值是逐步下降的。 这是我们一般人为设计的学习策略，即==逐步梯度下降法，以“每次都比上一次进步一些” 为原则进行学习！==</p><h3 id="接下来-构造优化算法">接下来 构造优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_STEPS = <span class="number">15</span></span><br><span class="line">theta = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(theta,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">theta_init = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   PyTorch 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i]))+1)*loss</span></span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#print(loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            x = x.detach_()</span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#x.retain_grad()</span></span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph</span><br></pre></td></tr></table></figure><h3 id="对比不同优化器的优化效果">对比不同优化器的优化效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">T = np.arange(TRAINING_STEPS)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201209548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="100"><figcaption>100</figcaption></figure><pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211</code></pre><p>通过上述实验可以发现，这些优化器都可以发挥作用，似乎RMS表现更加优越一些，不过这并不代表RMS就比其他的好,可能这个优化问题还是较为简单，调整要优化的函数，可能就会看到不同的结果。</p><h2 id="meta-optimizer-从手工设计优化器迈步到自动设计优化器">Meta-optimizer ：从手工设计优化器迈步到自动设计优化器</h2><p>上述这些优化器的更新策略是根据人的经验主观设计，要来解决一般的优化问题的。</p><p><strong>No Free Lunch Theorems for Optimization</strong> [Wolpert and Macready, 1997] 表明组合优化设置下，==没有一个算法可以绝对好过一个随机策略==。这暗示，一般来讲，对于一个子问题，特殊化其优化方法是提升性能的唯一方法。</p><p>而针对一个特定的优化问题，也许一个特定的优化器能够更好的优化它，我们是否可以不根据人工设计，而是让优化器本身根据模型与数据，自适应地调节，这就涉及到了meta-learning ### 用一个可学习的梯度更新规则，替代手工设计的梯度更新规则</p><p><span class="math display">\[\theta_{t+1}=\theta_{t}+g\textit{}_{t}\left (f\left ( \theta_{t}\right ),\phi \right)\]</span></p><p>这里的<span class="math inline">\(g(\cdot)\)</span>代表其梯度更新规则函数，通过参数<span class="math inline">\(\phi\)</span>来确定，其输出为目标函数f当前迭代的更新梯度值，<span class="math inline">\(g\)</span>函数通过RNN模型来表示，保持状态并动态迭代</p><p>假如一个优化器可以根据历史优化的经验来自身调解自己的优化策略，那么就一定程度上做到了自适应，这个不是说像Adam，momentum，RMSprop那样自适应地根据梯度调节学习率，（其梯度更新规则还是不变的），而是说自适应地改变其梯度更新规则，而Learning to learn 这篇论文就使用LSTM（RNN）优化器做到了这一点，毕竟RNN存在一个可以保存历史信息的隐状态，LSTM可以从一个历史的全局去适应这个特定的优化过程，做到论文提到的所谓的“CoordinateWise”，我的理解是：LSTM的参数对每个时刻节点都保持“聪明”，是一种“全局性的聪明”，适应每分每秒。</p><h4 id="构建lstm优化器">构建LSTM优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line">lstm=torch.nn.LSTM(Input_DIM,Hidden_nums ,Layers)</span><br><span class="line">Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span><br><span class="line">batchsize = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(lstm)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM_Optimizee</span><span class="params">(gradients, state)</span>:</span></span><br><span class="line">    <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">    <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">    gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)   </span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                 torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">   </span><br><span class="line">    update, state = lstm(gradients, state) <span class="comment"># 用optimizee_lstm代替 lstm</span></span><br><span class="line">    update = Linear(update)</span><br><span class="line">    <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">    <span class="keyword">return</span> update.squeeze().squeeze(), state</span><br></pre></td></tr></table></figure><pre><code>LSTM(10, 20, num_layers=2)</code></pre><p>从上面LSTM优化器的设计来看，我们几乎没有加入任何先验的人为经验在里面，只是用了长短期记忆神经网络的架构</p><h4 id="优化器本身的参数即lstm的参数代表了我们的更新策略">优化器本身的参数即LSTM的参数，代表了我们的更新策略</h4><p><strong>这个优化器的参数代表了我们的更新策略，后面我们会学习这个参数，即学习用什么样的更新策略</strong></p><p>对了如果你不太了解LSTM的话，我就放这个网站 http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 博客的几个图，它很好解释了什么是RNN和LSTM：</p><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="600"></center><h5 id="好了看一下我们使用刚刚初始化的lstm优化器后的优化结果">好了，看一下我们使用刚刚初始化的LSTM优化器后的优化结果</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(TRAINING_STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,TRAINING_STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201313760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211,lstm=554.2158203125</code></pre><h5 id="咦为什么lstm优化器那么差根本没有优化效果">咦，为什么LSTM优化器那么差，根本没有优化效果？</h5><p><strong>先别着急质疑！因为我们还没有学习LSTM优化器！</strong></p><p>用到的LSTM模型完全是随机初始化的！并且LSTM的参数在TRAIN_STEPS=[0,T]中的每个节点都是保持不变的！</p><h4 id="下面我们就来优化lstm优化器的参数">下面我们就来优化LSTM优化器的参数！</h4><p>不论是原始优化问题，还是隶属元学习的LSTM优化目标，我们都一个共同的学习目标：</p><p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p><p>或者说我们希望迭代后的loss值变得很小，传统方法，是基于每个迭代周期，一步一步，让loss值变小，可以说，传统优化器进行梯度下降时所站的视角是在某个周期下的，那么，我们其实可以换一个视角，更全局的视角，即，我们希望所有周期迭代的loss值都很小，这和传统优化是不违背的，并且是全局的，这里做个比喻，优化就像是下棋，优化器就是</p><h5 id="下棋手">“下棋手 ”</h5><p>如果一个棋手，在每走一步之前，都能看未来很多步被这一步的影响，那么它就能在当前步做出最佳策略，而LSTM的优化过程，就是把一个历史全局的“步”放在一起进行优化，所以LSTM的优化就具备了“瞻前顾后”的能力！</p><p>关于这一点，论文给出了一个期望loss的定义： <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]\]</span></p><p>但这个实现起来并不现实，我们只需要将其思想具体化。</p><ul><li><p>Meta-optimizer优化：目标函数“所有周期的loss都要很小！”，而且这个目标函数是独立同分布采样的（比如，这里意味着任意初始化一个优化问题模型的参数，我们都希望这个优化器能够找到一个优化问题的稳定的解）</p></li><li><p>传统优化器：&quot;对于当前的目标函数，只要这一步的loss比上一步的loss值要小就行”</p></li></ul><h5 id="特点-2.考虑优化器优化过程的历史全局性信息-3.独立同分布地采样优化问题目标函数的参数">特点 ： 2.考虑优化器优化过程的历史全局性信息 3.独立同分布地采样优化问题目标函数的参数</h5><p>接下来我们就站在更全局的角度，来优化LSTM优化器的参数</p><p>LSTM是循环神经网络，它可以连续记录并传递所有周期时刻的信息，其每个周期循环里的子图共同构建一个巨大的图，然后使用Back-Propagation Through Time (BPTT)来求导更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lstm_losses,global_graph_loss= learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span>) <span class="comment"># [loss1,loss2,...lossT] 所有周期的loss</span></span><br><span class="line"><span class="comment"># 因为这里要保留所有周期的计算图所以retain_graph_flag =True</span></span><br><span class="line">all_computing_graph_loss = torch.tensor(lstm_losses).sum() </span><br><span class="line"><span class="comment">#构建一个所有周期子图构成的总计算图,使用BPTT来梯度更新LSTM参数</span></span><br><span class="line"></span><br><span class="line">print(all_computing_graph_loss,global_graph_loss )</span><br><span class="line">print(global_graph_loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(554.2158) tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)</code></pre><p>可以看到，变量<strong>global_graph_loss</strong>保留了所有周期产生的计算图grad_fn=<ThAddBackward></ThAddBackward></p><p>下面针对LSTM的参数进行全局优化,优化目标：“所有周期之和的loss都很小”。 值得说明一下：在LSTM优化时的参数，是在所有Unroll_TRAIN_STEPS=[0,T]中保持不变的，在进行完所有Unroll_TRAIN_STEPS以后，再整体优化LSTM的参数。</p><p>这也就是论文里面提到的coordinate-wise，即“对每个时刻点都保持‘全局聪明’”，即学习到LSTM的参数是全局最优的了。因为我们是站在所有TRAIN_STEPS=[0,T]的视角下进行的优化！</p><p>优化LSTM优化器选择的是Adam优化器进行梯度下降</p><h4 id="通过梯度下降法来优化-优化器">通过梯度下降法来优化 优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Global_Train_Steps = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(global_loss_1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        <span class="comment">#adam_global_optimizer.zero_grad()</span></span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#print(global_loss)</span></span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure><pre><code>xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]</code></pre><h5 id="为什么loss值没有改变为什么lstm参数的梯度不存在的">为什么loss值没有改变？为什么LSTM参数的梯度不存在的？</h5><p>通过分析推理，我发现了LSTM参数的梯度为None,那么反向传播就完全没有更新LSTM的参数！</p><p>为什么参数的梯度为None呢，优化器并没有更新指定的LSTM的模型参数，一定是什么地方出了问题，我想了好久，还是做一些简单的实验来找一找问题吧。</p><blockquote><p>ps: 其实写代码做实验的过程，也体现了人类本身学会学习的高级能力，那就是：通过实验来实现想法时，实验结果往往和预期差别很大，那一定有什么地方出了问题，盲目地大量试错法可能找不到真正问题所在，如何找到问题所在并解决，就是一种学会如何学习的能力，也是一种强化学习的能力。这里我采用的人类智能是：以小见大法。 In a word , if we want the machine achieving to AGI, it must imiate human's ability of reasoning and finding where the problem is and figuring out how to solve the problem. Meta Learning contains this idea.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">z= torch.empty(<span class="number">2</span>)</span><br><span class="line">torch.nn.init.uniform_(z , <span class="number">-2</span>, <span class="number">2</span>)</span><br><span class="line">z.requires_grad = <span class="literal">True</span></span><br><span class="line">z.retain_grad()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (z*z).sum()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam([z],lr=<span class="number">0.01</span>)</span><br><span class="line">grad =[]</span><br><span class="line">losses= []</span><br><span class="line">zgrad =[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    q = f(z)</span><br><span class="line">    loss = q**<span class="number">2</span></span><br><span class="line">    <span class="comment">#z.retain_grad()</span></span><br><span class="line">    loss.backward(retain_graph = <span class="literal">True</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment">#print(x,x.grad,loss,)</span></span><br><span class="line">    </span><br><span class="line">    loss.retain_grad()</span><br><span class="line">    print(q.grad,q.requires_grad)</span><br><span class="line">    grad.append((z.grad))</span><br><span class="line">    losses.append(loss)</span><br><span class="line">    zgrad.append(q.grad)</span><br><span class="line">    </span><br><span class="line">print(grad)</span><br><span class="line">print(losses)</span><br><span class="line">print(zgrad)</span><br></pre></td></tr></table></figure><pre><code>None TrueNone True[tensor([-44.4396, -36.7740]), tensor([-44.4396, -36.7740])][tensor(35.9191, grad_fn=&lt;PowBackward0&gt;), tensor(35.0999, grad_fn=&lt;PowBackward0&gt;)][None, None]</code></pre><h5 id="问题出在哪里">问题出在哪里？</h5><p>经过多方面的实验修改，我发现LSTM的参数在每个周期内BPTT的周期内，并没有产生梯度！！怎么回事呢？我做了上面的小实验。</p><p>可以看到z.grad = None,但是z.requres_grad = True，z变量作为x变量的子节点，其在计算图中的梯度没有被保留或者没办法获取，那么我就应该通过修改一些PyTorch的代码，使得计算图中的叶子节点的梯度得以存在。然后我找到了retain_grad()这个函数，实验证明，它必须在backward()之前使用才能保存中间叶子节点的梯度！这样的方法也就适合于LSTM优化器模型参数的更新了吧？</p><p>那么如何保留LSTM的参数在每个周期中产生的梯度是接下来要修改的！</p><p>这是因为我计算loss = f(x)，然后loss.backward() 这里的loss计算并没有和LSTM产生关系，我先来想一想loss和LSTM的关系在哪里？</p><p>论文里有一张图，可以作为参考：</p><center>图2<img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/blog/learn-to-learn/graph.PNG" width="600"></center><p><strong>LSTM参数的梯度来自于每次输出的“update”的梯度 update的梯度包含在生成的下一次迭代的参数x的梯度中</strong></p><p>哦!因为参数<span class="math inline">\(x_t = x_{t-1}+update_{t-1}\)</span> 在BPTT的每个周期里<span class="math inline">\(\frac{\partial loss_t}{\partial \theta_{LSTM}}=\frac{\partial loss_t}{\partial update_{t-1}}*\frac{\partial update_{t-1}}{\partial \theta_{LSTM}}\)</span>,那么我们想通过<span class="math inline">\(loss_0,loss_1,..,loss_t\)</span>之和来更新<span class="math inline">\(\theta_{LSTM}\)</span>的话，就必须让梯度经过<span class="math inline">\(x_t\)</span> 中 的<span class="math inline">\(update_{t-1}\)</span>流回去，那么每次得到的<span class="math inline">\(x_t\)</span>就必须包含了上一次更新产生的图（可以想像，这个计算图是越来越大的），想一想我写的代码，似乎没有保留上一次的计算图在<span class="math inline">\(x_t\)</span>节点中，因为我用了x = x.detach_() 把x从图中拿了下来！这似乎是问题最关键所在！！！（而tensorflow静态图的构建，直接建立了一个完整所有周期的图，似乎Pytorch的动态图不合适？no，no）</p><p>（注：以上来自代码中的<span class="math inline">\(x_t\)</span>对应上图的<span class="math inline">\(\theta_t\)</span>，<span class="math inline">\(update_{t}\)</span>对应上图的<span class="math inline">\(g_t\)</span>）</p><p>我为什么会加入x = x.detach_() 是因为不加的话，x变成了子节点，下一次求导pytorch不允许，其实只需要加一行x.retain_grad()代码就行了,并且总的计算图的globa_graph_loss在逐步降低！问题解决！</p><p>目前在运行global_training(lstm)函数的话，就会发现LSTM的参数已经根据计算图中的梯度回流产生了梯度，每一步可以更新参数了， 但是这个BPTT算法用cpu算起来，有点慢了~</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># x = x.detach_()</span></span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            x.retain_grad()</span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam([&#123;<span class="string">'params'</span>:optimizee.parameters()&#125;,&#123;<span class="string">'params'</span>:Linear.parameters()&#125;],lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure><pre><code>tensor(193.6147, grad_fn=&lt;ThAddBackward&gt;)tensor(7.6411)</code></pre><h5 id="计算图不再丢失了lstm的参数的梯度经过计算图的流动已经产生了">计算图不再丢失了，LSTM的参数的梯度经过计算图的流动已经产生了！</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/2018112320175122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">15</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201810120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123201822472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre><p>可以看出来，经过优化后的LSTM优化器，似乎已经开始掌握如何优化的方法，即我们基本训练出了一个可以训练模型的优化器！</p><p><strong>但是效果并不是很明显！</strong></p><p>不过代码编写取得一定进展 (0 ..0) /，接下就是让效果更明显，性能更稳定了吧？</p><p><strong>不过我先再多测试一些周期看看LSTM的优化效果！先别高兴太早！</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">50</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201835829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=150.99790954589844,rms=5.940474033355713,adam=14.17563247680664,lstm=268.0199279785156</code></pre><h4 id="又出了什么幺蛾子">又出了什么幺蛾子？</h4><p>即我们基本训练出了一个可以训练模型的优化器！但是 经过更长周期的测试，发现训练好的优化器只是优化了指定的周期的loss，而并没有学会“全局优化”的本领，这个似乎是个大问题！</p><h5 id="不同周期下输入lstm的梯度幅值数量级不在一个等级上面">不同周期下输入LSTM的梯度幅值数量级不在一个等级上面</h5><p>要处理一下！</p><p>论文里面提到了对梯度的预处理，即处理不同数量级别的梯度，来进行BPTT，因为每个周期的产生的梯度幅度是完全不在一个数量级，前期梯度下降很快，中后期梯度下降平缓，这个对于LSTM的输入，变化裕度太大，应该归一化，但这个我并没有考虑，接下似乎该写这个部分的代码了</p><p>论文里提到了： One potential challenge in training optimizers is that different input coordinates (i.e. the gradients w.r.t. different optimizee parameters) can have very different magnitudes. This is indeed the case e.g. when the optimizee is a neural network and different parameters correspond to weights in different layers. This can make training an optimizer difficult, because neural networks naturally disregard small variations in input signals and concentrate on bigger input values.</p><h5 id="用梯度的归一化幅值方向二元组替代原梯度作为lstm的输入">用梯度的（归一化幅值，方向）二元组替代原梯度作为LSTM的输入</h5><p>To this aim we propose to preprocess the optimizer's inputs. One solution would be to give the optimizer <span class="math inline">\(\left(\log(|\nabla|),\,\operatorname{sgn}(\nabla)\right)\)</span> as an input, where <span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p><p><span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p><p><span class="math display">\[\nabla^k\rightarrow  \left\{\begin{matrix} \left(\frac{\log(|\nabla|)}{p}\,, \operatorname{sgn}(\nabla)\right)&amp; \text{if } |\nabla| \geq e^{-p}\\ (-1, e^p \nabla)   &amp; \text{otherwise}\end{matrix}\right.\]</span></p><p>作者将不同幅度和方向下的梯度，用一个标准化到<span class="math inline">\([-1,1]\)</span>的幅值和符号方向二元组来表示原来的梯度张量！这样将有助于LSTM的参数学习！ 那我就重新定义LSTM优化器！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line"><span class="comment">#lstm1=torch.nn.LSTM(Input_DIM*2,Hidden_nums ,Layers)</span></span><br><span class="line"><span class="comment">#Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size)</span><br><span class="line">        <span class="comment">#elf.lstm = torch.nn.LSTM(10, 20,2)</span></span><br><span class="line">        <span class="comment">#elf.Linear = torch.nn.Linear(20,10)</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作"""</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                         torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        </span><br><span class="line">        update = Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,gradients, prev_state)</span>:</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line"></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM*<span class="number">2</span>, Output_DIM, Hidden_nums ,Layers , batchsize=<span class="number">1</span>,)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">grads = torch.randn(<span class="number">10</span>)*<span class="number">10</span></span><br><span class="line">print(grads.size())</span><br><span class="line"></span><br><span class="line">update,state =  LSTM_Optimizee(grads,<span class="literal">None</span>)</span><br><span class="line">print(update.size(),)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([10])torch.Size([10])</code></pre><p>编写成功！ 执行！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):     </span><br><span class="line">            loss = f(x)    </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">           <span class="comment"># print('loss&#123;&#125;:'.format(i),loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            </span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">           <span class="comment">#print(update)</span></span><br><span class="line">            losses.append(loss)     </span><br><span class="line">            x = x + update  </span><br><span class="line">            x.retain_grad()</span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(LSTM_Optimizee)</span><br></pre></td></tr></table></figure><pre><code>tensor(239.6029, grad_fn=&lt;ThAddBackward&gt;)tensor(158.0625)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">30</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span>  </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201922189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=94.19924926757812,rms=5.705832481384277,adam=13.772469520568848,lstm=1224.2393798828125</code></pre><p><strong>为什么loss出现了NaN？？？为什么优化器的泛化性能很差？？很烦</strong></p><p>即超过Unroll周期以后，LSTM优化器不再具备优化性能？？？</p><p>然后我又回顾论文，发现，对优化器进行优化的每个周期开始，要重新随机化，优化问题的参数，即确保我们的LSTM不针对一个特定优化问题过拟合，什么？优化器也会过拟合！是的！l</p><p>论文里面其实也有提到：</p><p>Given a distribution of functions <span class="math inline">\(f\)</span> we will write the expected loss as: <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]（1）\]</span></p><p>其中f是随机分布的，那么就需要在Unroll的初始进行从IID 标准Gaussian分布随机采样函数的参数</p><p>另外我完善了代码，根据原作代码实现，把LSTM的输出乘以一个系数0.01，那么LSTM的学习变得更加快速了。</p><p>还有一个地方，就是作者优化optimiee用了100个周期，即5个连续的Unroll周期，这一点似乎我之前也没有考虑到！</p><hr><hr><h2 id="以上是代码编写遇到的种种问题下面就是最完整的有效代码了">以上是代码编写遇到的种种问题，下面就是最完整的有效代码了！！！</h2><p><strong>我们考虑优化论文中提到的Quadratic函数，并且用论文中完全一样的实验条件！</strong></p><p><span class="math display">\[f(\theta) = \|W\theta - y\|_2^2\]</span> for different 10x10 matrices <span class="math inline">\(W\)</span> and 10-dimensional vectors <span class="math inline">\(y\)</span> whose elements are drawn from an IID Gaussian distribution. Optimizers were trained by optimizing random functions from this family and tested on newly sampled functions from the same distribution. Each function was optimized for 100 steps and the trained optimizers were unrolled for 20 steps. We have not used any preprocessing, nor postprocessing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning to learn by gradient descent by gradient descent</span></span><br><span class="line"><span class="comment"># =========================#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1611.03824</span></span><br><span class="line"><span class="comment"># https://yangsenius.github.io/blog/LSTM_Meta/</span></span><br><span class="line"><span class="comment"># https://github.com/yangsenius/learning-to-learn-by-pytorch</span></span><br><span class="line"><span class="comment"># author：yangsen</span></span><br><span class="line"><span class="comment"># #### “通过梯度下降来学习如何通过梯度下降学习”</span></span><br><span class="line"><span class="comment"># #### 要让优化器学会这样   "为了更好地得到，要先去舍弃"  这样类似的知识！</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> timeit <span class="keyword">import</span> default_timer <span class="keyword">as</span> timer</span><br><span class="line"><span class="comment">#####################      优化问题   ##########################</span></span><br><span class="line">USE_CUDA = <span class="literal">False</span></span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">batchsize = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    USE_CUDA = <span class="literal">True</span>  </span><br><span class="line">USE_CUDA = <span class="literal">False</span>  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'\n\nUSE_CUDA = &#123;&#125;\n\n'</span>.format(USE_CUDA))</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(W,Y,x)</span>:</span></span><br><span class="line">    <span class="string">"""quadratic function : f(\theta) = \|W\theta - y\|_2^2"""</span></span><br><span class="line">    <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">        W = W.cuda()</span><br><span class="line">        Y = Y.cuda()</span><br><span class="line">        x = x.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ((torch.matmul(W,x.unsqueeze(<span class="number">-1</span>)).squeeze()-Y)**<span class="number">2</span>).sum(dim=<span class="number">1</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################    手工的优化器   ###################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.01</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA == <span class="literal">True</span>:</span><br><span class="line">            state = state.cuda()</span><br><span class="line">            </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################    自动 LSTM 优化器模型  ##########################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.input_flag = <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> preprocess != <span class="literal">True</span>:</span><br><span class="line">             self.input_flag = <span class="number">1</span></span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size*self.input_flag, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size) <span class="comment">#1-&gt; output_size</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作</span></span><br><span class="line"><span class="string">        coordinate-wise LSTM """</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">            <span class="keyword">if</span> USE_CUDA :</span><br><span class="line">                 prev_state = (torch.zeros(Layers,batchsize,Hidden_nums).cuda(),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums).cuda())</span><br><span class="line">         </span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        update = self.Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            input_gradients = input_gradients.cuda()</span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = input_gradients.unsqueeze(<span class="number">0</span>)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">      </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line">    </span><br><span class="line"><span class="comment">#################   优化器模型参数  ##############################</span></span><br><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line">output_scale_value=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   构造一个优化器  #######</span></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM, Output_DIM, Hidden_nums ,Layers , batchsize=batchsize,\</span><br><span class="line">                preprocess=<span class="literal">False</span>,output_scale=output_scale_value)</span><br><span class="line">print(LSTM_Optimizee)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    LSTM_Optimizee = LSTM_Optimizee.cuda()</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">######################  优化问题目标函数的学习过程   ###############</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Learner</span><span class="params">( object )</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args :</span></span><br><span class="line"><span class="string">        `f` : 要学习的问题</span></span><br><span class="line"><span class="string">        `optimizee` : 使用的优化器</span></span><br><span class="line"><span class="string">        `train_steps` : 对于其他SGD,Adam等是训练周期，对于LSTM训练时的展开周期</span></span><br><span class="line"><span class="string">        `retain_graph_flag=False`  : 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">        `reset_theta = False `  :  默认每次学习前 不随机初始化参数</span></span><br><span class="line"><span class="string">        `reset_function_from_IID_distirbution = True` : 默认从分布中随机采样函数 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return :</span></span><br><span class="line"><span class="string">        `losses` : reserves each loss value in each iteration</span></span><br><span class="line"><span class="string">        `global_loss_graph` : constructs the graph of all Unroll steps for LSTM's BPTT </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,    f ,   optimizee,  train_steps ,  </span></span></span><br><span class="line"><span class="function"><span class="params">                                            eval_flag = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            retain_graph_flag=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_theta = False ,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_function_from_IID_distirbution = True)</span>:</span></span><br><span class="line">        self.f = f</span><br><span class="line">        self.optimizee = optimizee</span><br><span class="line">        self.train_steps = train_steps</span><br><span class="line">        <span class="comment">#self.num_roll=num_roll</span></span><br><span class="line">        self.eval_flag = eval_flag</span><br><span class="line">        self.retain_graph_flag = retain_graph_flag</span><br><span class="line">        self.reset_theta = reset_theta</span><br><span class="line">        self.reset_function_from_IID_distirbution = reset_function_from_IID_distirbution  </span><br><span class="line">        self.init_theta_of_f()</span><br><span class="line">        self.state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">        self.losses = []   <span class="comment"># 保存每个训练周期的loss值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_theta_of_f</span><span class="params">(self,)</span>:</span>  </span><br><span class="line">        <span class="string">''' 初始化 优化问题 f 的参数 '''</span></span><br><span class="line">        self.DIM = <span class="number">10</span></span><br><span class="line">        self.batchsize = <span class="number">128</span></span><br><span class="line">        self.W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">        self.Y = torch.randn(batchsize,DIM)</span><br><span class="line">        self.x = torch.zeros(self.batchsize,self.DIM)</span><br><span class="line">        self.x.requires_grad = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            self.W = self.W.cuda()</span><br><span class="line">            self.Y = self.Y.cuda()</span><br><span class="line">            self.x = self.x.cuda()</span><br><span class="line">        </span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Reset_Or_Reuse</span><span class="params">(self , x , W , Y , state, num_roll)</span>:</span></span><br><span class="line">        <span class="string">''' re-initialize the `W, Y, x , state`  at the begining of each global training</span></span><br><span class="line"><span class="string">            IF `num_roll` == 0    '''</span></span><br><span class="line"></span><br><span class="line">        reset_theta =self.reset_theta</span><br><span class="line">        reset_function_from_IID_distirbution = self.reset_function_from_IID_distirbution</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">            theta = torch.zeros(batchsize,DIM)</span><br><span class="line">           </span><br><span class="line">            theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">            x = theta_init_new</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">################   每次全局训练迭代，从独立同分布的Normal Gaussian采样函数     ##################</span></span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_function_from_IID_distirbution == <span class="literal">True</span> :</span><br><span class="line">            W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">            Y = torch.randn(batchsize,DIM)     <span class="comment">#代表 数据的标签 #  独立同分布的标准正太分布</span></span><br><span class="line">         </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span>:</span><br><span class="line">            state = <span class="literal">None</span></span><br><span class="line">            print(<span class="string">'reset W, x , Y, state '</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            W = W.cuda()</span><br><span class="line">            Y = Y.cuda()</span><br><span class="line">            x = x.cuda()</span><br><span class="line">            x.retain_grad()</span><br><span class="line">          </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span>  x , W , Y , state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, num_roll=<span class="number">0</span>)</span> :</span> </span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Total Training steps = Unroll_Train_Steps * the times of  `Learner` been called</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        SGD,RMS,LSTM 用上述定义的</span></span><br><span class="line"><span class="string">         Adam优化器直接使用pytorch里的，所以代码上有区分 后面可以完善！'''</span></span><br><span class="line">        f  = self.f </span><br><span class="line">        x , W , Y , state =  self.Reset_Or_Reuse(self.x , self.W , self.Y , self.state , num_roll )</span><br><span class="line">        self.global_loss_graph = <span class="number">0</span>   <span class="comment">#每个unroll的开始需要 重新置零</span></span><br><span class="line">        optimizee = self.optimizee</span><br><span class="line">        print(<span class="string">'state is None = &#123;&#125;'</span>.format(state == <span class="literal">None</span>))</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):     </span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                <span class="comment">#self.global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">              </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">              </span><br><span class="line">                update, state = optimizee(x.grad, state)</span><br><span class="line">              </span><br><span class="line">                self.losses.append(loss)</span><br><span class="line">             </span><br><span class="line">                x = x + update  </span><br><span class="line">                x.retain_grad()</span><br><span class="line">                update.retain_grad()</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.state = (state[<span class="number">0</span>].detach(),state[<span class="number">1</span>].detach())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses ,self.global_loss_graph </span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#Pytorch Adam</span></span><br><span class="line"></span><br><span class="line">            x.detach_()</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):</span><br><span class="line">                </span><br><span class="line">                optimizee.zero_grad()</span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                </span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">                </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag)</span><br><span class="line">                optimizee.step()</span><br><span class="line">                self.losses.append(loss.detach_())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses, self.global_loss_graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   LSTM 优化器的训练过程 Learning to learn   ###############</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Learning_to_learn_global_training</span><span class="params">(optimizee, global_taining_steps, Optimizee_Train_Steps, UnRoll_STEPS, Evaluate_period ,optimizer_lr=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Training the LSTM optimizee . Learning to learn</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:   </span></span><br><span class="line"><span class="string">        `optimizee` : DeepLSTMCoordinateWise optimizee model</span></span><br><span class="line"><span class="string">        `global_taining_steps` : how many steps for optimizer training o可以ptimizee</span></span><br><span class="line"><span class="string">        `Optimizee_Train_Steps` : how many step for optimizee opimitzing each function sampled from IID.</span></span><br><span class="line"><span class="string">        `UnRoll_STEPS` :: how many steps for LSTM optimizee being unrolled to construct a computing graph to BPTT.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    global_loss_list = []</span><br><span class="line">    Total_Num_Unroll = Optimizee_Train_Steps // UnRoll_STEPS</span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = optimizer_lr)</span><br><span class="line"></span><br><span class="line">    LSTM_Learner = Learner(f, optimizee, UnRoll_STEPS, retain_graph_flag=<span class="literal">True</span>, reset_theta=<span class="literal">True</span>,)</span><br><span class="line">  <span class="comment">#这里考虑Batchsize代表IID的话，那么就可以不需要每次都重新IID采样</span></span><br><span class="line">  <span class="comment">#即reset_function_from_IID_distirbution = False 否则为True</span></span><br><span class="line"></span><br><span class="line">    best_sum_loss = <span class="number">999999</span></span><br><span class="line">    best_final_loss = <span class="number">999999</span></span><br><span class="line">    best_flag = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps): </span><br><span class="line"></span><br><span class="line">        print(<span class="string">'\n=======&gt; global training steps: &#123;&#125;'</span>.format(i))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> range(Total_Num_Unroll):</span><br><span class="line">            </span><br><span class="line">            start = timer()</span><br><span class="line">            _,global_loss = LSTM_Learner(num)   </span><br><span class="line"></span><br><span class="line">            adam_global_optimizer.zero_grad()</span><br><span class="line">            global_loss.backward() </span><br><span class="line">       </span><br><span class="line">            adam_global_optimizer.step()</span><br><span class="line">            <span class="comment"># print('xxx',[(z.grad,z.requires_grad) for z in optimizee.lstm.parameters()  ])</span></span><br><span class="line">            global_loss_list.append(global_loss.detach_())</span><br><span class="line">            time = timer() - start</span><br><span class="line">            <span class="comment">#if i % 10 == 0:</span></span><br><span class="line">            print(<span class="string">'-&gt; time consuming [&#123;:.1f&#125;s] optimizee train steps :  [&#123;&#125;] | Global_Loss = [&#123;:.1f&#125;] '</span>\</span><br><span class="line">                  .format(time,(num +<span class="number">1</span>)* UnRoll_STEPS,global_loss,))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % Evaluate_period == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            best_sum_loss, best_final_loss, best_flag  = evaluate(best_sum_loss,best_final_loss,best_flag , optimizer_lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> global_loss_list,best_flag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(best_sum_loss,best_final_loss, best_flag,lr)</span>:</span></span><br><span class="line">    print(<span class="string">'\n --&gt; evalute the model'</span>)</span><br><span class="line">    STEPS = <span class="number">100</span></span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>, retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses, sum_loss = LSTM_learner()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        best = torch.load(<span class="string">'best_loss.txt'</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'can not find best_loss.txt'</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        best_sum_loss = best[<span class="number">0</span>]</span><br><span class="line">        best_final_loss = best[<span class="number">1</span>]</span><br><span class="line">        print(<span class="string">"load_best_final_loss and sum_loss"</span>)</span><br><span class="line">    <span class="keyword">if</span> lstm_losses[<span class="number">-1</span>] &lt; best_final_loss <span class="keyword">and</span>  sum_loss &lt; best_sum_loss:</span><br><span class="line">        best_final_loss = lstm_losses[<span class="number">-1</span>]</span><br><span class="line">        best_sum_loss =  sum_loss</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'\n\n===&gt; update new best of final LOSS[&#123;&#125;]: =  &#123;&#125;, best_sum_loss =&#123;&#125;'</span>.format(STEPS, best_final_loss,best_sum_loss))</span><br><span class="line">        torch.save(LSTM_Optimizee.state_dict(),<span class="string">'best_LSTM_optimizer.pth'</span>)</span><br><span class="line">        torch.save([best_sum_loss ,best_final_loss,lr ],<span class="string">'best_loss.txt'</span>)</span><br><span class="line">        best_flag = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_sum_loss, best_final_loss, best_flag</span><br></pre></td></tr></table></figure><pre><code>USE_CUDA = FalseLSTM_Optimizee_Model(  (lstm): LSTM(10, 20, num_layers=2)  (Linear): Linear(in_features=20, out_features=10, bias=True))</code></pre><h3 id="我们先来看看随机初始化的lstm优化器的效果">我们先来看看随机初始化的LSTM优化器的效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#############  注意：接上一片段的代码！！   #######################3#</span></span><br><span class="line"><span class="comment">##########################   before learning LSTM optimizee ###############################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">   </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><pre><code>reset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123202026137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=945.0716552734375,rms=269.4500427246094,adam=134.2750244140625,lstm=562912.125</code></pre><h5 id="随机初始化的lstm优化器没有任何效果loss发散了因为还没训练优化器">随机初始化的LSTM优化器没有任何效果，loss发散了，因为还没训练优化器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#######   注意：接上一段的代码！！</span></span><br><span class="line"><span class="comment">#################### Learning to learn (优化optimizee) ######################</span></span><br><span class="line">Global_Train_Steps = <span class="number">1000</span> <span class="comment">#可修改</span></span><br><span class="line">Optimizee_Train_Steps = <span class="number">100</span></span><br><span class="line">UnRoll_STEPS = <span class="number">20</span></span><br><span class="line">Evaluate_period = <span class="number">1</span> <span class="comment">#可修改</span></span><br><span class="line">optimizer_lr = <span class="number">0.1</span> <span class="comment">#可修改</span></span><br><span class="line">global_loss_list ,flag = Learning_to_learn_global_training(   LSTM_Optimizee,</span><br><span class="line">                                                        Global_Train_Steps,</span><br><span class="line">                                                        Optimizee_Train_Steps,</span><br><span class="line">                                                        UnRoll_STEPS,</span><br><span class="line">                                                        Evaluate_period,</span><br><span class="line">                                                          optimizer_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show learning process results </span></span><br><span class="line"><span class="comment">#torch.load('best_LSTM_optimizer.pth'))</span></span><br><span class="line"><span class="comment">#import numpy as np</span></span><br><span class="line"><span class="comment">#import matplotlib</span></span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Global_T = np.arange(len(global_loss_list))</span></span><br><span class="line"><span class="comment">#p1, = plt.plot(Global_T, global_loss_list, label='Global_graph_loss')</span></span><br><span class="line"><span class="comment">#plt.legend(handles=[p1])</span></span><br><span class="line"><span class="comment">#plt.title('Training LSTM optimizee by gradient descent ')</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><pre><code>=======&gt; global training steps: 0reset W, x , Y, state state is None = True-&gt; time consuming [0.2s] optimizee train steps :  [20] | Global_Loss = [4009.4] state is None = False-&gt; time consuming [0.3s] optimizee train steps :  [40] | Global_Loss = [21136.7] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [60] | Global_Loss = [136640.5] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [80] | Global_Loss = [4017.9] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [100] | Global_Loss = [9107.1]  --&gt; evalute the modelreset W, x , Y, state state is None = True......................</code></pre><p>输出结果已经省略大部分</p><h5 id="接下来看一下优化好的lstm优化器模型和sgdrmspropadam的优化性能对比表现吧">接下来看一下优化好的LSTM优化器模型和SGD，RMSProp，Adam的优化性能对比表现吧~</h5><p>鸡冻</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############  **注意： **接上一片段的代码****</span></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show contrast results SGD,ADAM, RMS ,LSTM ###############################</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> flag ==<span class="literal">True</span> :</span><br><span class="line">    print(<span class="string">'\n==== &gt; load best LSTM model'</span>)</span><br><span class="line">    last_state_dict = copy.deepcopy(LSTM_Optimizee.state_dict())</span><br><span class="line">    torch.save(LSTM_Optimizee.state_dict(),<span class="string">'final_LSTM_optimizer.pth'</span>)</span><br><span class="line">    LSTM_Optimizee.load_state_dict( torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line">    </span><br><span class="line">LSTM_Optimizee.load_state_dict(torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line"><span class="comment">#LSTM_Optimizee.load_state_dict(torch.load('final_LSTM_optimizer.pth'))</span></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): <span class="comment">#可以多试几次测试实验，LSTM不稳定</span></span><br><span class="line">    </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#p4.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><pre><code>==== &gt; load best LSTM modelreset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123204117756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=967.908935546875,rms=257.03814697265625,adam=122.87742614746094,lstm=105.06891632080078reset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123202329192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=966.5319213867188,rms=277.1605224609375,adam=143.6751251220703,lstm=109.35062408447266</code></pre><p>（以上代码在一个文件里面执行。复制粘贴格式代码好像需要Chrome或者IE浏览器打开才行？？？）</p><h2 id="实验结果分析与结论">实验结果分析与结论</h2><p>可以看到：==<strong>SGD 优化器</strong>对于这个问题已经<strong>不具备优化能力</strong>，RMSprop优化器表现良好，<strong>Adam优化器表现依旧突出，LSTM优化器能够媲美甚至超越Adam（Adam已经是业界认可并大规模使用的优化器了）</strong>==</p><h3 id="请注意lstm优化器最终优化策略是没有任何人工设计的经验">请注意：LSTM优化器最终优化策略是没有任何人工设计的经验</h3><p><strong>是自动学习出的一种学习策略！并且这种方法理论上可以应用到任何优化问题</strong></p><p>换一个角度讲，针对给定的优化问题，LSTM可以逼近或超越现有的任何人工优化器，不过对于大型的网络和复杂的优化问题，这个方法的优化成本太大，优化器性能的稳定性也值得考虑，所以这个工作的创意是独特的，实用性有待考虑~~</p><p>以上代码参考Deepmind的<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow版本</a>，遵照论文思路，加上个人理解，力求最简，很多地方写得不够完备，如果有问题，还请多多指出！</p><p><img src="https://img-blog.csdnimg.cn/20181125191428527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"> 上图是论文中给出的结果，作者取最好的实验结果的平均表现（试出了最佳学习率？）展示，我用下面和论文中一样的实验条件（不过没使用NAG优化器），基本达到了论文中所示的同样效果？性能稳定性较差一些</p><p>（我怀疑论文的实验Batchsize不是代码中的128？又或者作者把batchsize当作函数的随机采样？我这里把batchsize当作确定的参数，随机采样单独编写）。</p><h2 id="实验条件">实验条件：</h2><ul><li>PyTorch-0.4.1 cpu</li><li>优化问题为Quadratic函数:</li><li>W : [128,10,10] Y: [128 , 10] x: [128, 10] 从IID的标准Gaussian分布中采样，初始 x = 0</li><li><p>全局优化器optimizer使用Adam优化器， 学习率为0.1（或许有更好的选择，没有进行对比实验）</p></li><li>CoordinateWise LSTM 使用LSTM_Optimizee_Model：<ul><li>(lstm): LSTM(10, 20, num_layers=2)</li><li>(Linear): Linear(in_features=20, out_features=10, bias=True)</li></ul></li><li>==未使用任何数据预处理==（LogAndSign）和后处理</li><li>UnRolled Steps = 20 Optimizee_Training_Steps = 100</li><li>*Global_Traing_steps = 1000 原代码=10000，或许进一步优化LSTM优化器，能够到达更稳定的效果。</li><li><p>另外，原论文进行了mnist和cifar10的实验，本篇博客没有进行实验，代码部分还有待完善，还是希望读者多读原论文和原代码，多动手编程实验！</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">()</span>:</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"知识"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_to</span><span class="params">(learn)</span>:</span></span><br><span class="line">    print(learn())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"学习策略"</span></span><br><span class="line"></span><br><span class="line">print(learning_to(learn))</span><br></pre></td></tr></table></figure><h2 id="后叙">后叙</h2><blockquote><p>人可以从自身认识与客观存在的差异中学习，来不断的提升认知能力，这是最基本的学习能力。而另一种潜在不容易发掘，但却是更强大的能力--在学习中不断调整适应自身与外界的学习技巧或者规则--其实构建了我们更高阶的智能。比如，我们在学习知识时，我们总会先接触一些简单容易理解的基本概念，遇到一些理解起来困难或者十分抽象的概念时，我们往往不是采取强行记忆，即我们并不会立刻学习跟我们当前认知的偏差非常大的事物，而是把它先放到一边，继续学习更多简单的概念，直到有所“领悟”发现先前的困难概念变得容易理解</p></blockquote><blockquote><p>心理学上，元认知被称作反省认知，指人对自我认知的认知。弗拉威尔称，元认知是关于个人自己认知过程的知识和调节这些过程的能力：对思维和学习活动的知识和控制。那么学会适应性地调整学习策略，也成为机器学习的一个研究课题，a most ordinary problem for machine learning is that although we expect to find the invariant pattern in all data, for an individual instance in a specified dataset，it has its own unique attribute, which requires the model taking different policy to understand them seperately .</p></blockquote><blockquote><p>以上均为原创，转载请注明来源<br>https://blog.csdn.net/senius/article/details/84483329 or https://yangsenius.github.io/blog/LSTM_Meta/ 溜了溜了</p></blockquote><h2 id="下载地址与参考">下载地址与参考</h2><p><strong><a href="https://yangsenius.github.io/blog/LSTM_Meta/learning_to_learn_by_pytorch.py" target="_blank" rel="noopener">下载代码: learning_to_learn_by_pytorch.py</a></strong></p><p><a href="https://github.com/yangsenius/learning-to-learn-by-pytorch" target="_blank" rel="noopener">Github地址</a></p><p>参考： <a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">1.Learning to learn by gradient descent by gradient descent</a> <a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">2. Learning to learn in Tensorflow by DeepMind</a> <a href="https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2" target="_blank" rel="noopener">3.learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2</a></p><p><em>目录</em> [TOC]</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原创博文，转载请注明来源&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;“浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！”&lt;/p&gt;
&lt;p&gt;Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公式符号，初学者很容易看不下去，而比较好的方法就是先浏览目录，掌握一些简单的概念（回归与分类啊，监督与无监督啊），并在按顺序的阅读过程学会“前瞻”与“回顾”，进行快速学习。又比如在早期接受教育的学习阶段，盲目的“题海战术”或死记硬背的“知识灌输”如果不加上恰当的反思和总结，往往会耗时耗力，最后达到的效果却一般，这是因为在接触新东西，掌握新技能时，是需要“技巧性”的。&lt;/p&gt;
&lt;p&gt;从学习知识到学习策略的层面上，总会有“最强王者”在告诉我们，“钻石的操作、黄铜的意识”也许并不能取胜，要“战略上最佳，战术上谨慎”才能更快更好地进步。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
      <category term="Reproduce" scheme="http://senyang-ml.github.io/tags/Reproduce/"/>
    
      <category term="PyTorch" scheme="http://senyang-ml.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Associative embedding End-to-End Learning for Joint Detection and Grouping</title>
    <link href="http://senyang-ml.github.io/2018/07/12/AE-hourglass/"/>
    <id>http://senyang-ml.github.io/2018/07/12/AE-hourglass/</id>
    <published>2018-07-12T01:52:16.000Z</published>
    <updated>2020-05-29T05:30:46.446Z</updated>
    
    <content type="html"><![CDATA[<p>2018.04.07：</p><h2 id="论文阅读">论文阅读</h2><p>这篇文章的核心思想是比较精炼概括的，它的亮点是用一个框架解决了在计算机视觉中常见的任务中经常遇到的两个通用环节：Detection and Grouping，用中文来讲就是，检测（小的视觉单元作为候选）和（根据得分）重组（一个合理的结构）。 <a id="more"></a></p><p>从以下的视觉任务中可以体现：</p><ul><li><p><strong>人体姿态估计问题</strong>：一般按照bottom-to-up的方式，先检测出body key points然后按照约束来组合完整的人体，但多人姿态估计的问题又衍生出另一种up-down的方式，就是先检测出单个人体再识别其姿态，比如Mask R-CNN, RMPE等方法。</p></li><li><strong>目标检测</strong>：往往先寻找不同位置和尺度下的bounding boxes，然后打分筛选</li><li><strong>实例分割</strong>：寻找相关联的像素，然后将像素合理重组成物体实例（mask）。</li><li><p><strong>多目标追踪</strong>：检测物体实例，重组其运动轨迹</p></li></ul><p>这些方式，本质上都符合人类自身视觉从部分认识整体，以整体推理部分的直觉。以往的工作都认识到这一点，只是这篇论文做了一次提炼概括了，并指出了一个问题：</p><p>以往的两步策略（detection ﬁrst and grouping second）忽略了两个环节之间内在的紧密耦合。</p><blockquote><p>（在之前看的CMU的Realtime Multi-Person2D Pose Estimation using Part Affinity Fields, 他们的论文当中，除了人体关键点作为监督信息外，还引入了Part Affinity Fields，也就是和肢体方向保持一致的单位向量作为监督信号，我的感觉是，这实际上就是没有充分利用两个环节上的耦合性，或者说是人体关键点与肢体连接的耦合性信息，毕竟人体的关节与整体的关系是统一的， 而OpenPose用的是寻找最佳的图匹配的方式，但同时将关键点位置，和肢体向量同时作为监督信息，会导致信息冗余，增加复杂度吧？所以我觉得作者这种融合两步的思想就很实际，很前卫）</p></blockquote><p>所以作者针对多人体姿态估计，将两步工作融入到一个框架里，即在一般的输出Heatmap层，附加了一层作为“tag“（也就是论文提到的embedding的含义），并设计了一个grouping loss作为监督关键点是否分配给了正确的人体的函数。论文巧妙的地方就是没有给“tag”赋予”ground-truth”来作为强监督，而是用“tag“值的相似与差异来表示多个人体。用于预测Heatmap的网络架构基于作者之前的工作“Stacked Hourglass”.</p><p>论文中Related work中的Perceptual Organization的叙述部分，给我了比较多的启示：</p><blockquote><p>Perceptual Organization是感知组织的意思，我理解成人类在认识事物或概念所遵循的层级组织关系。所谓的强人工智能，就需要解决这一棘手问题吧。作者提到了这一工作涉及到的许多任务，有Figure–ground segmentation (perception)，hierarchical image parsing， spectral clustering，conditional random ﬁelds，generative probabilistic models等等一系列问题，这些方法都遵循，从pre-detect visual units到measure affinity，再到grouping，但是目前没有统一到一个统一的架构上来，作者就是从这角度出发，不加一些额外的设计来完成一个端到端的网络架构。作者提到了图像层级解析，特别符合人类认知图像，所以，作者的Hourglass模块设计成沙漏状，先是压缩图像，获得全局信息，然后利用全局信息与低层特征融合，输出一个与同样大小的heatmap，其实就是想将这样的层级解析的思想间接地蕴含在内，只不过网络的训练将这些信息都隐含在了参数里，无法与人的解析思路类比.</p></blockquote><p>详情查看博客 <a href="https://yangsenius.github.io/blog/embedding/" target="_blank" rel="noopener" class="uri">https://yangsenius.github.io/blog/embedding/</a></p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原创, 禁止转载</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018.04.07：&lt;/p&gt;
&lt;h2 id=&quot;论文阅读&quot;&gt;论文阅读&lt;/h2&gt;
&lt;p&gt;这篇文章的核心思想是比较精炼概括的，它的亮点是用一个框架解决了在计算机视觉中常见的任务中经常遇到的两个通用环节：Detection and Grouping，用中文来讲就是，检测（小的视觉单元作为候选）和（根据得分）重组（一个合理的结构）。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pose machine：Articulated Pose Estimation via Inference Machines</title>
    <link href="http://senyang-ml.github.io/2018/06/01/pose-machine-eccv2014/"/>
    <id>http://senyang-ml.github.io/2018/06/01/pose-machine-eccv2014/</id>
    <published>2018-06-01T01:52:16.000Z</published>
    <updated>2020-05-24T04:06:10.042Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pose-machinearticulated-pose-estimation-via-inference-machines">Pose machine：Articulated Pose Estimation via Inference Machines</h1><p>由显式的图模型推断人体铰链模型到隐式地表达丰富的多部件关系的转折点！</p><h2 id="介绍">介绍</h2><p>估计铰链式人体姿态的复杂度来源有两个：</p><ul><li>潜在骨架模型的自由度（20大约）导致的高位配置空间的搜索范围</li><li>图像中人体姿态的多变性</li></ul><p>传统的图模型，基于树结构或者星状结构的简单图模型难以捕捉多部件之间的关系和依赖性，而且容易导致双重计数错误产生。 非树模型需要近似推断来解决，参数学习非常复杂！ <a id="more"></a> 图模型的第二个困难的地方是： &gt; A second limitation of graphical models is that deﬁning the potential functions requires careful consideration when specifying the types of interactions. This choice is usually dominated by parametric forms such as simple quadratic models in order to enable tractable inference [1]. Finally, to further enable eﬃcient inference in practice, many approaches are also restricted to use simple classiﬁers such as mixtures of linear models for part detection [5]. These are choices guided by tractabilty of inference rather than the complexity of the data. Such tradeoﬀs result in a restrictive model that do not address the inherent complexity of the problem.</p><p>论文建立了<strong>一个类似于场景解析的层级推断机制来估计人体姿态</strong></p><h2 id="pose-machine">Pose machine</h2><p>它是一个序列预测算法，它<strong>效仿了信息传递的机制</strong>来预测每一个人体部件的置信度，在每个阶段迭代提升它的预测能力</p><ul><li><p>1.它联合了多变量之间的交互关系</p></li><li><p>2.它从数据中学习空间关系，而不需要精确的参数模型</p></li><li><p>3.它模块化的架构能够允许使用高承载的预测器来处理不同形状的姿态</p></li></ul><p>解决在传统模型人体姿态估计的两大难点。</p><p>可以这么说，pose machine最重要的想法就是：运用级联，将对每个单独部件的估计信息结合起来，从中提取语义特征再进一步地精确估计下个阶段的每个部件位置。这个思想是开创性的，它不仅符合人类的直觉，并且实验证明是有提升效果的，后来的CPM，用卷积网络的架构准确实现了这一方法，取得了当时的最好效果，对各种形状的人体姿态都有帮组，在2016年，CPM在MPII,LSP,FLIC数据库上都到达了最好表现，而近年来成功的关于姿态估计的所有论文都几乎蕴含了这一思想：试图归纳出图中全局的信息作为下个级联阶段的预测，比如stacker hourglass、cpn</p><p>本博客详见 <a href="https://yangsenius.github.io/blog/pose-machine/" target="_blank" rel="noopener">Pose-Machine</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;pose-machinearticulated-pose-estimation-via-inference-machines&quot;&gt;Pose machine：Articulated Pose Estimation via Inference Machines&lt;/h1&gt;
&lt;p&gt;由显式的图模型推断人体铰链模型到隐式地表达丰富的多部件关系的转折点！&lt;/p&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;估计铰链式人体姿态的复杂度来源有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;潜在骨架模型的自由度（20大约）导致的高位配置空间的搜索范围&lt;/li&gt;
&lt;li&gt;图像中人体姿态的多变性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;传统的图模型，基于树结构或者星状结构的简单图模型难以捕捉多部件之间的关系和依赖性，而且容易导致双重计数错误产生。 非树模型需要近似推断来解决，参数学习非常复杂！&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Causal inference(因果推断)</title>
    <link href="http://senyang-ml.github.io/2018/05/20/inference/"/>
    <id>http://senyang-ml.github.io/2018/05/20/inference/</id>
    <published>2018-05-20T01:52:16.000Z</published>
    <updated>2020-05-24T03:47:45.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推断的介入">推断的介入</h1><p>善有善报，恶有恶报。</p><p>虽然世间并没有将这份准则执行地那么恰如其分，但是万物归根结底，必然有着因果之间的联系。</p><p>而人生匆匆，为什么愚蠢的我们总是在苦苦寻觅事情的答案，why？</p><p>剪不断，理还乱，只好奉上因果推断</p><a id="more"></a><h2 id="因果推断">因果推断</h2><p>在机器学习领域，因果推断，是在图模型中被着重研究的一种理论。</p><p>2017年NIPS会议上，<a href="https://mp.weixin.qq.com/s/ytt8Hwv4JHZdV4649XG6Xw" target="_blank" rel="noopener">Judael Pearl</a>发表了机器学习的局限性演讲，并提出了因果推断的三个层次：</p><table><colgroup><col style="width: 43%"><col style="width: 37%"><col style="width: 18%"></colgroup><thead><tr class="header"><th></th><th>公式</th><th>含义</th></tr></thead><tbody><tr class="odd"><td><strong>观察</strong></td><td><span class="math inline">\(P\left( y\mid x \right)\)</span></td><td>Seeing: what is？ 观察到x时，会如何影响到y的信念</td></tr><tr class="even"><td><strong>介入</strong></td><td><span class="math inline">\(P\left(y\mid do(x) \right)\)</span></td><td>Doing: what if？ 如果做出对x的介入时，会如何影响到y的信念</td></tr><tr class="odd"><td><strong>反事实</strong></td><td><span class="math inline">\(P\left(y_{x}\mid x^{,},y^{,} \right)\)</span></td><td>Imaging: why? 是x影响到y的吗，基于当下，假设发生的是x^，会对y产生何种影响</td></tr></tbody></table><p>为来形象直观地理解Judael Pearl因果推断三层次，我举一个简单的追求异性的例子，它深刻蕴含着这三个推断层次：</p><p><strong>观察</strong>：在追求一位异性时，不管你是情场高手，还是恋爱小白，你必须承认，我们都会收集足够多的关于他/她的信息x，来作为判断能否成功追求到他/她的依据，也许有的人会认真考虑这个问题，（如果发现成功率低就会直接选择放弃，而选择不再追，其实有点可惜了，因为这仅仅处于<strong>观察</strong>阶段），而有的人在直觉上就已经做出了估计。这也就是我们基于已经观察到的信息x，来评估成功追到他/她的概率<span class="math inline">\(P\left( y\mid x \right)\)</span></p><p><strong>介入</strong>：在追求异性时，我们必须有所行动，仅仅是观察并不能影响到成功追到他/她的概率。理所当然，我们所作出的行动一般都会遵循一个显而易见的行动目标或指南：做一些有意义的事情 <span class="math inline">\(do(x)\)</span>来增加能成功追求到他/她的概率<span class="math inline">\(P\left(y\mid do(x) \right)\)</span>，我们会尝试一些方法，来判断这样做是会引起对方的反感，还是能增加对自己的好感，进而影响成功追求到他/她的概率（如果你发现无论 <span class="math inline">\(do(whatever)\)</span>，总不能提高<span class="math inline">\(P\left( y\mid do(whatever) \right)\)</span>，那么事实就会证明，放弃可能是比较明智的选择）</p><p><strong>反事实</strong>： 在追求异性遭遇挫败时，或者反思自己的追求历程时，我们往往会基于当前的态势做出一些假设，要是当初我那样去做<span class="math inline">\(x^{,}\)</span>而不是做<span class="math inline">\(x\)</span>，会不会就更有可能把他/她给追到呢 <span class="math inline">\(P\left( y_{x}\mid x^{,},y^{,} \right)\)</span> 。这样的假设才会让我们总结失败或者成功的真正原因，找出哪些才是真正影响我们最后能否追求成功的因素。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然而在现实生活中，上述的推断分析还是考虑地过于单纯、局限，因为情感问题不是靠推断就能解决的。</span><br></pre></td></tr></table></figure><p>这样的推断，在大多数情况下，会更多地出现在我们的直觉当中，所以见到这样的理论你也会见怪不怪，但是对于机器而言，如何向用数学的语言来描述这个推断过程，确实一件极其困难的事情，更何况想让计算机能够自动计算这种推断。<strong>而当下首先要解决的是如何用合理的计算机语言来描述这个推断过程</strong>。</p><p>基于这三个层次的因果推断属于较为高级的人类智能，而目前的机器学习仅仅停留在第一个阶段，就是由观察到信念的阶段，这就是当下机器学习的局限性所在：可解释差，没有引入人类的合理性推断在里面。那么，是否可以在这方面做出更多的思考呢？</p><h2 id="概念假设推断">概念假设推断</h2><p>下表是我根据反事实理论设想的<strong>概念假设推断表</strong>，可以将概念层级联系在一起。</p><table><colgroup><col style="width: 41%"><col style="width: 23%"><col style="width: 17%"><col style="width: 17%"></colgroup><thead><tr class="header"><th></th><th>原始数据</th><th>K个表示中间概念的特征</th><th>高层概念表示</th></tr></thead><tbody><tr class="odd"><td>Groud-truth</td><td>X</td><td><span class="math inline">\(Z^{*}:z^{*} \left( 1 \right)=g^{*}_1,...,z^{*} \left( K \right)=g^{*}_K\)</span></td><td><span class="math inline">\(G^{*}\)</span></td></tr><tr class="even"><td>Learning</td><td>X</td><td>观察单个显著特征 <span class="math inline">\(z \left( i \right)=g_i\)</span></td><td>显著特征下最容易激活的高层概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span></td></tr><tr class="odd"><td>Inference</td><td>X</td><td><span class="math inline">\(\max_{\Theta }P(Z=Z^{*}\mid z^{*}\left( i \right)=z \left( i \right)=g_i,G^{*}=G_{z \left( i \right)=g_i};\Theta)\)</span></td><td><span class="math inline">\(G\)</span></td></tr></tbody></table><p><span class="math display">\[\max_{\Theta }P\left(Z=Z^{*}\mid z^{*}\left( i \right)=z \left( i \right)=g_i,G^{*}=G_{z \left( i \right)=g_i};\Theta\right)\]</span></p><p>公式的含义，表示：数据信息X在传递到中间层时，如果某一属性实体 <span class="math inline">\(z \left(i \right)=g_i\)</span>（弱概念）处于激活态时，那么这一属性实体会导致具有相似属性的更高层概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 处于活跃状态，来进行反向印证概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 激活时对应的 的中间层其他属性的一致性： 即此时真实观测的 <span class="math inline">\(z \left( 1 \right),...,z \left( K \right)\)</span> 与 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 激活时其他属性值的期望值 <span class="math inline">\(z^{*}\left( 1 \right),...z^{*} \left( K \right)\)</span> 之间的差异程度，差异越小，那么 <span class="math inline">\(Z\)</span> 越接近于<span class="math inline">\(Z^{*}\)</span>，即优化目标，满足了假设推断。</p><h2 id="怎么将推断应用到当前的机器学习">怎么将推断应用到当前的机器学习？</h2><p>用一个实际的例子来说明，比如我们在一幅图像中检测到了手指的存在，那么我们会推断是一个手臂会大概率的存在，然后我们检测图像中是否有在手臂激活时肩膀、上臂、前臂、肘关节的存在，如果这几个中级概念属性一致，那么我们可以大致推断出这里有张手臂。这种做法的好处，就是概念可以类似积木式地向上堆叠，比如我们观测到手臂后，可以推断是否为人类，来进一步观察是否有毛发、手臂颜色如何、是否有躯干形状、头部形状等等，同理可以类似推断其他物体。而关于中间概念的检测，可以依赖神经网络预测。至于推断层面，不需要神经网络来解决！</p><p>而问题的难点在于中间概念的检测，目前的机器学习都没有去解决这个问题，往往是低层像素程度的特征抽取和高层标签信息的分类，介于两者之间的中间概念是被忽略的。</p><p>而目前有工作，通过CNN可视化在尝试恢复中间层的特征信息。需要思考的两点是：1.如何表示中间特征可视化得到的信息 2.信息从前到后是一个压缩、去噪的有损失过程，怎么能够继续用图像可视化，这是违背了信息传递的基本事实，除非引入了新的信息或者融合了原始信息。</p><p><strong>这是一个开放式的问题</strong></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;推断的介入&quot;&gt;推断的介入&lt;/h1&gt;
&lt;p&gt;善有善报，恶有恶报。&lt;/p&gt;
&lt;p&gt;虽然世间并没有将这份准则执行地那么恰如其分，但是万物归根结底，必然有着因果之间的联系。&lt;/p&gt;
&lt;p&gt;而人生匆匆，为什么愚蠢的我们总是在苦苦寻觅事情的答案，why？&lt;/p&gt;
&lt;p&gt;剪不断，理还乱，只好奉上因果推断&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Causal Inference" scheme="http://senyang-ml.github.io/tags/Causal-Inference/"/>
    
      <category term="Probability Graph Model" scheme="http://senyang-ml.github.io/tags/Probability-Graph-Model/"/>
    
  </entry>
  
  <entry>
    <title>The limitation of thinking with &#39;label&#39; in Deep Learning (深度学习中标签思维的局限)</title>
    <link href="http://senyang-ml.github.io/2018/04/29/label_limitation/"/>
    <id>http://senyang-ml.github.io/2018/04/29/label_limitation/</id>
    <published>2018-04-29T03:52:16.000Z</published>
    <updated>2020-05-24T05:42:57.740Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习中标签思维的局限">深度学习中标签思维的局限</h1><p>在生活中，聪明的人常常告诫我们，不要给事物轻易下定义，不要轻易给他人打上标签，因为“标签”的一种贬义替代词汇是“偏见”。然而目前的机器学习正利用人类的标签思维进行学习，所以机器学习到头来，学习到的都是我们的偏见吗？</p><h2 id="端到端是标签思维的一种表现">端到端是标签思维的一种表现</h2><p>我们首先来谈论一下端到端的学习。什么是端到端学习？简单来讲就是，机器学习一个从特定输入到特定输出的过程。端到端的学习应该在深度学习产生后进入了巅峰状态。对物体特征的提取技术由手工设计，如（HOG、Har、SIFT、LBP等），被自动提取技术（CNN）所替代了，少了人工的介入，那么端到端就能够轻易实现了。但实际上，端到端是一种局限的智能，为什么说它是局限的？一方面，它导致了模型的黑箱化，不可解释性，即我们不能认知中间特征数据所表示的内涵，另外，一个样本的信息量是巨大的，若是从不同的角度去解读，提取它的内涵就会得到不同的“标签”，甚至可以提取出内涵对立的标签（<strong>比如，如果我们特意地让机器去区分人类和动物的差异，机器就不能学习到，人类本身就是动物，这一基本事实</strong>），然而机器将大量相似的图像进行归纳学习为一个单独的标签，那么模型学到的是一种呆板的、偏见的抽象，这绝对不是真正的智能，就像Judeal Pearl说的如今的机器学习就是曲线拟合而已。</p><h2 id="标签思维导致对抗样本的产生">标签思维导致对抗样本的产生</h2><p><a href="https://www.jiqizhixin.com/articles/2018-01-06-6" target="_blank" rel="noopener">近来的研究表明</a> DNN 容易受到对抗样本（adversarial example）的影响：在输入中加入精心设计的对抗扰动（adversarial perturbation）可以误导目标 DNN，使其在运行中给该输入加标签时出错。</p><p>一个常见的对抗攻击来自于2014年Goodfellow提出的Fast Gradient Sign Method(FGSM),它通过在梯度方向上进行添加增量来诱导网络对生成的图片X’进行误分类 <span class="math display">\[ x^{,}=x+\varepsilon sign(\bigtriangledown _{x} J(\theta ,x,y)) \]</span></p><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/FGSM.png" width="800"></center><a id="more"></a><p>在图片样本加上精心设计的噪声干扰后，机器会以99.3%的置信度认为图片中动物是<strong>长臂猿</strong>，而不是<strong>熊猫</strong>！</p><p>当在实际世界中应用 DNN 时，这样的对抗样本就会带来安全问题。比如，加上了对抗扰动的输入可以误导自动驾驶汽车的感知系统，使其在分类道路标志时出错，从而可能造成灾难性的后果。</p><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/label_stop.png" width="800"></center><p>谷歌团队的这篇论文<a href="https://arxiv.org/pdf/1801.02774.pdf" target="_blank" rel="noopener">Adversarial Spheres</a>，从<a href="http://mp.weixin.qq.com/s/x5rTpvvCfABWWkjpgnJ5BA" target="_blank" rel="noopener">流形的角度</a>探讨了对抗样本的产生</p><center><img src="http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8XROWibCiaB0victgLYCDcbYyynbLxe0svCF0f7ePuIOpTicBH4wDGS2kibYXJ0HVrQSuOwcAFQIgPjRA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" width="800"></center><p>对抗样本的破坏能力之所以强大，是因为DNN就没有根本理解图像的内涵，它把标签实际当成了图像中一种可以进行量化的真实属性值，学到的是这个“属性”和其他层次属性（如颜色，纹理，布局，边缘等）的映射关系，这种手工标签和智能所理解的“标签”是有本质的区别的。</p><ul><li><p>手工设计的标签:表示通过设定阈值得到的属性概念的对立程度。比如即图像中一只狗，这种手工标签表示的是“是狗”和“不是狗”这两种概念的对立程度</p></li><li><p>人类智能的“标签”：表示和很多低层信息，中层概念紧密相连，甚至同时存在的一种整体概念连接关系，而不是单纯的表示概率的标量。我们提到标签“狗”时，这个“狗”绝不是单个标量，而是和动物、摇尾巴、宠物、躯干、四肢、舌头、毛发等并行存在的概念。</p></li></ul><p>对于同一种“标签”来说，它的图像差异可能非常大，甚至出现对立，如果你用深度学习去学习像素到标签的映射是非常困难，难以收敛的，或者容易过拟合，所以一些被认为是正常的标签，模型并不认识。</p><p>还有，幽默理论认为：人之所以会笑，是因为突然觉察到概念和实物之间关系的不协调，笑就是对这种不协调之间的反映，所以事物的概念并不是一种固化的东西。</p><h2 id="对抗样本与标签空间">对抗样本与“标签”空间</h2><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/duikang.png" width="50%"></center><p>对抗样本的产生函数：就是利用了标签思维的局限性，利用这一弱点，设计出标签为 A，但是与B（C,D……)类的像素度量距离更近的样本（即，肉眼观察更接近B,C,D,或者指定如C）。</p><p>一般的神经网络其实也是在高维数据空间，寻找多个分界面，作为类与类的区别</p><p>无论模型训练出何种程度，一定可以在高维空间的分界面处寻找到A的对抗样本,数据维度越高，对抗样本在展开维空间上越相似于A，但实际却属于B，所以增加模型参数，有利于寻找对抗样本</p><p>标签思维，就是在扭曲、折叠客观实在的空间中，寻找一个高维流形球，球内球外，作为一个“标签”的判断依据，所以你在图像上，描一个轻微的线，只要你能将被加了一条线的图像样本在扭曲空间中实现一次在分界流形球面的“超球面穿越”，那么这个加工后的图片就能成为对抗样本。所以对抗样本的存在与标签思维训练是一个硬币的正反面，是共同存在的，当下所有用标签训练出来的深度学习模型，无论它声称的鲁棒性有多好，在理论上都是存在一些对抗样本能够完全地混淆模型的预测，如<a href="https://arxiv.org/pdf/1801.02774.pdf" target="_blank" rel="noopener">Adversarial Spheres</a>中所述。</p><h2 id="从标签到实体概念如何表征">从标签到实体，概念如何表征？</h2><p>智能需要多种定义，一种泛化的概念，打破局限的标签思维。标签应该从概念实体中产生，角度不同，标签不同 Hinton2017年提出的capsules，最后一层用活动向量来表示实体，实际上就是从标签思维到概念实体思维的一种突破。另外，深度学习去自主学习低层特征，而不是人工设计特征，这一点是可取的，但是对特征的利用环节，应当引入更多可以设计的接口。这是因为我们人脑的处理信息过程中，从底层的像素信息不是直接映射到概念，中间存在很多层次化的结构，在DNN中是一种粗暴的学习，所以对抗样本才会抓住这一漏洞。而加入一些可介入的中层概念接口，就会提升网络的鲁棒性。</p><h2 id="单一标签描述图片是否可靠">单一标签描述图片是否可靠？</h2>我们先看下列6幅图片，从一般的标签思维出发，其中有4张可以被认为是狗，另外2张被认为是海狮，但是其中几张并不是纯粹的动物照片，而是加入了人为的干扰因素，比如狗穿上了人的服装或者场景的差异。<center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/1.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/2.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/3.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/4.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/5.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/6.png" width="50%"></center><p>以上的所有图片，如果按照 标签思维 来分类的话，那么一共有两类：狗和海狮，这两类是如何产生的呢？</p><p><strong>首先</strong>，我们最关注的是图像的主题信息，也就是包含信息最多的部分，一般占据了图像大部分面积，然后我们发现主题信息都是动物，那么动物的种类就完全作为了我们给图像标签的依据（<strong>这是机器学习不到的依据</strong>）</p><p><strong>然后</strong>根据我们的经验，我们通过，耳朵，四肢，毛发长度，脸部五官的分布来区分两个种类。但是其中两幅图有干扰信息，比如人的衣服，帽子，眼镜，这样的干扰对小样本学习来说是致命的，机器很难去忽略此类明显信息而只关注主体，它们让类内差异变得巨大，而第一幅狗的图和两张海狮图，主体信息很明确，基本没有什么干扰，而让机器直接去关注主体的特征之间的差异，甚至可能将其归为一类物体。只有在样本足够足够大时，机器才能学会忽略次要信息，来关注主题信息，这也就是为什么目前的机器学习是需要大数据作支撑的，而非人类的小样本学习。 &gt; 它们都属于动物的这一基本事实，却完全不能用深度网络所学习到的标签表示，也就是说，深度学习训练用到的标签思维，完全是，先利用人类认识事物的基本属性的先验后，再利用图像像素层面寻找数据集中数据分布的差异，然后用几个标量值来表示这种差异的程度，</p><p>这也就是我们 标签思维 去训练物体的固有缺陷，总结一下人类所谓的“标签思维”：</p><ul><li><p>人类的标签思维，是忽略次要信息，只关注主体特征间的类内差异和类间差异，但我们很难去告诉机器，要去忽略哪种次要信息，尤其是当次要信息与主体特征信息难以分开时，次要信息会增加类间差异，导致训练模型过拟合</p></li><li>人类的标签思维，本质是从事物的单一属性出发，是刻板僵化的，是智能的片面反映。图像信息在不同角度的解读下，具有不同属性，如果你将上述图像分为：人类装扮的 和 非人类装扮的，那么可以将其中的两张图归为人类装扮类，剩余的为非人类装扮类。那么这种分类标准和动物种类有什么本质区别吗？</li><li><p>人类的标签思维，是概念层级某一层上的实例描述。这句话什么意思呢？举个例子，在生物分类学上，存在一个层级型的划分依据，把自然界的生物按照：<strong>界门纲目科属种</strong>的层次结构划分，比如，黑猩猩物种 界：动物界 门：脊索动物门 纲：哺乳纲 目：灵长目 科：猩猩科 属：黑猩猩属 种：黑猩猩。而我们脑海中“黑猩猩”的标签，是在一个最常见的认知层次上去认知的，如果它和一个汽车放在一起，我们也可以认为它属于“动物”标签，如果它和牛放在一起，我们可以认为它是“哺乳动物”标签，它和猿猴放在一起，我们认为它是“黑猩猩”标签。那么机器学习，该如何胜任这种划分？它能同时学到，黑猩猩，既有“动物”标签，也有“哺乳动物”标签，还有“黑猩猩”标签吗？</p></li></ul><p>我想是不太容易的。其实事物本身的存在是客观的，而标签是主观产生的，标签来源客观事物从不同角度出发的一种属性。如果按照标签思维去训练第一种分类标准，是得不到第二种分类的判断，反之亦成立，所以打破这种固有缺陷，就必须从实体角度出发，而就是将多种概念属性与实体联合，让机器学到的是一个戴眼镜穿衣服的狗，一个不戴眼镜不穿衣服游泳的海狮。</p><p><strong>概括地讲，人类的标签思维是为视觉感知与语言表达需要的一种折衷，标签最大的用处在于交流与储存，而对于视觉感知来讲，“标签”是一种片面的图像表征，对于智能机器来讲，它应该学到编码更多信息的层级表征，而不单纯是标签</strong></p><h2 id="faster-rcnn系列深度网络成功的直观原因">Faster RCNN系列深度网络成功的直观原因</h2><p>2018年5月23日补充： Faster-RCNN的设计成功很大程度上取决于，多个环节利用了人类视觉本身处理目标的先验能力，比如：</p><ul><li>我们通过大致扫一眼来看到目标物体大概在那个位置--RPN网络</li><li>我们在候选出的区域具体判定区域内的物体是什么类别--Fast RCNN网络</li></ul><p>在faster-rcnn里面，Vgg-16网络预训练出的 特征图(feature map)实际表征了一种数值化的中间抽象概念，而从表征中间抽象概念到标签概念的信息传递是一个有损失的信息降噪、压缩编码过程。这符合人类本身的对事物的认知：试图从事物中取概括一般的内涵，这种内涵在丰富的外延下，保持着鲁棒不变性，反映在图像的像素到抽象概念上，就是从数值表示含义上的不断抽象。</p><p>端到端的学习，让中间产生的语义性、概念性的特征或者表征，不在可见，限制了网络推理的功能，faster-rcnn所具备的思想实际在端到端的过程上利用了一些人类的先验知识，它将先验知识反映在网络架构的设计上，所以下一步的网络应该是一种特征概念开放式的网络，在此基础上可以进行因果推断</p><p><strong>总结一下，标签本质是一种偏见，是一种局限的智能，标签思维的引入使得机器学习面临着对抗样本的考验。而要解决这一问题，一方面需要重新定义事物的概念，另一方面，需要人的推断来介入</strong></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;深度学习中标签思维的局限&quot;&gt;深度学习中标签思维的局限&lt;/h1&gt;
&lt;p&gt;在生活中，聪明的人常常告诫我们，不要给事物轻易下定义，不要轻易给他人打上标签，因为“标签”的一种贬义替代词汇是“偏见”。然而目前的机器学习正利用人类的标签思维进行学习，所以机器学习到头来，学习到的都是我们的偏见吗？&lt;/p&gt;
&lt;h2 id=&quot;端到端是标签思维的一种表现&quot;&gt;端到端是标签思维的一种表现&lt;/h2&gt;
&lt;p&gt;我们首先来谈论一下端到端的学习。什么是端到端学习？简单来讲就是，机器学习一个从特定输入到特定输出的过程。端到端的学习应该在深度学习产生后进入了巅峰状态。对物体特征的提取技术由手工设计，如（HOG、Har、SIFT、LBP等），被自动提取技术（CNN）所替代了，少了人工的介入，那么端到端就能够轻易实现了。但实际上，端到端是一种局限的智能，为什么说它是局限的？一方面，它导致了模型的黑箱化，不可解释性，即我们不能认知中间特征数据所表示的内涵，另外，一个样本的信息量是巨大的，若是从不同的角度去解读，提取它的内涵就会得到不同的“标签”，甚至可以提取出内涵对立的标签（&lt;strong&gt;比如，如果我们特意地让机器去区分人类和动物的差异，机器就不能学习到，人类本身就是动物，这一基本事实&lt;/strong&gt;），然而机器将大量相似的图像进行归纳学习为一个单独的标签，那么模型学到的是一种呆板的、偏见的抽象，这绝对不是真正的智能，就像Judeal Pearl说的如今的机器学习就是曲线拟合而已。&lt;/p&gt;
&lt;h2 id=&quot;标签思维导致对抗样本的产生&quot;&gt;标签思维导致对抗样本的产生&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2018-01-06-6&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;近来的研究表明&lt;/a&gt; DNN 容易受到对抗样本（adversarial example）的影响：在输入中加入精心设计的对抗扰动（adversarial perturbation）可以误导目标 DNN，使其在运行中给该输入加标签时出错。&lt;/p&gt;
&lt;p&gt;一个常见的对抗攻击来自于2014年Goodfellow提出的Fast Gradient Sign Method(FGSM),它通过在梯度方向上进行添加增量来诱导网络对生成的图片X’进行误分类 &lt;span class=&quot;math display&quot;&gt;\[ x^{,}=x+\varepsilon sign(\bigtriangledown _{x} J(\theta ,x,y)) \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/FGSM.png&quot; width=&quot;800&quot;&gt;
&lt;/center&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Label" scheme="http://senyang-ml.github.io/tags/Label/"/>
    
      <category term="Adversarial Sample" scheme="http://senyang-ml.github.io/tags/Adversarial-Sample/"/>
    
  </entry>
  
</feed>
