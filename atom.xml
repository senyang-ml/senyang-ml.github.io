<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Sen Yang</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://senyang-ml.github.io/"/>
  <updated>2020-05-31T07:25:57.262Z</updated>
  <id>http://senyang-ml.github.io/</id>
  
  <author>
    <name>yangsenius</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Enhance hexo next</title>
    <link href="http://senyang-ml.github.io/2020/05/24/enhance-hexo-next/"/>
    <id>http://senyang-ml.github.io/2020/05/24/enhance-hexo-next/</id>
    <published>2020-05-24T07:08:20.000Z</published>
    <updated>2020-05-31T07:25:57.262Z</updated>
    
    <content type="html"><![CDATA[<p>目标：希望让hexo的next主题，能达到vscode markdown enhanced插件的markdown和MathJax的渲染效果, 并同时提高写博客效率</p><p>方法：</p><ul><li>修改Next相关配置文件</li><li>利用Mac的截图和Typora的快速插图动作</li><li>利用edge浏览器的集锦功能</li></ul><a id="more"></a><p>在我寻找方案的过程中，我利用了新版的Mircosoft的Edge浏览器搜索相关资料。它完美衔接google的Chromium内核，增加了很独特的集锦功能，可以把整理的网页以及笔记放在一起。如下图所示，右侧简洁地记下了所有过程。</p><figure><img src="/2020/05/24/enhance-hexo-next/image-20200524153534179.png" alt="image-20200524153534179"><figcaption>image-20200524153534179</figcaption></figure><p><a href="https://blog.csdn.net/littlehaes/article/details/81503455" target="_blank" rel="noopener">hexo博客迁移到另一台电脑_json_littlehaes的博客-CSDN博客</a></p><p><a href="https://www.jianshu.com/p/0ee8b976ceab" target="_blank" rel="noopener">多个github帐号更新多个hexo博客 - 简书</a></p><p>修改Mathjax的渲染引擎，用了新的渲染链接</p><p><a href="http://docs.mathjax.org/en/v2.7-latest/configuration.html?highlight=cdn#using-a-local-configuration-file-with-a-cdn" target="_blank" rel="noopener">Loading and Configuring MathJax — MathJax 2.7 documentation</a></p><p>修改Next/source中的相关文件</p><p><a href="https://blog.csdn.net/qw8880000/article/details/80235648" target="_blank" rel="noopener">修改文章内链接样式 ｜ hexo_好好编程的博客-CSDN博客</a></p><p>最后，直接把集锦里的所有链接，都复制到你的markdown里面就可以里</p><p><strong>快速插图</strong></p><p>在markdown插入图片时，有一个非常方便的操作，前提是你使用了<strong>Tpyora</strong>这个完美的markdown工具</p><p>你打开偏好设置，点击图像，选择以下图中的<code>插入图片时```复制到指定路径为./filename</code>, 那么</p><p>你在直接复制图片到markdown的内容里时，图片会自动保存在hexo-assert-image自动生成的文件里，比如这里的enhance hexo next.md对应的文件夹<code>enhance hexo next</code> 下面。这样就可以快速地生成有效的图片路径</p><figure><img src="/2020/05/24/enhance-hexo-next/image-20200524152842014.png" alt="image-20200524152842014"><figcaption>image-20200524152842014</figcaption></figure><p>从windows写博客，到Macbook Pro写博客，效率和体验确实有挺大的提升。</p><h4 id="彩蛋----网页风格配色">彩蛋 -- 网页风格配色</h4><p>本网站的配色，完全手工挑选。怎么可以做到呢？很简单: 1. 可以点击你当前博客的某个要素，右击查看该元素的样式(css style)，现在的浏览器都有这个调试功能， 2. 在浏览器的一侧下方的样式中，出现了 <figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-class">.posts-expand</span> <span class="selector-class">.post-tags</span> <span class="selector-tag">a</span> &#123;</span><br><span class="line">    <span class="attribute">display</span>: inline-block;</span><br><span class="line">    <span class="attribute">margin-right</span>: <span class="number">10px</span>;</span><br><span class="line">    <span class="attribute">font-size</span>: <span class="number">13px</span>;</span><br><span class="line">    <span class="attribute">text-decoration</span>: none;</span><br><span class="line">    <span class="attribute">color</span>: <span class="number">#658a3a</span>;</span><br><span class="line">    <span class="attribute">border-bottom</span>: none;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure> 3. 复制<code>.posts-expand .post-tags</code> 在 vscode中搜索相关的<code>.styl</code>的文件，找到<code>color、background</code>等属性的位置，可以稍微学习一下语法 4. 在浏览器中调试好颜色后，直接复制粘贴色号到你的相关的<code>.styl</code>的文件的相关位置就可以了！</p><p>本次的分享到这里结束啦～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目标：希望让hexo的next主题，能达到vscode markdown enhanced插件的markdown和MathJax的渲染效果, 并同时提高写博客效率&lt;/p&gt;
&lt;p&gt;方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;修改Next相关配置文件&lt;/li&gt;
&lt;li&gt;利用Mac的截图和Typora的快速插图动作&lt;/li&gt;
&lt;li&gt;利用edge浏览器的集锦功能&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Mac" scheme="http://senyang-ml.github.io/tags/Mac/"/>
    
      <category term="Hexo" scheme="http://senyang-ml.github.io/tags/Hexo/"/>
    
      <category term="Typora" scheme="http://senyang-ml.github.io/tags/Typora/"/>
    
  </entry>
  
  <entry>
    <title>Stacked Capsule Autoencoders-堆叠的胶囊自编码器</title>
    <link href="http://senyang-ml.github.io/2020/02/11/stacked-capsule-autoencoders/"/>
    <id>http://senyang-ml.github.io/2020/02/11/stacked-capsule-autoencoders/</id>
    <published>2020-02-11T11:18:17.000Z</published>
    <updated>2020-05-29T09:43:24.496Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言">1. 引言</h2><p>《stacked capsule autoencoders》使用无监督的方式达到了98.5%的MNIST分类准确率。 <a href="https://arxiv.org/abs/1906.06818" target="_blank" rel="noopener">Stacked Capsule Autoencoders</a> 发表在 NeurIPS-2019，作者团队阵容豪华。可以说是官方capsule的第3个版本。前两个版本的是：</p><ul><li>Dynamic Routing Between Capsules<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a></li><li>Matrix capsule with EM routing<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li></ul><a id="more"></a><p>当然还有最早的<a href="http://www.cs.toronto.edu/~fritz/absps/transauto6.pdf" target="_blank" rel="noopener">Transforming Auto-encoders</a>，发表在2011年ICANN，论文第一次引入“capsule”的概念。值得一提的是，这篇论文的作者是Hinton、Alex Krizhevsky等人，对，是AlexNet的Alex。原来Alex本人在2012年发表AlexNet之前在研究这种“奇怪”的东西。2011年的他可能没想到，第二年的他们，为了参与ImageNet大规模数据集图像识别挑战赛而设计的一款基于的传统CNN的AlexNet，引爆了接下来已经持续7年之久的“Deep Learning”潮流，现如今CVPR 2020投稿量都过10000了，是谁惹得“祸“的还不清楚吗？</p><h2 id="概念">2. 概念</h2><p>从2017年开始， Hinton等人研究的Capsule Network得到了深度学习社区的大量关注。可以说Capsule Network在反思CNN的一些固有偏见，比如CNN的学习过分强调不变性（invariant ）特征的学习，数据增强也服务于这一目的。而这样做，实际上，忽略了一个真实世界中的事实：</p><p>1）<strong>物体-部件 关系（Object-Part-relationship）是视角不变的（viewpoint invariant）</strong>， 2）<strong>物体-观察者（Object-Viewer-relationship） 是视角同变性（viewpoint equivariant）的</strong>。</p><p>equivariant：<span class="math inline">\(\forall_{T \in \mathcal{T}} Tf(\mathbf{x}) = f(T\mathbf{x})\)</span> invariant： <span class="math inline">\(\forall_{T \in \mathcal{T}} Tf(\mathbf{x}) = f(T\mathbf{x})\)</span></p><p>并且Capsule强调物体的存在是因为当部件以合理的关系组合才得以存在，所以进一步引出了routing的机制，来发掘part-whole关系。</p><p>Matrix Capsule的那篇论文中提出，用混合高斯模型来学习这些关系，并提出了EM routing的算法，它从GMM和EM的角度，解释为更低层capsule与更高层capsule之间的自聚类算法。然而这种计算一旦放在前向传播会大大增加计算量，这也是其模型理论受限的部分， 所以其实验数据集主打还是MNIST, SmallNorb，如果换做更大一点的ImageNet, COCO，不知道Capsule的精深理念能不能发扬光大。可能Hinton深知自己的先驱身份所以他应该相信后面会有人帮他填满这些实验？</p><p>不过这种Gaussian Mixture的建模方式是非常合理的，一是GMM作为生成模型，自身就具备很强的解释性，二是这种参数学习以极大似然估计的方式，不再过分依赖梯度回传的更新机制，所以GMM的思想继续用在了Stacked Capsule Autoencoder里面。</p><blockquote><p>这里想说的是: 如果有人问，当前深度学习的核心理念是什么，我个人觉得，一个比较好的回答就是，学习目标形式化进而转换为参数的梯度学习。然而这样的同质研究越来越多，又越来越难以深入其内部，DL社区就开始自我反思。而Capsule的理念里面，就尝试去摆脱D中L对梯度回传的过分依赖，对卷积结构的过分依赖，所以Capsule Network本身将一些自编码器、重构、混合高斯、注意力等机制引入其中。</p></blockquote><p>其实读这篇论文会感觉作者用到的技术太多了，很容易忽略它背后的动机。我个人对它背后动机的理解为: 将图像中的实例的部件及属性从像素二维空间中以像素重建的方式抽取出出来；再用重构的方式解释部件与整体的关系。这也是SACE的两个主要构成环节。</p><p>接下来，本文希望通过简洁的语言来描述该论文的主要思路，所以跳过了论文对Toy Setup的描述，直接总结了SCAE的两个主要模块。</p><h2 id="stacked-capsule-autoencoders-scae">3. Stacked Capsule Autoencoders (SCAE)</h2><h3 id="scae-pcae-ocae">SCAE = PCAE +OCAE</h3><p>(Part Capsule Autoencoder + Object Capsule Autoencoder)</p><p>以这幅官方给出的这幅示意图为例</p><figure><img src="/2020/02/11/stacked-capsule-autoencoders/scae.jpg" alt="SCAE"><figcaption>SCAE</figcaption></figure><h4 id="pace">PACE</h4><p><strong>目标</strong>：将 <span class="math inline">\(h\times w \times c\)</span> 的图像，编码成 <span class="math inline">\(M\)</span> 个part capsules，每个part capsule能够对应图像中的一种部件part的所有属性，用多个属性构成的一个向量 <span class="math inline">\(X \in R^{6+1+z_m}\)</span> 来表示一个part实例，这个实例属性中包括6维的姿态，1维的存在概率和 <span class="math inline">\(z_m\)</span> 维的自身独特性特征。</p><p><strong>编码方法</strong>：采用CNN+Attention Pooling 方式:</p><p><span class="math display">\[h\times w\times c \Rightarrow M \times X \]</span></p><p><strong>解码方法</strong>：可学习的<span class="math inline">\(M\)</span>个 <span class="math inline">\(11\times 11\)</span>大小的模板+仿射变换（6维姿态导出）</p><p><strong>学习训练方法</strong>：用可学习的模板，进行高斯混合来重构原始图像+所有位置上的像素数据的极大似然估计</p><h4 id="oace">OACE</h4><p><strong>目标</strong>：把 part capsules当成 <span class="math inline">\(M\)</span> 个 <span class="math inline">\(X\)</span> 构成的集合，使用 Set Transformer学习集合中元素与元素之间的成对关系以及高阶的关系，来预测 <span class="math inline">\(K\)</span> 个object capsules，每个object capsule能够对应图像中的一个物体的所有属性，用多个属性构成的一个向量 <span class="math inline">\(Y \in R^{9+1+z_k}\)</span> 来表示一个part实例，这个实例属性中包括9维的object-viewer-matrix，1维的存在概率和 <span class="math inline">\(z_K\)</span> 维的自身独特性特征。然后对每个object capsule 使用MLP解码出，每个object capsule与所有part capsule之间的隶属关系，使用高斯混合模型来建模，即part capsule m的姿态 <span class="math inline">\(x_m\)</span>的可以看成是多个object capsule的贡献的高斯混合, 那么第 <span class="math inline">\(k\)</span> 个object capsule 对第 <span class="math inline">\(m\)</span> 个part capsule的贡献为 <span class="math inline">\(p(x_m|k,m)\)</span></p><p><strong>编码方法</strong>：Set Transformer 学习集合中各个元素的pair-wise关系以及高阶的关系，是permutation-invariant的模型。输出 <span class="math inline">\(K\)</span> 个object capsules实例</p><p><strong>解码方法</strong>：用 <span class="math inline">\(K\)</span> 个MLP预测高斯混合模型（详见论文中的公式）的候选投票，一个是方差，然后计算出高斯混合模型每个高斯成分的均值和方差，这样就可以计算出 <span class="math inline">\(p(x_m|k,m)\)</span>。</p><p><strong>学习训练方法</strong>：部件part-capsule及其属性，被物体object-capsule整体下的高斯混合来进行重构解释的极大似然估计</p><h2 id="aaai-2020-hinton发表了关于stacked-capsule-autoencoder的演讲">4. AAAI 2020 Hinton发表了关于Stacked Capsule AutoEncoder的演讲</h2><p>演讲地址在：<a href="https://www.bilibili.com/video/av88128940" target="_blank" rel="noopener">Geoffrey Hinton：Stacked Capsule Autoencoders(堆叠胶囊自编码器) (AAAI 2020)_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></p><p><strong>矩阵具备很好的同变性（equivariant）的性质，仿射矩阵可以发挥出出奇的作用。</strong></p><p>他还提到了一个很有意思的概念：Coincidence Fitering</p><p>他指出了现在的CNN的问题 并没有很好的利用--偶然性过滤，但是Transformer的注意力机制中体现出了偶然性过滤</p><p>我比较喜欢，这种用恰到好处的语言来描述提炼出一个抽象的解释性名词，既形象生动，又直指核心问题</p><h4 id="什么叫偶然性过滤">什么叫偶然性过滤？</h4><p>我个人的直觉理解是，CNN主要做的是卷积核的权重与激活值的矩阵乘法（或者说是向量的内积），那么由于输出目标监督是稀疏的（groudtruth往往是稀疏的，无论是分类还是回归，坐标还是热图），就会让中间的激活也倾向于是稀疏的，也就是说，大部分的权重矩阵与激活向量的乘法运算时对应元素的取值倾向，是无关的，或者说他们的对应取值是偶然的，这样的值是不显著的，容易会被不激活，但是问题是，这种稀疏激活模式，并不是发生在两个高维激活向量之间，而是可学习的权重和神经元的激活值之间，而真正的偶然性过滤需要发生在两个高维向量之间的某些对应元素上，比如两个10000维的向量在第2437位上具备相似的取值，而其他元素对应不存在什么关系，这种两个偶然性中出现的一致是不平凡的，那么其他平凡的偶然性就会被过滤掉。Transformer中的注意力机制就是高维向量间的内积，就会形成一种covariance structure，它会过滤掉大部分偶然的输入 (非目标输入)，只有契合该covariance structure的输入才能形成明显的激活！</p><p>(上述是我的直觉理解，描述不一定准确, 期待有更多的讨论和解读)</p><p><img src="/2020/02/11/stacked-capsule-autoencoders/coincidence-filtering.jpg" alt="SCAE"> &gt; [!NOTE] &gt;（~而且有时候，我们的实验结果和结论都有可能是“偶然的”）</p><p>演讲地址在：<a href="https://www.bilibili.com/video/av88128940" target="_blank" rel="noopener">Geoffrey Hinton：Stacked Capsule Autoencoders(堆叠胶囊自编码器) (AAAI 2020)_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a> 另外，我也整理出了关于 Dynamic Routing Between Capsules，Matrix capsule with EM routing 和 Stacked Capsule Autoencoders 的PPT。百度云盘地址：https://pan.baidu.com/s/1BCdJiNWGqD-SNao7Lmv4sw 提取码：e58t</p><p>Hinton联合了很多的学者在不断迭代着Capsule的概念和技术，不断地引入新鲜的东西到深度学习中。一方面我觉着我们需要相信大佬的直觉，另一方面我们也不能盲目地追捧，还是要尽量抛开作者光环，去探讨论文研究中是不是拥有着奇思妙想或者醍醐灌顶的东西存在。</p><section class="footnotes"><hr><ol><li id="fn1"><p><a href="https://arxiv.org/abs/1710.09829" target="_blank" rel="noopener">Dynamic Routing Between Capsules</a><a href="#fnref1" class="footnote-back">↩</a></p></li><li id="fn2"><p><a href="https://openreview.net/pdf?id=HJWLfGWRb" target="_blank" rel="noopener">Matrix capsule with EM routing</a><a href="#fnref2" class="footnote-back">↩</a></p></li></ol></section>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;引言&quot;&gt;1. 引言&lt;/h2&gt;
&lt;p&gt;《stacked capsule autoencoders》使用无监督的方式达到了98.5%的MNIST分类准确率。 &lt;a href=&quot;https://arxiv.org/abs/1906.06818&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Stacked Capsule Autoencoders&lt;/a&gt; 发表在 NeurIPS-2019，作者团队阵容豪华。可以说是官方capsule的第3个版本。前两个版本的是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamic Routing Between Capsules&lt;a href=&quot;#fn1&quot; class=&quot;footnote-ref&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Matrix capsule with EM routing&lt;a href=&quot;#fn2&quot; class=&quot;footnote-ref&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Capsule Network" scheme="http://senyang-ml.github.io/tags/Capsule-Network/"/>
    
      <category term="Unsupervised Learning" scheme="http://senyang-ml.github.io/tags/Unsupervised-Learning/"/>
    
      <category term="PPT" scheme="http://senyang-ml.github.io/tags/PPT/"/>
    
  </entry>
  
  <entry>
    <title>Distribution-Aware Coordinate Representation for Human Pose Estimation</title>
    <link href="http://senyang-ml.github.io/2019/11/02/distribution-aware/"/>
    <id>http://senyang-ml.github.io/2019/11/02/distribution-aware/</id>
    <published>2019-11-02T03:52:16.000Z</published>
    <updated>2020-05-29T04:57:07.945Z</updated>
    
    <content type="html"><![CDATA[<p>在arxiv上看到了这篇<a href="https://arxiv.org/abs/1910.06278" target="_blank" rel="noopener">论文</a> ,个人认为这是一个很有意思的工作, 利用用heatmap上的最大值以及其对应位置m, 来估计真实高斯分布均值位置μ. 这样的量化误差（下采样导致的量化最小单位误差）能够得到最大程度上的减轻．</p><p>论文实验验证了该方法比经验上的估计方法更准确.（取峰值到次峰值的１／４偏移处的位置，这个估计其实也是近似符合高斯分布了）.</p><a id="more"></a><p><strong>公式６一阶导</strong></p><p><span class="math display">\[\left.\mathcal{D}^{\prime}(\boldsymbol{x})\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=\left.\frac{\partial \mathcal{P}^{T}}{\partial \boldsymbol{x}}\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=-\left.\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\right|_{\boldsymbol{x}=\boldsymbol{\mu}}=0\]</span></p><p>那么<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{x})\)</span> 是一个和<span class="math inline">\(\boldsymbol{x}\)</span> 形状一样的向量, 然而在公式(7)对向量<span class="math inline">\(\boldsymbol{\mu}\)</span>泰勒展开:</p><p><strong>公式７,高斯分布均值<span class="math inline">\(\mu\)</span>处关于<span class="math inline">\(ｍ\)</span>位置的二阶泰勒展开</strong></p><p><span class="math display">\[\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{T} \mathcal{D}^{\prime \prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\]</span></p><p>中的第二项<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\)</span> 中的<span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})\)</span> 是不是应该加上转置,才能得到标量? 即 <span class="math inline">\(\mathcal{D}^{\prime}(\boldsymbol{m})^T(\boldsymbol{\mu}-\boldsymbol{m})\)</span></p><h2 id="推导">推导</h2><p>泰勒展开公式</p><p><span class="math display">\[\mathcal{P}(\boldsymbol{\mu})=\mathcal{P}(\boldsymbol{m})+\mathcal{D}^{\prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})+\frac{1}{2}(\boldsymbol{\mu}-\boldsymbol{m})^{T} \mathcal{D}^{\prime \prime}(\boldsymbol{m})(\boldsymbol{\mu}-\boldsymbol{m})\]</span></p><p>代入<span class="math inline">\(P(\mu)\)</span>和<span class="math inline">\(P(m)\)</span>的高斯分布公式，即，将<span class="math inline">\(\mu,m\)</span>代入下面的式子，约掉常数项</p><p><span class="math display">\[\begin{aligned} \mathcal{P}(\boldsymbol{x}; \boldsymbol{\mu}, \Sigma)=\ln (\mathcal{G})=&amp;-\ln (2 \pi)-\frac{1}{2} \ln (|\Sigma|)\\ &amp;-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^{T} \Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \end{aligned}\]</span></p><p>可以得到</p><p><span class="math display">\[\begin{aligned}0&amp;=-\frac{1}{2}(m-\mu)^{\top} \Sigma^{-1}(m-\mu) +D^{\prime}(m)^{\top}(\mu-m)+\frac{1}{2}(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top}(\mu-m)&amp;=(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top} &amp;=(\mu-m)^{\top} D^{\prime \prime}(m)\\-D^{\prime}(m)^{\top} D^{\prime \prime}(m)^{-1} &amp;=(\mu-m)^{\top} \\-D^{\prime\prime}(m)^{-\top} D^{\prime}(m) &amp;=\mu-m \\ \mu &amp;=m-D^{\prime \prime}(m)^{-\top} D^{\prime}(m) \end{aligned}\]</span></p><p><strong>因为<span class="math inline">\(D^{\prime \prime}(m)＝－ \Sigma^{-1}\)</span>，在论文中方差矩阵假设为对角阵(可逆)<span class="math inline">\(\Sigma=\left[\begin{array}{ll}{\sigma^{2}} &amp; {0} \\ {0} &amp; {\sigma^{2}}\end{array}\right]\)</span> (因为ｘｙ方向独立),　这意味着<span class="math inline">\(D^{\prime \prime}(m)=D^{\prime \prime}(m)^T\)</span>, 所以</strong></p><p><span class="math display">\[\begin{aligned}\mu &amp;=m-D^{\prime \prime}(m)^{-\top} D^{\prime}(m) \\ \mu&amp;=m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\end{aligned}\]</span></p><p>补充一个细节:</p><p>上面的推导, 在第三个等式约掉(<span class="math inline">\(\mu-m\)</span>)的条件是假设<span class="math inline">\(\mu\)</span>不等于<span class="math inline">\(m\)</span>, 所以下面的等式是更完备的推导: <span class="math display">\[\begin{aligned}0&amp;=-\frac{1}{2}(m-\mu)^{\top} \Sigma^{-1}(m-\mu) +D^{\prime}(m)^{\top}(\mu-m)+\frac{1}{2}(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m)\\-D^{\prime}(m)^{\top}(\mu-m)&amp;=(\mu-m)^{\top} D^{\prime \prime}(m)(\mu-m) \\-D^{\prime}(m)^{\top} D^{\prime \prime}(m)^{-1} (\mu-m)&amp;=(\mu-m)^{\top}(\mu-m)  \\  0 &amp;=\left[\mu-m+D^{\prime \prime}(m)^{-\top} D^{\prime}(m)\right] (\mu-m) \\  0 &amp;=[\mu-m+D^{\prime \prime}(m)^{-1} D^{\prime}(m)] (\mu-m)\end{aligned}\]</span></p><p>这个推导的建立在两个假设上面: (1) 下采样后得到的heatmap上面的取值, 被假设为服从真实关键点位置的高斯分布 (2) 二阶泰勒展开的近似</p><p>那么 <span class="math inline">\(\mu =m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\)</span> 也包含了<span class="math inline">\(\mu=m\)</span>的可能, 因为</p><p><span class="math inline">\(D^{\prime}(m)=0\Leftrightarrow m\)</span>在高斯分布的均值位置<span class="math inline">\(\Leftrightarrow \mu=m\)</span></p><p>所以 <span class="math inline">\(\mu =m-D^{\prime \prime}(m)^{-1} D^{\prime}(m)\)</span>是完备的</p><p>在ilovepose网站(赞!)上, 原作者也给出了关于公式的解释: <a href="http://www.ilovepose.cn/t/99" target="_blank" rel="noopener" class="uri">http://www.ilovepose.cn/t/99</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在arxiv上看到了这篇&lt;a href=&quot;https://arxiv.org/abs/1910.06278&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;论文&lt;/a&gt; ,个人认为这是一个很有意思的工作, 利用用heatmap上的最大值以及其对应位置m, 来估计真实高斯分布均值位置μ. 这样的量化误差（下采样导致的量化最小单位误差）能够得到最大程度上的减轻．&lt;/p&gt;
&lt;p&gt;论文实验验证了该方法比经验上的估计方法更准确.（取峰值到次峰值的１／４偏移处的位置，这个估计其实也是近似符合高斯分布了）.&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Distribution" scheme="http://senyang-ml.github.io/tags/Distribution/"/>
    
  </entry>
  
  <entry>
    <title>Reproduce PersonLab (2)</title>
    <link href="http://senyang-ml.github.io/2019/09/02/personlab-2/"/>
    <id>http://senyang-ml.github.io/2019/09/02/personlab-2/</id>
    <published>2019-09-02T08:17:42.000Z</published>
    <updated>2020-05-24T05:43:37.750Z</updated>
    
    <content type="html"><![CDATA[<p>Bilinear interpolation kernel + Hough voting + Greedy Algorithm</p><p>个人认为： - PersonLab中最给人启发的是：构造 geometric embedding: short-range offsets, mid-range offsets and long-range offsets 几何信息来表示人体姿态, 以此监督神经网络的预测，并根据预测结果，施以贪婪算法，关联出所有人的姿态和分割区域。</p><ul><li>PersonLab中最值得琢磨的数学部分是：如何将表示keypoints大致位置的heatmaps和short-offsets maps通过双线性插值核，然后进行Hough Vote得到精确位置的Hough Score Maps的过程。</li></ul><a id="more"></a><p>后者是我复现过程中遇到的一个难题。其实构造Hough Score maps在Google的上一篇论文 <code>G-RMI</code> 中就出现了，然而因为G-RMI没有开源的代码，所以我对其提到的双线性插值核函数<span class="math inline">\(G(\cdot)\)</span>的理解仍然还是很模糊。另外，如果按照论文公式给出的Hough Voting公式，去遍历图像的每一个像素位置进行计算的话，计算的时间复杂度会很高，所以，作者一定有什么巧妙的方式可以处理它，可是PersonLab也没有开源。</p><p>后来我发现，github上有一个<a href="https://github.com/octiapp/KerasPersonLab" target="_blank" rel="noopener">KerasPersonLab</a>的实现，论文的所有细节都几乎实现了，所以来看看他的关于构造Hough Score Maps的。</p><blockquote><p>不禁佩服这个项目的开发者，为什么他们就可以有这个能力把论文中没有提到的细节完美地实现出来，这一定是他们的功底比我强很多，或者他们知道了一些我不知道的先验知识。对两人的了解后，我发现他们是<a href="http://octi.tv/" target="_blank" rel="noopener"><code>Octi</code></a>公司的两位算法工程师, 我的猜测是开发APP需要将PersonLab的算法移植到手机上面，所以激发了两个人完成了实现，PersonLab的ArXiv发布时间是2018年三月底，这个项目的发布时间是2018年5月份，真的是厉害了。GitHub上其他的Personlab实现版本都是在此之后，不少repo借鉴了他们的这个项目。</p></blockquote><p>关于构造Hough Score Map 有这样的一段代码： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accumulate_votes</span><span class="params">(votes, shape)</span>:</span></span><br><span class="line">    xs = votes[:,<span class="number">0</span>]</span><br><span class="line">    ys = votes[:,<span class="number">1</span>]</span><br><span class="line">    ps = votes[:,<span class="number">2</span>]</span><br><span class="line">    tl = [np.floor(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    tr = [np.floor(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    bl = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    br = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">    dx = xs - tl[<span class="number">1</span>]</span><br><span class="line">    dy = ys - tl[<span class="number">0</span>]</span><br><span class="line">    tl_vals = ps*(<span class="number">1.</span>-dx)*(<span class="number">1.</span>-dy)</span><br><span class="line">    tr_vals = ps*dx*(<span class="number">1.</span>-dy)</span><br><span class="line">    bl_vals = ps*dy*(<span class="number">1.</span>-dx)</span><br><span class="line">    br_vals = ps*dy*dx</span><br><span class="line">    data = np.concatenate([tl_vals, tr_vals, bl_vals, br_vals])</span><br><span class="line">    I = np.concatenate([tl[<span class="number">0</span>], tr[<span class="number">0</span>], bl[<span class="number">0</span>], br[<span class="number">0</span>]])</span><br><span class="line">    J = np.concatenate([tl[<span class="number">1</span>], tr[<span class="number">1</span>], bl[<span class="number">1</span>], br[<span class="number">1</span>]])</span><br><span class="line">    good_inds = np.logical_and(I &gt;= <span class="number">0</span>, I &lt; shape[<span class="number">0</span>])</span><br><span class="line">    good_inds = np.logical_and(good_inds, np.logical_and(J &gt;= <span class="number">0</span>, J &lt; shape[<span class="number">1</span>]))</span><br><span class="line">    heatmap = np.asarray(coo_matrix( (data[good_inds], (I[good_inds],J[good_inds])), shape=shape ).todense())</span><br><span class="line">    <span class="keyword">return</span> heatmap</span><br></pre></td></tr></table></figure></p><p>当我刚看到这些的时候是无法理解的，我认为这里面有着论文Hough voting最核心的部分。我想Accumulate——vote的出现一定和Hough Transform有一定的关联，另外，为什么会出现<code>np.ceil</code>和<code>np.floor</code>?这个<code>tl_vals = ps*(1.-dx)*(1.-dy)</code>又是什么鬼？似乎和双线性有着一点点的联系。我要慢慢摸清楚，顺藤摸瓜。</p><h2 id="hough-transform">Hough Transform</h2><p>回顾一下最经典的Hough transform 算法，那就以Hough Line Transform 为例子。OpenCV关于<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a>，有一个比较好的<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">解释文档</a>.</p><p>我想简洁地概括一下Hough Line Transform：</p><p>在二维数据空间坐标中，线其上的点，<span class="math inline">\((x,y)\)</span>的满足 <span class="math inline">\(y = mx+c\)</span>，一条直线就确定了一个唯一的参数坐标 <span class="math inline">\((m,c)\)</span></p><p>换一种视角去理解。</p><p>在二维参数空间坐标中，“线”其上的“点”，<span class="math inline">\((m,c)\)</span>满足于<span class="math inline">\(c=-xm+y\)</span>, 一条参数空间上的“线”就确定了二维平面中的唯一数据坐标<span class="math inline">\((x,y)\)</span> (参数空间上的一条“线”，对应了数据空间中某个点的直线簇)</p><p>这里的参数空间就是，hough space</p><h3 id="什么是hough-voting呢">什么是Hough Voting呢？</h3><p>举直线（可以进一步泛化，称之为模型A）的例子</p><p>我们可以把数据空间上的数据点，想象成是由其模型A--直线生成的，那么数据空间中的一个点，在直线模型的参数空间（hough Space）上，可能属于很多个直线（点的直线簇）。我们用一个2维数组来记录直线的参数，构造出Hough Space。比如<a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a>给出的参数形式 <span class="math inline">\(\rho = x \cos \theta + y \sin \theta\)</span> 中的 <span class="math inline">\((\rho,\theta)\)</span> 来构造一个二维Hough Space, 然后我们需要选择一个量化等级，来控制其中一个参数增减的最小单位。这样的话，拿到一个点，我们就可以给其在Hough Space中可能属于的所有模型各投一次票。如果对数据空间中的所有数据点进行上述操作，并用Accumulator累积Hough Space各个模型的投票总数，我们就可以获得不同模型的得分，那么根据得分最大的模型对应的参数，就能够刻画出数据空间中数据点们共同依赖的那个模型！</p><p>根据这种思想我们可以将直线模型泛化到更一般的圆模型、椭圆甚至无规则的形状！！</p><p>这就是Hough Transform 和hough Voting的基本要义了。</p><p>然后我研究了一下<code>coo_matrix</code>函数，它能快速地构造一个稀疏矩阵，并且注意到，代码给出的格式满足： <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">coo_matrix((data, (i, j)), [shape&#x3D;(M, N)])</span><br><span class="line">to construct from three arrays:</span><br><span class="line">    1. data[:]   the entries of the matrix, in any order</span><br><span class="line">    2. i[:]      the row indices of the matrix entries</span><br><span class="line">    3. j[:]      the column indices of the matrix entries</span><br><span class="line"></span><br><span class="line">Where &#96;&#96;A[i[k], j[k]] &#x3D; data[k]&#96;&#96;.  When shape is not</span><br><span class="line">specified, it is inferred from the index arrays</span><br></pre></td></tr></table></figure> 并且它具有累积功能，而就是它，完成了hough voting的功能</p><h3 id="bilinear-interpolation">bilinear interpolation</h3><p>再去分析一下，与np.ceil和np.floor作差以及连乘的意义 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">xs = votes[:,<span class="number">0</span>]</span><br><span class="line">ys = votes[:,<span class="number">1</span>]</span><br><span class="line">ps = votes[:,<span class="number">2</span>]</span><br><span class="line">tl = [np.floor(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">tr = [np.floor(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">bl = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.floor(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">br = [np.ceil(ys).astype(<span class="string">'int32'</span>), np.ceil(xs).astype(<span class="string">'int32'</span>)]</span><br><span class="line">dx = xs - tl[<span class="number">1</span>]</span><br><span class="line">dy = ys - tl[<span class="number">0</span>]</span><br><span class="line">tl_vals = ps*(<span class="number">1.</span>-dx)*(<span class="number">1.</span>-dy)</span><br><span class="line">tr_vals = ps*dx*(<span class="number">1.</span>-dy)</span><br><span class="line">bl_vals = ps*dy*(<span class="number">1.</span>-dx)</span><br><span class="line">br_vals = ps*dy*dx</span><br></pre></td></tr></table></figure> 其功能相当于在进行双线性插值：</p><p>f(dx,dy)=f(0,0)(1-dx)(1-dy)+f(1,0)x(1-dy)+f(0,1)(1-dx)dy+f(1,1)dxdy</p><p>针对，每个位置上预测的<strong>小数点精度</strong>上（如果预测为整数，仅有<code>tl_vals</code>不为0）的偏量，以双线性的方式，并乘以预测置信度，累积到相邻的<strong>整数</strong>像素位置（这样可以不会牺牲任何量化上的精度），进而可以得到精确的预测位置，即反应在Hough Score Maps的极大值位置上</p><p>对于Hough Score Maps对应的每一个关键点类型，最后施以： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_heatmaps</span><span class="params">(kp_maps, short_offsets,radius, kpts_num)</span>:</span></span><br><span class="line">    heatmaps = []</span><br><span class="line">    map_shape = kp_maps.shape[:<span class="number">2</span>]</span><br><span class="line">    idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(kpts_num):</span><br><span class="line">        this_kp_map = kp_maps[:,:,i:i+<span class="number">1</span>]</span><br><span class="line">        votes = idx + short_offsets[:,:,i]</span><br><span class="line">        votes = np.reshape(np.concatenate([votes, this_kp_map], axis=<span class="number">-1</span>), (<span class="number">-1</span>, <span class="number">3</span>))</span><br><span class="line">        heatmaps.append(accumulate_votes(votes, shape=map_shape) / (np.pi*radius**<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.stack(heatmaps, axis=<span class="number">-1</span>)</span><br></pre></td></tr></table></figure> 即得到了Hough Score Maps.</p><h2 id="greedy-decoding-和-assign-pixels-to-person-skeleton-instance">Greedy Decoding 和 Assign pixels to person skeleton instance</h2><ul><li>PersonLab中巧妙的算法是：利用树结构，通过<code>Greedy Decoding</code>将偏移向量连通成独立的人体骨架；利用mask中像素与keypoints的偏移向量，以聚类的方式形成人体intance mask。</li></ul><p>关于<code>Greedy Decoding</code>部分，可以参考<a href="https://senyang-ml.github.io/2019/07/17/pifpaf/#Fast-greedy-decoding">我的另一篇博客</a>。</p><p>关于长向量<code>long-range-offset</code>的预测部分,从直观上看，可以发现，偏移量向内指向人体的关节，可以形成了论文提到的<code>basins of attraction</code>. 体现了一种区域分割的思想。</p><p>更多请参考如下：</p><ul><li><a href="https://github.com/octiapp/KerasPersonLab" target="_blank" rel="noopener">KerasPersonLab</a></li><li><a href="https://docs.opencv.org/3.3.0/d6/d10/tutorial_py_houghlines.html" target="_blank" rel="noopener">Hough Line Transform</a></li><li><a href="https://arxiv.org/abs/1803.08225" target="_blank" rel="noopener">PersonLab</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bilinear interpolation kernel + Hough voting + Greedy Algorithm&lt;/p&gt;
&lt;p&gt;个人认为： - PersonLab中最给人启发的是：构造 geometric embedding: short-range offsets, mid-range offsets and long-range offsets 几何信息来表示人体姿态, 以此监督神经网络的预测，并根据预测结果，施以贪婪算法，关联出所有人的姿态和分割区域。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PersonLab中最值得琢磨的数学部分是：如何将表示keypoints大致位置的heatmaps和short-offsets maps通过双线性插值核，然后进行Hough Vote得到精确位置的Hough Score Maps的过程。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Hough Voting" scheme="http://senyang-ml.github.io/tags/Hough-Voting/"/>
    
      <category term="Algorithm" scheme="http://senyang-ml.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Pose Neural Fabrics Search</title>
    <link href="http://senyang-ml.github.io/2019/08/26/Pose-Neural-Fabrics-Search/"/>
    <id>http://senyang-ml.github.io/2019/08/26/Pose-Neural-Fabrics-Search/</id>
    <published>2019-08-26T08:23:53.000Z</published>
    <updated>2020-05-29T02:11:55.947Z</updated>
    
    <content type="html"><![CDATA[<p>Neural Architecture Search (NAS) for Human Pose Estimation.</p><center class="half"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/cell.jpg" width="50%"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/cell-based_fabric.jpg" width="40%"></center><p>Search part-specific Cell-based Neural Fabrics (CNFs) with the guide of prior knowledge of human body structure.</p><center class="half"><img src="/2019/08/26/Pose-Neural-Fabrics-Search/pnfs_framework.jpg"></center><p><a href="https://arxiv.org/abs/1909.07068" target="_blank" rel="noopener">ArXiv</a> <a href="https://github.com/yangsenius/PoseNFS" target="_blank" rel="noopener">Code</a> <a href="https://yangsenius.github.io/PoseNFS/" target="_blank" rel="noopener">Project Page</a></p><a id="more"></a><h2 id="introduction">Introduction</h2><p>Neural Architecture Search (NAS), the process of learning the structure of neural network, can play a potential role at automatically designing network architectures. Current methods mainly take image classification as a basic task and only search for a micro cell to build a chain-like structure, thus the neural search space is still at the limit of a micro search space. However, when applying NAS to dense prediction tasks such as semantic segmentation and human pose estimation, the micro search space is no longer able to generate more complex architectures. Therefore, it become a necessity to artificially design the macro search space allowing identifying hierarchical structure upon cells for these tasks. In addition, existing works focus on discovering an alternative to the human-designed module in a common pipeline. Such practice actually decouples the automating architecture engineering from tasks, and is thus unable to take advantage of the domain knowledge of a specific task.</p><p>In this work, we study how to search neural architectures with the guide of prior knowledge for human pose estimation task and propose a framework named Pose Neural Fabrics Search. We notice that modern methods conducting human pose estimation based on deep CNNs, regardless of top-down or bottom-up pipeline, convert it into pixel-wise prediction problem; they usually focus on two aspects: <strong>neural architecture design</strong> and <strong>pose representation</strong>. Next, we will discuss our motivations from these two aspects. ...</p><p><a href="https://arxiv.org/pdf/1909.07068.pdf" target="_blank" rel="noopener">arXiv:1909.07068</a> <a href="2019-pose_neural_fabrics_search.pdf">newly updated</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Neural Architecture Search (NAS) for Human Pose Estimation.&lt;/p&gt;
&lt;center class=&quot;half&quot;&gt;
&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/cell.jpg&quot; width=&quot;50%&quot;&gt;&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/cell-based_fabric.jpg&quot; width=&quot;40%&quot;&gt;
&lt;/center&gt;
&lt;p&gt;Search part-specific Cell-based Neural Fabrics (CNFs) with the guide of prior knowledge of human body structure.&lt;/p&gt;
&lt;center class=&quot;half&quot;&gt;
&lt;img src=&quot;/2019/08/26/Pose-Neural-Fabrics-Search/pnfs_framework.jpg&quot;&gt;
&lt;/center&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1909.07068&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ArXiv&lt;/a&gt; &lt;a href=&quot;https://github.com/yangsenius/PoseNFS&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Code&lt;/a&gt; &lt;a href=&quot;https://yangsenius.github.io/PoseNFS/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Neural Architecture Search" scheme="http://senyang-ml.github.io/tags/Neural-Architecture-Search/"/>
    
  </entry>
  
  <entry>
    <title>Reproduce PersonLab (1)</title>
    <link href="http://senyang-ml.github.io/2019/08/19/Reproduce-PersonLab/"/>
    <id>http://senyang-ml.github.io/2019/08/19/Reproduce-PersonLab/</id>
    <published>2019-08-19T08:57:32.000Z</published>
    <updated>2020-03-13T11:06:11.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="personlab复现过程">PersonLab复现过程</h1><blockquote><p>我打算记录整个复现的心路历程,和代码实践遇到的问题 正在写完这一行字的我, 什么代码都没写~ 所以以下的文字是一篇<code>记叙文</code>, 默认的叙述手法是<code>顺序</code> <a id="more"></a></p></blockquote><p>选个复现思路: - 自上而下: 从<code>train.py</code>的<code>main()</code>,逐渐构建所需要的函数以及其子函数 - 自下而上: 从每个最小的函数(如从读入数据集,预处理图像,构建标签)开始,逐渐向上封装函数</p><p>我倾向于后者:</p><p>我先想清楚了整个项目需要哪些模块, 然后创建了如下的几个空文件:</p><p>backbone_network.py, label_constrction.py, data_augmentation.py , data_iteration.py, loss.py, evaluate.py, greedy_decoding.py, instance_association.py, model.py , train.py</p><p>空文件先创建好,刺激你去复现,后面遇到问题再改</p><h1 id="从label_construction.py-开始">从<code>label_construction.py</code> 开始</h1><p>PersonLab 最精华的部分应该是<strong>如何利用COCO给定的标签信息, 利用人类的直觉和知识重新加工成几何上的监督信息</strong>, 即<code>heatmaps</code>,<code>short-range offset</code>,<code>hough_socre_maps</code>,<code>mid-range pairwise offset</code>, <code>long-range offset</code>,<code>persons_mask</code>. 所以此部分也是复现的关键步骤.</p><p><strong>对了</strong>, 有一点需要强调, <strong>复现前要仔细阅读论文的实验部分(Experiment)的描述.我读了一些关键的描述语言, 后面如果涉及到再详细说明</strong></p><p><strong>继续</strong></p><p><code>coco keypoint detection task</code> 数据集提供的标签格式是<code>.json</code>, 举个例子,<code>person_keypoints_val2017.josn</code>其中包含<code>info</code>和<code>annotation</code>,</p><ul><li><code>info</code>负责提供数据集中每张图像的<code>image_id</code>,<code>file_name</code>,<code>height</code>,<code>width</code></li><li><code>annotation</code>负责提供标注信息,其中的多个样本可能来自于同一张图像(多人问题嘛),每个样本包括如下: <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;segmentation&quot;: [[76,46.53,省略,31.03,99,省略,46.03]],</span><br><span class="line">&quot;num_keypoints&quot;: 15,</span><br><span class="line">&quot;area&quot;: 2404.375,</span><br><span class="line">&quot;iscrowd&quot;: 0,</span><br><span class="line">&quot;keypoints&quot;: [102,50,1,0,0,0,101,46,2,0,0,0,97,46,2,82,44,2,91,49,2,97,43,2,109,66,2,112,43,2,128,73,2,71,74,2,76,79,2,94,65,2,110,81,2,84,90,2,129,99,2],&quot;image_id&quot;: 149770,</span><br><span class="line">&quot;bbox&quot;: [65,31.03,81,77.5],</span><br><span class="line">&quot;category_id&quot;: 1,</span><br><span class="line">&quot;id&quot;: 427983&#125;</span><br></pre></td></tr></table></figure> 我们的目标就是找到每张图像中所有样本,并且主要根据他们的<code>keypoints</code>,<code>segmentation</code>的坐标信息来构造上面提到的监督信号,<code>bbox</code>的坐标其实就不需要了.</li></ul><p><strong>以下论文提到的两点请注意</strong>:</p><p>在利用<code>segmentation</code>的时候,需要注意,作者提到了关于处理特殊情况的操作: &gt; we back-propagate across the full image, only excluding areas that contain people that have not been fully annotated with keypoints (person crowd areas and small scale person segments in the COCO dataset)</p><p>这意味着我们要考虑<code>iscrowd==1</code>的情况,我们在进行loss计算时,要把<code>iscrowd==1</code> 的区域mask掉.</p><p>此外, 在论文的<code>Imputing missing keypoint annotations</code>章节,作者说明: &gt;The standard COCO dataset does not contain keypoint annotations in the training set for the small person instances, and ignores them during model evaluation.However, it contains segmentation annotations and evaluates mask predictions for those small instances. Since training our geometric embeddings requires keypoint annotations for training, we have run the single-person pose estimator of [G-RMI] (trained on COCO data alone) in the COCO training set on image crops around the ground truth box annotations of those small person instances to impute those missing keypoint annotations.</p><p>因为暂时不打算用另外一个模型预测小尺寸图像的keypoints,这里我们直接忽略掉小尺寸的instance segmentation</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> pycocotools.coco <span class="keyword">import</span> COCO</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_coco_annotations</span><span class="params">(ann_dir,img_dir,mode=<span class="string">'val'</span>)</span>:</span></span><br><span class="line">    ann_file = os.path.join(ann_dir,<span class="string">'person_keypoints_&#123;&#125;2017.json'</span>.format(mode))</span><br><span class="line">    coco = COCO(ann_file)</span><br><span class="line">    image_id_list = coco.getImgIds()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> img_id <span class="keyword">in</span> image_id_list:</span><br><span class="line"></span><br><span class="line">        anns = coco.loadAnns(coco.getAnnIds(imgIds=img_id))</span><br><span class="line">        <span class="keyword">if</span> len(anns)==<span class="number">0</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">        file_name = coco.imgs[img_id][<span class="string">'file_name'</span>]</span><br><span class="line">        file_path = os.path.join(img_dir,mode+<span class="string">'2017'</span>,file_name)</span><br><span class="line">        img = cv2.imread(file_path)</span><br><span class="line">        h, w, c = img.shape</span><br><span class="line"></span><br><span class="line">        <span class="comment">#crowd_mask = np.zeros((h,w),dtype='bool')</span></span><br><span class="line">        instance_masks = []</span><br><span class="line">        keypoints_skeletons = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ann <span class="keyword">in</span> anns:</span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'area'</span>] ==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            mask = coco.annToMask(ann)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'iscrowd'</span>] ==<span class="number">1</span>:</span><br><span class="line">                <span class="comment"># IGNORE CROWD IN PAPAER: TODO      </span></span><br><span class="line">                <span class="keyword">continue</span>       </span><br><span class="line">            <span class="keyword">if</span> ann[<span class="string">'num_keypoints'</span>] ==<span class="number">0</span>:</span><br><span class="line">                <span class="comment"># IGNORE: TODO   </span></span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            keypoints_skeletons.append(ann[<span class="string">'keypoints'</span>])</span><br><span class="line">            instance_masks.append(mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> img, keypoints_skeletons, instance_masks</span><br><span class="line">    </span><br><span class="line">ann_dir = <span class="string">'/data/dataset/coco/annotations/'</span></span><br><span class="line">img_dir = <span class="string">'/data/dataset/coco/images/'</span></span><br><span class="line"></span><br><span class="line">c = get_coco_annotations(ann_dir,img_dir)</span><br><span class="line"></span><br><span class="line">print(c)</span><br></pre></td></tr></table></figure><pre><code>&lt;generator object get_coco_annotations at 0x0000024759D61518&gt;</code></pre><p>上面是构造了一个生成器, 迭代产生(img, keypoints_skeletons, instance_masks) 这样的元组.</p><p>接下来要做的事,就是如何把最原始的标签重新加工编码成一个更加几何化的监督表示.</p><p>我们要明确一点, 论文中构造的偏移向量的表示都是在原图像尺寸的量化精度下的, 而不是神经网络直接输出的feature map的尺寸, 因为network的输出已经被降采样了, 所以不论是预测出的<code>heatmap</code>,还是<code>offsets</code>都被上采样,来还原到原图的尺寸精度上.</p><p>所以我们编码用来监督的表示时,需要参考原图的高度和宽度.</p><h2 id="根据关键点的位置构造以关键点位置为中心半径为r的圆形disk区域区域内的取值为1区域外的取值为0">根据关键点的位置构造以关键点位置为中心，半径为r的圆形disk区域，区域内的取值为1，区域外的取值为0</h2><p>最直接的做法是，遍历heatmap的每个位置，通过计算距离来判断是否落在某个关键点的半径范围内，但这样的时间复杂度为O(H*W) 而通过数学的直观角度，大部分的区域都是disk外的，有没有更快捷的方法，这里我采用的是：</p><p><code>引入高斯核的技巧，即在每个关键点的位置生成高斯分布的函数，其分布满足中心对称，那么通过设定阈值（半径处取值），大于阈值设置为1，小于阈值设置为0</code></p><blockquote><p>高斯核的技巧，避免了heatmap上所有位置的遍历，此处我用了opencv带的cv2.GaussianBlur()函数，这个实际上也是个滤波器，也包含遍历，但其复杂度为<code>O(H*W)</code>但其函数通过openc库实现。（是否需要考证一下这种方法的计算速度？毕竟滤波器遍历了整个heatmap。有没有更直接的指定位置插入高斯核的现成的函数？）</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">disk_mask_heatmap</span><span class="params">(one_hot_heatmap,radius)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> radius % <span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        radius = radius - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># odd number for kernel size</span></span><br><span class="line">    heatmap = cv2.GaussianBlur(one_hot_heatmap, ksize= (radius,radius), sigmaX=<span class="number">1</span>, sigmaY=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_threshold_value</span><span class="params">(ksize)</span>:</span></span><br><span class="line">        <span class="comment"># In order to get a circle(disk ) mask, </span></span><br><span class="line">        <span class="comment"># we need to find a threshold value `t` in Gaussian kernel size, v=1 if v&gt;t else 0 </span></span><br><span class="line">        <span class="comment"># sample value in position: circle center + radius//2  </span></span><br><span class="line">        <span class="comment"># to get a approximate disk of radius</span></span><br><span class="line">        square = np.zeros(shape=(ksize[<span class="number">0</span>]*<span class="number">2</span>,ksize[<span class="number">1</span>]*<span class="number">2</span>))</span><br><span class="line">        center = square.shape[<span class="number">0</span>]//<span class="number">2</span>, square.shape[<span class="number">1</span>]//<span class="number">2</span></span><br><span class="line">        square[center[<span class="number">0</span>],center[<span class="number">1</span>]]=<span class="number">1</span></span><br><span class="line">        gaussian = cv2.GaussianBlur(square, ksize, sigmaX=<span class="number">1</span>, sigmaY=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        circle_border_value = gaussian[center[<span class="number">0</span>],center[<span class="number">1</span>]+ksize[<span class="number">1</span>]//<span class="number">2</span>] <span class="comment"># border of circle</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> circle_border_value</span><br><span class="line"></span><br><span class="line">    threshold = get_threshold_value(ksize=(radius,radius))</span><br><span class="line">    heatmap = (heatmap &gt;= threshold)</span><br><span class="line">    <span class="keyword">return</span> heatmap</span><br></pre></td></tr></table></figure><p>上述方法可以解决了生成disk的难题，但是接下考虑构造short-range offset时，必须找到在关键点周围半径内的位置计算偏移。这种功能要求代码，必须给定一个关键点位置，就可以获得其周围对应的位置。这个需求也就是：<code>在产生指定圆的时候，同时记录其圆内所有像素的位置和像素相对于圆心的偏移。</code></p><p>但是上面的代码不是逐个处理关键点的方法，而是一次性生成的，所以不能够实现上面的需求</p><p>获取heatmap上位置索引的技巧，比如，给定一个HxW大小的heatmaps</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">H,W =<span class="number">3</span>,<span class="number">3</span> </span><br><span class="line">map_shape = (H, W) </span><br><span class="line">idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>)) </span><br><span class="line">print(idx.shape)</span><br><span class="line">print(idx)</span><br><span class="line">print(idx[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]<span class="number">-2</span>)</span><br></pre></td></tr></table></figure><pre><code>(3, 3, 2)[[[0 0]  [1 0]  [2 0]] [[0 1]  [1 1]  [2 1]] [[0 2]  [1 2]  [2 2]]]-1</code></pre><p>这就可以得到了heatmap每个位置的地址索引. 继续考虑如何构造disk区域。 ## 获取每个关键点影响周围的disk区域</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_keypoint_discs</span><span class="params">(all_keypoints,map_shape,K=<span class="number">17</span>,radius=<span class="number">4</span>)</span>:</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line">    </span><br><span class="line">    discs = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(all_keypoints))]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">        </span><br><span class="line">        centers = [keypoints[i,:<span class="number">2</span>] <span class="keyword">for</span> keypoints <span class="keyword">in</span> all_keypoints <span class="keyword">if</span> keypoints[i,<span class="number">2</span>] &gt; <span class="number">0</span>]</span><br><span class="line">        dists = np.zeros(map_shape+(len(centers),))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> k, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">            dists[:,:,k] = np.sqrt(np.square(center-idx).sum(axis=<span class="number">-1</span>))</span><br><span class="line">        <span class="keyword">if</span> len(centers) &gt; <span class="number">0</span>:</span><br><span class="line">            inst_id = dists.argmin(axis=<span class="number">-1</span>)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(all_keypoints)):</span><br><span class="line">            <span class="keyword">if</span> all_keypoints[j][i,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                discs[j].append(np.logical_and(inst_id==count, dists[:,:,count]&lt;=radius))</span><br><span class="line">                count +=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                discs[j].append(np.array([]))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># discs.shape N*K*[indices of the specified keypoint]</span></span><br><span class="line">    discs = np.array(discs)</span><br><span class="line">    <span class="keyword">return</span> discs</span><br></pre></td></tr></table></figure><p>返回的discs，包含K个heatmap，其中每个heatmap对应一种类型的人体关键点的所有人体的位置，每个位置的周围disk内的所有像素的位置的索引。进而达到了功能的需求。</p><p>进而我们可以设计这样的函数,根据所有人体的所有关键点的位置集合,</p><ul><li>获取每个关键点位置周围的disk区域,进行赋值,得到heatmaps</li><li>获取每个关键点位置周围的disk区域内每个像素的位置索引,与中心位置在x,y方向上作差,得到short offsets</li><li>获取起始关键点位置周围的disk区域内每个像素的位置索引,用终点关键点的位置于这些像素位置的索引作差，得到mid-range offsets</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_keypoint_discs</span><span class="params">(all_keypoints,map_shape,kpts_num=<span class="number">17</span>,radius=<span class="number">4</span>)</span>:</span></span><br><span class="line">print(all_keypoints)</span><br><span class="line"></span><br><span class="line">idx = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">discs = [[] <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(all_keypoints))]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"></span><br><span class="line">centers = [keypoints[i,:<span class="number">2</span>] <span class="keyword">for</span> keypoints <span class="keyword">in</span> all_keypoints <span class="keyword">if</span> keypoints[i,<span class="number">2</span>] &gt; <span class="number">0</span>]</span><br><span class="line">dists = np.zeros(map_shape+(len(centers),))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k, center <span class="keyword">in</span> enumerate(centers):</span><br><span class="line">dists[:,:,k] = np.sqrt(np.square(center-idx).sum(axis=<span class="number">-1</span>))</span><br><span class="line"><span class="keyword">if</span> len(centers) &gt; <span class="number">0</span>:</span><br><span class="line">inst_id = dists.argmin(axis=<span class="number">-1</span>)</span><br><span class="line">count = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> range(len(all_keypoints)):</span><br><span class="line"><span class="keyword">if</span> all_keypoints[j][i,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">discs[j].append(np.logical_and(inst_id==count, dists[:,:,count]&lt;=radius))</span><br><span class="line">count +=<span class="number">1</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">discs[j].append(np.array([]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line">discs = np.array(discs)</span><br><span class="line"><span class="keyword">return</span> discs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kpts_maps</span><span class="params">(keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line">kpts_maps = np.zeros(map_shape+(kpts_num,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"><span class="comment">#print(keypoints_skeletons[n,k,2])</span></span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,k,<span class="number">2</span>] &gt; <span class="number">0.</span>:</span><br><span class="line"></span><br><span class="line">disk_indices = discs[n][k]</span><br><span class="line">kpts_maps[disk_indices,k] = <span class="number">1.0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> kpts_maps</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">short_offsets</span><span class="params">(keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># discs.shape N*kpts_num*[indices of the specified keypoint]</span></span><br><span class="line"><span class="comment">#disk_mask_map = np.zeros(shape=map_shape+(kpts_num,),dtype='bool')</span></span><br><span class="line">short_offsets = np.zeros(map_shape+(kpts_num,<span class="number">2</span>,))</span><br><span class="line"><span class="comment">#map_shape = (H, W)</span></span><br><span class="line"><span class="comment"># [H,W,2]  for each pixel's (x,y) position index</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(kpts_num):</span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,k,<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">disk_indices = discs[n][k]</span><br><span class="line"></span><br><span class="line">kpt_x = keypoints_skeletons[n][k,<span class="number">0</span>]</span><br><span class="line">kpt_y = keypoints_skeletons[n][k,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">short_offsets[disk_indices,k,<span class="number">0</span>] = pixels_indices[disk_indices,<span class="number">0</span>] - kpt_x</span><br><span class="line">short_offsets[disk_indices,k,<span class="number">1</span>] = pixels_indices[disk_indices,<span class="number">1</span>] - kpt_y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> short_offsets</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mid_range_offsets</span><span class="params">(pair_wise_kpts,keypoints_skeletons,discs,map_shape,kpts_num=<span class="number">17</span>)</span>:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">pair_wisd_kpts: for COCO, edges of tree structures of body is `kpts_num-1`</span></span><br><span class="line"><span class="string">such as [[kpt_shoulder_r,kpt_ankle_r],[kpt_shoulder_r,nose],..,[]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">discs.shape Number of people*kpts_num*[disk indices of the specified keypoint]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">return [H,W,2*(kpts_num-1),2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line">directed_edges = []</span><br><span class="line"><span class="keyword">for</span> edge <span class="keyword">in</span> pair_wise_kpts:</span><br><span class="line">directed_edges.append(edge)</span><br><span class="line"><span class="comment">#directed_edges.append(edge[::-1])</span></span><br><span class="line"></span><br><span class="line">mid_offsets = np.zeros(map_shape+(len(directed_edges),<span class="number">2</span>,))</span><br><span class="line"></span><br><span class="line"><span class="comment"># [H,W,2]  for each pixel's (x,y) position index</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> edge_id, direct_edge <span class="keyword">in</span> enumerate(directed_edges):</span><br><span class="line">begin_kpt_id, end_kpt_id = direct_edge</span><br><span class="line">begin_kpt = keypoints_skeletons[n][begin_kpt_id]</span><br><span class="line">end_kpt = keypoints_skeletons[n][end_kpt_id]</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> begin_kpt[<span class="number">2</span>] &gt; <span class="number">0</span> <span class="keyword">and</span> end_kpt[<span class="number">2</span>] &gt;<span class="number">0</span>:</span><br><span class="line">disk_indices = discs[n][begin_kpt_id]</span><br><span class="line"><span class="comment">#print(end_kpt[0],pixels_indices[disk_indices])</span></span><br><span class="line">mid_offsets[disk_indices,edge_id,<span class="number">0</span>] = end_kpt[<span class="number">0</span>] - pixels_indices[disk_indices,<span class="number">0</span>]</span><br><span class="line">mid_offsets[disk_indices,edge_id,<span class="number">1</span>] = end_kpt[<span class="number">1</span>] - pixels_indices[disk_indices,<span class="number">1</span>]</span><br><span class="line"><span class="comment">#print(mid_offsets[disk_indices,edge_id])</span></span><br><span class="line"><span class="keyword">return</span> mid_offsets</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">visualize</span><span class="params">()</span>:</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment">#keypoints_skeletons1 = np.array([[[28,28,1],[61,61,1],[155,155,1]],[[20,20,1],[55,55,1],[6,6,1]]])</span></span><br><span class="line">keypoints_skeletons = np.array(</span><br><span class="line">[</span><br><span class="line">[[<span class="number">28</span>,<span class="number">228</span>,<span class="number">1</span>],[<span class="number">161</span>,<span class="number">361</span>,<span class="number">1</span>],[<span class="number">55</span>,<span class="number">455</span>,<span class="number">1</span>],[<span class="number">64</span>,<span class="number">54</span>,<span class="number">1</span>],[<span class="number">368</span>,<span class="number">550</span>,<span class="number">1</span>]],</span><br><span class="line"><span class="comment">#[[20,120,1],[55,55,1],[6,6,1]]</span></span><br><span class="line">]</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">pair_wise_kpts = [[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">4</span>],[<span class="number">3</span>,<span class="number">4</span>]]</span><br><span class="line">print(<span class="string">"keypoints coordiantes:\n"</span>,keypoints_skeletons,<span class="string">"\npair wise keypoints:\n"</span>,pair_wise_kpts)</span><br><span class="line"></span><br><span class="line">map_shape = (<span class="number">600</span>,<span class="number">600</span>)</span><br><span class="line">kpts_num = <span class="number">5</span></span><br><span class="line">radius = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">discs = get_keypoint_discs(keypoints_skeletons,map_shape,kpts_num,radius)</span><br><span class="line"></span><br><span class="line">kpts_heatmaps = kpts_maps(keypoints_skeletons,discs,map_shape,kpts_num)</span><br><span class="line">visual_kpts_heatmaps = np.amax(kpts_heatmaps,axis=<span class="number">-1</span>)</span><br><span class="line">short = short_offsets(keypoints_skeletons,discs,map_shape,kpts_num)</span><br><span class="line"></span><br><span class="line">offsets_magnitude = np.sqrt(np.square(short).sum(axis=<span class="number">-1</span>))</span><br><span class="line">visual_offsets_magnitude=np.max(offsets_magnitude,axis=<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># show mid_range_offset</span></span><br><span class="line">mid_offsets = mid_range_offsets(pair_wise_kpts,</span><br><span class="line">keypoints_skeletons,</span><br><span class="line">discs,</span><br><span class="line">map_shape,</span><br><span class="line">kpts_num)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mid_offsets_edge = mid_offsets[:,:,0::2,:] # directed edges</span></span><br><span class="line">mid_offsets_edge = mid_offsets   <span class="comment"># undirected edges</span></span><br><span class="line"></span><br><span class="line">mid_offsets_edge = mid_offsets_edge.astype(<span class="string">'int'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># (h,w,2)</span></span><br><span class="line">pixels_indices = np.rollaxis(np.indices(map_shape[::<span class="number">-1</span>]), <span class="number">0</span>, <span class="number">3</span>).transpose((<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># canvs</span></span><br><span class="line">background = np.zeros(map_shape+(<span class="number">3</span>,))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(len(discs)):</span><br><span class="line"><span class="keyword">for</span> edge_id, edge <span class="keyword">in</span> enumerate(pair_wise_kpts):</span><br><span class="line"><span class="keyword">if</span> keypoints_skeletons[n,edge[<span class="number">0</span>],<span class="number">2</span>]&gt;<span class="number">0</span> <span class="keyword">and</span> keypoints_skeletons[n,edge[<span class="number">1</span>],<span class="number">2</span>] &gt; <span class="number">0</span>:</span><br><span class="line">begin_disk_indices = discs[n][edge[<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (x,y) <span class="keyword">in</span> pixels_indices[begin_disk_indices]:</span><br><span class="line"><span class="comment"># sparse disk setting for better visualization</span></span><br><span class="line"><span class="keyword">if</span> x %<span class="number">8</span>==<span class="number">0</span> <span class="keyword">and</span> y%<span class="number">8</span> ==<span class="number">0</span>:</span><br><span class="line"><span class="comment">#continue</span></span><br><span class="line">begin_kpt = (x,y)</span><br><span class="line">                        <span class="comment"># note: mid_offsets_edge[y,x,edge_id,0] not: mid_offsets_edge[x,y,edge_id,0]</span></span><br><span class="line">end_kpt = (x + mid_offsets_edge[y,x,edge_id,<span class="number">0</span>], y + mid_offsets_edge[y,x,edge[<span class="number">0</span>],<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">cv2.line(background,begin_kpt,end_kpt,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),thickness=<span class="number">2</span>)</span><br><span class="line">                        </span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">plt.imshow(visual_kpts_heatmaps )</span><br><span class="line">plt.title(<span class="string">"heatmaps of keypoints"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">plt.imshow(visual_offsets_magnitude)</span><br><span class="line">plt.title(<span class="string">"the magnitude of short offsets"</span>)</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">plt.imshow(background)</span><br><span class="line">plt.title(<span class="string">"mid_range_offsets of keypoints"</span>)</span><br><span class="line"></span><br><span class="line">plt.savefig(<span class="string">'visualization.png'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">visualize()</span><br></pre></td></tr></table></figure><pre><code>keypoints coordiantes: [[[ 28 228   1]  [161 361   1]  [ 55 455   1]  [ 64  54   1]  [368 550   1]]] pair wise keypoints: [[0, 1], [1, 2], [2, 4], [3, 4]][[[ 28 228   1]  [161 361   1]  [ 55 455   1]  [ 64  54   1]  [368 550   1]]]Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).</code></pre><p><img src="/2019/08/19/Reproduce-PersonLab/output_9_2.png" alt="visualization"> <!-- <img src="/2019/08/19/Reproduce-PersonLab/output_9_2.png" class="" title="[visualization]"> --&gt;</p><p>经过了很多的BUG和debug的过程，终于可视化出了自己期望的效果</p><p>会发现，我们可以得到了和论文Ｇ－ＲＭＩ和 PersonLab 中一样的disk的效果： short-offsets指向圆心，其模长的分布为：中心为０，向外靠近ｄｉｓｋ边缘时增大，边缘最大，然后陡然减为0 （是不是可以考虑优化一下？） mid-offsets: 从起点关键点的disk内的像素点指向，终点的关键点位置的向量场</p><p>待续 ...</p>--></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;personlab复现过程&quot;&gt;PersonLab复现过程&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;我打算记录整个复现的心路历程,和代码实践遇到的问题 正在写完这一行字的我, 什么代码都没写~ 所以以下的文字是一篇&lt;code&gt;记叙文&lt;/code&gt;, 默认的叙述手法是&lt;code&gt;顺序&lt;/code&gt;&lt;/p&gt;&lt;/blockquote&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Reproduce" scheme="http://senyang-ml.github.io/tags/Reproduce/"/>
    
  </entry>
  
  <entry>
    <title>Chelsea Finn</title>
    <link href="http://senyang-ml.github.io/2019/07/21/Chelsea-Finn/"/>
    <id>http://senyang-ml.github.io/2019/07/21/Chelsea-Finn/</id>
    <published>2019-07-21T15:55:16.000Z</published>
    <updated>2020-03-13T11:00:57.000Z</updated>
    
    <content type="html"><![CDATA[<p>Chelsea Finn 这个人很厉害~</p><a id="more"></a><p>了解她的学术经历发现，她2018年毕业于 University of California, Berkeley, Berkeley CA，2014年开始读博，也就是4年博士毕业。</p><p>往前追溯，她从2010年到2014年在Massachusetts Institute ofTechnology, Cambridge MA读完本科，拿到学士学位，也就是说，她当初选择了直博，或者是硕博连读。</p><p>为什么说她厉害，主要是因为她17年在ICML上发表了<code>Model-Agnostic Meta-Learning for Fast Adaptation of DeepNetworks</code>（简称为<code>MAML</code>）。这篇论文之后在<code>meta-learning</code>相关领域很有影响力，其实就连最近流行起来的可导神经网络架构搜索<code>DARTS</code>的二阶近似也在一定程度上借鉴了这篇的元学习算法。你能想到这是她在博二或者（博三）时的发的？如果按照中国正常上学的年龄计算，发<code>MAML</code>时候的她应该是25岁吧。</p><p>附： MAML: https://arxiv.org/abs/1703.03400 (icmL 2017) Chelsea Finn: https://people.eecs.berkeley.edu/~cbfinn/</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Chelsea Finn 这个人很厉害~&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch使用torch.nn.DataParallel进行多GPU训练的一个BUG，已解决</title>
    <link href="http://senyang-ml.github.io/2019/07/20/pytorch-multigpu/"/>
    <id>http://senyang-ml.github.io/2019/07/20/pytorch-multigpu/</id>
    <published>2019-07-20T10:42:12.000Z</published>
    <updated>2020-03-13T10:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<p>解决了PyTorch 使用torch.nn.DataParallel 进行多GPU训练的一个BUG:</p><p><strong>模型(参数)和数据不在相同设备上</strong> <a id="more"></a> 我使用<code>torch.nn.DataParallel</code>进行多GPU训练时出现了一个BUG，困扰了我许久：</p><p><code>RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'; but device 1 does not equal 0 (while checking arguments for cudnn_convolution)</code></p><p>这个错误表明, <code>input</code>数据在<code>device 1</code> 上, 而模型的参数在<code>device 0</code> 上 (暗示数据是被拆分到了各个GPU上,但是BUG出现位置的此处参数可能没有成功复制到其他GPU上, 或者说, 还是调用了复制前的那个参数地址)</p><p>因为模型比较复杂，继承与调用太多，之前调试了好久, 也没有解决掉, 在Github上有一个issue和我的问题很像: https://github.com/pytorch/pytorch/issues/8637 但是我还是没有找到自己的bug在哪里.</p><h4 id="今天-我又准备再此尝试解决它">今天, 我又准备再此尝试解决它</h4><p>经过6个小时的<code>print调试法</code>以及后面关键的VScode的<code>Debug</code>功能, 我大功告成,找到了问题所在,原来</p><p>我的<code>A(nn.Module)</code>类的<code>forward</code> 前向计算函数里面, 有一处调用了一个该类的列表<code>self.cell_fabrics</code>, 其列表的元素是通过<code>self.cell_fabrics = [self.cell_1, self.cell_2,...,self.cell_n]</code> 来赋值的,其中每个<code>self.cell</code>也是<code>nn.Module</code>类</p><p>即用<code>self.cell_fabrics = [self.cell_0_0, self.cell_0_1, … , self.cell_3_0, self.cell_3_1,…, self.cell_5_0]</code> 这样的方式,将所有的<code>cell类</code>放到<code>A类</code>的一个<code>列表属性</code>中, 而当整个<code>A类</code>通过<code>torch.nn.DataParallel</code>被复制了一份放到设备<code>cuda:1</code>上以后, 且在预计在设备<code>cuda:1</code>上执行下面代码段时:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span></span></span><br><span class="line"><span class="function">    <span class="title">for</span> <span class="title">layer</span> <span class="title">in</span> <span class="title">self</span>.<span class="title">cell_fabrics</span>:</span></span><br><span class="line">        <span class="keyword">for</span> cell <span class="keyword">in</span> layer:</span><br><span class="line">           y = cell (x,)</span><br></pre></td></tr></table></figure><p>经验证，<code>x</code>是在设备<code>cuda:1</code> 上面, 但是 <code>cell</code> 中的参数却明显都在 <code>cuda:0</code>上</p><p>也就是说:</p><p><strong>此时 <code>self.cell_fabrics</code> 的列表中保存的各个对象 (<code>self.cell</code>) 的地址，还是指向在没有进行<code>torch.nn.DataParallel</code>之前的<code>nn.Module</code> 的那些<code>self.cell</code>, 而<code>nn.DataParallel</code>类的<code>nn.Module</code>的参数都默认存放在<code>device(type='cuda',index=0)</code>上 .</strong></p><p><code>torch.nn.DataParallel(model,device_ids=[range(len(gpus))])</code>的机制是, 将属于<code>nn.Module</code>类的<code>model</code>以及其广播的所有<code>nn.Module</code>子类的上的所有参数,复制成<code>len(gpus)</code>份,送到各个GPU上. 这种广播机制的范围是注册(registered)成为其属性的<code>nn.Module</code>子类, <strong>属性为列表list中的各个对象是不会被复制的, 所以其list中的对象还是存放在默认设备<code>device 0</code>上</strong></p><p>所以 在使用<code>torch.nn.DataParallel</code>进行多GPU训练的时候, 请注意：所有属于模型参数的模块以及其子模块必须以<code>nn.Module</code>的类型注册为模型的属性, 如果需要一个列表来批量存放子模块或者参数的话, 请采用<code>nn.ModuleList</code>或者<code>nn.ModuleDict</code>这样的继承了<code>nn.Module</code>的类来进行定义, 并且在<code>forward(self,)</code>前向传播的过程中，需要直接调用属于 <code>nn.Module</code>,<code>nn.ModuleList</code>或者<code>nn.ModuleDict</code> 这样的属性。</p><p>那么<code>torch.nn.DataParallel</code>将会正常地将模型参数准确复制到多个GPU上, 并根据数据的<code>batchsize</code>的大小平分成GPU的数量分别送到相应的GPU设备上,</p><p>然后运用<strong>多线程</strong>的方式, 同时对这些数据进行加工处理, 然后收集各个GPU上最终产生对模型的各参数的梯度, 最后汇总到一起更新原模型的参数!</p><p>参考: 1. https://github.com/pytorch/pytorch/issues/8637 2. https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;解决了PyTorch 使用torch.nn.DataParallel 进行多GPU训练的一个BUG:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;模型(参数)和数据不在相同设备上&lt;/strong&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="PyTorch" scheme="http://senyang-ml.github.io/tags/PyTorch/"/>
    
      <category term="Multi-gpus" scheme="http://senyang-ml.github.io/tags/Multi-gpus/"/>
    
      <category term="Bug" scheme="http://senyang-ml.github.io/tags/Bug/"/>
    
  </entry>
  
  <entry>
    <title>G-RMI-&gt; PersonLab -&gt; PifPaf  -- Human Pose Estimation</title>
    <link href="http://senyang-ml.github.io/2019/07/17/pifpaf/"/>
    <id>http://senyang-ml.github.io/2019/07/17/pifpaf/</id>
    <published>2019-07-17T03:35:08.000Z</published>
    <updated>2020-03-13T10:58:00.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="g-rmi--personlab---pifpaf-composite-fields-for-human-pose-estimation---cvpr-2019-论文解读">G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读</h3><blockquote><p>博客地址：https://yangsenius.github.io/blog/2-pifpaf/</p></blockquote><p>arxiv地址: https://arxiv.org/abs/1903.06593 github地址: https://github.com/vita-epfl/openpifpaf</p><p>今年的CVPR19的论文最近已经在CVF Openaccess 网站上放出来了。 <a id="more"></a></p><p>还记得去年18CVPR论文出来的时候，我把所有有关的人体姿态估计的论文的题目和概要大致都看了，得出的一个浅显的结论就是：3D姿态估计、密集姿态估计要流行起来了。这是因为在去年CVPR18的论文中，出现了大量的3D有关的论文而少有2D姿态估计研究（比如在MPII, COCO keypoint数据集上的方法挺少，可能2d姿态的都去发了ECCV18）。</p><p>而今年19CVPR的姿态估计好像又呈现出一次小爆发</p><p>COCO数据集上的性能又来到了一次新高：似乎74mAP已经被突破了（HRNet, 0.770 mAP, ECSI, 0.746 mAP）。。。</p><p>各位研究者们，是不是感觉到了精度上、性能上的压力。。。深度调参还是方法革新，这是个问题.</p><p>众多论文中，我先阅读了这篇，OpenPIFPAF。 因为它奇怪的名字好像是茫茫论文海中出现的那个与众不同的一篇，吸引我去一探Ta的全貌与究竟</p><p>读完后，我觉得OpenPifpaf继承了几篇姿态估计论文的工作：</p><ul><li>openpose</li><li>G-RMI</li><li>PersonLAB (应该说大部分核心的想法来自于PersonLab)</li></ul><p>并致力于解决几个棘手的问题： - Bottom-up的多人姿态解析问题 - 自动驾驶中，图像中小尺寸人体的问题</p><p>其实很有必要介绍一下先前的工作</p><h2 id="g-rmi">G-RMI</h2><p>G-RMI 是google的一篇自上而下处理姿态估计问题的开篇</p><p>通过Faster-RCNN检测得到包含单个人体的bounding box，然后再进行单人姿态估计</p><figure><img src="https://img-blog.csdnimg.cn/20190322204206204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><p>本论文在预测<span class="math inline">\(K\)</span>个表示置信度的heatmaps之外，又引入了offset fields的方法，用<span class="math inline">\(2\times K\)</span>个heatmaps表示，即每个heatmap的位置上预测一个<span class="math inline">\(F_k(x_{i})=l_k-x_{i}\)</span>的位移偏量，用<span class="math inline">\(l_k\)</span>来表示真实位置，其中<span class="math inline">\(x_i,k \in \mathbb{Z}_+^2\)</span> <span class="math inline">\(i,k\)</span>表示位置索引和关键点类型。</p><p><span class="math display">\[h_{k}\left(x_{i}\right)=1 \text { if }\left\|x_{i}-l_{k}\right\| \leq R\]</span></p><p><span class="math display">\[F_k(x_{i})=l_k-x_{i}\]</span> After generating the heatmaps and offsets, we aggregate them to produce highly localized activation maps <span class="math inline">\(f_{k}\left(x_{i}\right)\)</span> as follows: <span class="math display">\[f_{k}\left(x_{i}\right)=\sum_{j} \frac{1}{\pi R^{2}} G\left(x_{j}+F_{k}\left(x_{j}\right)-x_{i}\right) h_{k}\left(x_{j}\right)\]</span></p><p>其中第三个公式中的<span class="math inline">\(G(\cdot)\)</span>论文中说它是双线性插值核，并用霍夫投票的形式。在今年19CVPR的openpifpaf论文，又再次利用这个公式，不过用一个高斯核来代替了<span class="math inline">\(G(\cdot)\)</span>函数，我从中推断出这是起到了平滑取值的作用，就像我们在构造产生grountruth heatmaps那样的做法。下面的<span class="math inline">\(\pi R^{2}​\)</span>是一个归一化，和高斯核那样类似。</p><p>注：这个<span class="math inline">\(G(\cdot)\)</span>函数其实是很多人理解这篇论文的绊脚石。实际上,这就是对上采样后的heatmaps再次进行一次平滑.</p><blockquote><p>A different approach addressing this issue would be to predict activation maps,as in [27], which allow for multiple predictions of the same keypoint. However, the size of the activation maps, and thus the localization precision, is limited by the size of the net’s output feature maps, which is a fraction of the input imagesize, due to the use of max-pooling with decimation.In order to address the above limitations, we adopt acombined classification and regression approach. For each spatial position, we first classify whether it is in the vicin-ity of each of the K keypoints or not (which we call a “heatmap”), then predict a 2-D local offset vector to get amore precise estimate of the corresponding keypoint loca-tion. Note that this approach is inspired by work on object detection, where a similar setup is used to predict bounding boxes, e.g. [14, 37]. Figure 2 illustrates these three output channels per keypoint. <img src="https://img-blog.csdnimg.cn/20190322204225926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p></blockquote><p>训练loss是：</p><p><span class="math display">\[L(\theta)=\lambda_{h} L_{h}(\theta)+\lambda_{o} L_{o}(\theta) \]</span></p><p><span class="math inline">\(\lambda_{h}=4\)</span> and <span class="math inline">\(\lambda_{o}=1\)</span> is a scalar factor to balance.</p><p>We use a single ResNet model with two convolutional output heads. The output of the firshead passes through a sigmoid function to yield the heatmap probabilities <span class="math inline">\(h_{k}\left(x_{i}\right)\)</span> for each position <span class="math inline">\(x_{i}\)</span> and each keypoint <span class="math inline">\(k\)</span> . The training target <span class="math inline">\(\overline{h}_{k}\left(x_{i}\right)\)</span> is a map of zeros and ones, with <span class="math inline">\(\overline{h}_{k}\left(x_{i}\right)=1\)</span> if <span class="math inline">\(\left\|x_{i}-l_{k}\right\| \leq R\)</span> and 0 otherwise. The corresponding loss function <span class="math inline">\(L_{h}(\theta)\)</span> is the sum of logistic losses for each position and keypoint separately.</p><p><span class="math display">\[L_{o}(\theta)=\sum_{k=1 : K} \sum_{i :\left\|l_{k}-x_{i}\right\| \leq R} H\left(\left\|F_{k}\left(x_{i}\right)-\left(l_{k}-x_{i}\right)\right\|\right)\]</span></p><p>where <span class="math inline">\(H(u)\)</span> is the Huber robust loss, <span class="math inline">\(l_{k}\)</span> is the position of the <span class="math inline">\(k\)</span> -th keypoint, and we only compute the loss for positions <span class="math inline">\(x_{i}\)</span> within a disk of radius <span class="math inline">\(R\)</span> from each keypoint.</p><p>Huber robust loss的函数图像为：</p><figure><img src="https://img-blog.csdn.net/20151229152931179" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="g-rmi的开创行思路keypoints-location-disk-mask-logistic-classication-and-short-range-offset解决了下采样导致对量化误差问题">G-RMI的开创行思路:keypoints location disk mask logistic classication and short-range offset解决了下采样导致对量化误差问题!!</h2><p><strong>作者构造出<code>0,1</code>取值构成的<code>kepoints location  masks heatmaps</code>和<code>某关键点对应对heatmap的每个grid位置相对于其真实位置的偏移</code>的<code>分类</code>加<code>回归</code>预测方法!!</strong></p><p>这一点<code>PersonLab</code>和<code>PifPaf</code>都沿袭了这一思路</p><p>而<code>PersonLab</code>在此基础上,为了解决多人关联肢体的算法设计问题,又继续引入了<code>mid-range pairwise offset</code>来针对<code>instance association</code>这一问题, 可以说将将<code>G-RMI</code>的方法拓展到多人问题上.</p><h1 id="personlab">PersonLab</h1><p>PersonLab在构造监督标签和网络预测表示上面下了不少功夫。 公式总览：</p><p><span class="math inline">\(\textbf{K heatmaps: } \qquad\qquad\qquad\qquad\qquad p_{k}(x)=1 \text { if } x \in \mathcal{D}_{R}\left(y_{j, k}\right), \mathcal{D}_{R}(y)=\{x :\|x -y\| \leq R\}\)</span></p><p><span class="math inline">\(\textbf{K short-range 2-D offset fields: } \qquad S_{k}(x)=y_{j, k}-x\)</span></p><p><span class="math inline">\(\textbf{K Hough score maps: }\qquad \qquad \qquad h_{k}(x)=\frac{1}{\pi R^{2}} \sum_{i=1 : N} p_{k}\left(x_{i}\right) B\left(x_{i}+S_{k}\left(x_{i}\right)-x\right)\)</span></p><p><span class="math inline">\(\textbf{2(K-1) mid-range 2-D offset fields: }\quad M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}_{R}\left(y_{j, k}\right)\right], \text{from k-th to l-th}\)</span></p><p><span class="math inline">\(\textbf{K long-range 2-D offset fields: }\qquad \qquad L_{k}(x)=y_{j, k}-x\)</span></p><p><img src="https://img-blog.csdnimg.cn/20190714160727112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">在Personlab中,<span class="math inline">\(h_k(x)\)</span>,即hough投票后对高精度score maps并不提供实例相关的信息, 而需要一种机制来<code>group together the keypoints belonging to each individual instance</code>. 因此, 作者继续构造了<code>mid-range pairwise offeset</code> 负责<code>connect the dots</code>.</p><p><code>mid-range pairwise offeset</code> 也是 2D的offset fields: <span class="math inline">\(M_{k, l}(x)\)</span> &gt; We compute 2<span class="math inline">\((K-1)\)</span> such offset fields, one for each directededge connecting pairs <span class="math inline">\((k, l)\)</span> of keypoints which are adjacent to each other in a tree-structured kinematic graph of the person, see Figs <img src="https://img-blog.csdnimg.cn/20190714162530980.png" alt="在这里插入图片描述"></p><p>这里的2<span class="math inline">\((K-1)=2\times16\)</span> 个向量场指的是 上图16种肢体连接的正反2种方向:<span class="math inline">\((k,l)和(l,k)\)</span>的偏移向量场 <span class="math inline">\(M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}_{R}\left(y_{j, k}\right)\right]\)</span> , 这个向量场还是在disk对半径范围内的.</p><p><code>Recurrent offset refinement</code>： 作者在预测一些大尺寸人体时,有时候mid-range pairwise offsets很长,精度可能不准,所以用了<code>Recurrent offset refinement</code>:</p><p><span class="math display">\[M_{k, l}(x) \leftarrow x^{\prime}+S_{l}\left(x^{\prime}\right), \text { where } x^{\prime}=M_{k, l}(x),\]</span> 来进一步提高精度,迭代2次上述对公式. 其中,<span class="math inline">\(S_{l}\left(x^{\prime}\right)\)</span>是short-range offset.</p><h3 id="fast-greedy-decoding">Fast greedy decoding</h3><p>有了所有人体的关键点的预测位置和每个关键点的<code>mid-range pairwise offset</code>, 接下来要做的就是 进行将属于同一个人体的关键点组合成一个实例的机制。</p><p><code>PersonLab</code> 构造出一个<code>优先级的队列</code>，根据Hough score maps <span class="math inline">\(h_{k}(x)\)</span>上的<code>局部最大值</code>的位置（这里强调局部最大值，是因为可能会有false positive 的位置）以及其score高于一定的阈值的（实验取0.01） 来<code>按得分大小顺序放入队列</code>，这些放入队列中的关键点应该被称为了<code>seed</code>点（pifpaf实际上没有解释清楚这个seed点），从最高响应值的seed点位置开始，以此不断找到其在<code>tree-structure</code>上的连接关键点。</p><p>在这个算法迭代的每一步中，如果发现当前的<code>seed</code>关键点落入了已经关联到先前某个人体<span class="math inline">\(j^{\prime},\)</span>的某个<span class="math inline">\(k\)</span>类型对seed关键点的 <span class="math inline">\(\mathcal{D}_{r}\left(y_{j^{\prime}, k}\right)\)</span> 半径内，那么就意味着，这个半径区域内有可能有两个同样类型的关键点，那么我们就可以开辟一个新的人体实例<span class="math inline">\(j\)</span>,作为另外一个人体的<span class="math inline">\(k\)</span>类型关键点作为<code>seed</code>. 其中,通过<code>seed</code>点计算与其<code>adjacent</code>的关键点的公式是:<span class="math inline">\(y_{j, l}=y_{j, k}+M_{k, l}\left(y_{j, k}\right)\)</span>.</p><p>这种机制对于所有关键点的是公平的,即 根据高得分的关键点位置作为起始seed,(我想起了Associative Embedding 对待起始点是从头部开始), 然而实际中容易检测的点往往是起始位置点, 这种方法能够一定程度处理遮挡问题.</p><blockquote><p>the position <span class="math inline">\(x_{i}\)</span> of the current candidate detection seed of type <span class="math inline">\(k\)</span> is within a disk <span class="math inline">\(\mathcal{D}_{r}\left(y_{j^{\prime}, k}\right)\)</span> of the corresponding keypoint of previously detected person instances <span class="math inline">\(j^{\prime},\)</span> then we reject it;</p></blockquote><p>Personlab的decoding 大致代码如下: <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"> config.EDGES = [</span><br><span class="line">        (<span class="number">0</span>, <span class="number">14</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">13</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">4</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="number">14</span>, <span class="number">16</span>),</span><br><span class="line">        (<span class="number">13</span>, <span class="number">15</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">7</span>),</span><br><span class="line">        (<span class="number">10</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">8</span>),</span><br><span class="line">        (<span class="number">11</span>, <span class="number">12</span>),</span><br><span class="line">        (<span class="number">8</span>, <span class="number">9</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">6</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_skeletons</span><span class="params">(keypoints, mid_offsets)</span>:</span></span><br><span class="line">    <span class="comment"># keypoints 是hough score maps 所有局部最大值位置产生对所有候选关键点  </span></span><br><span class="line">    <span class="comment"># keypoints 数组每个元素为&#123;'xy':[x,y],'id':k,'conf':score&#125;</span></span><br><span class="line">    keypoints.sort(key=(<span class="keyword">lambda</span> kp: kp[<span class="string">'conf'</span>]), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 按照从大到小排序</span></span><br><span class="line">    skeletons = []</span><br><span class="line">    <span class="comment"># skeletons 表示多个人体骨架,每个元素是单人的骨架坐标集合</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># config.EDGES 表示16个单向的骨架连接边,dir_edges 构造出双向的连接边</span></span><br><span class="line">    dir_edges = config.EDGES + [edge[::<span class="number">-1</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> config.EDGES]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为每个关键点生成跟它相连的关键点的集合,比如 左肘部有[左肩膀,左手腕]</span></span><br><span class="line">    skeleton_graph = &#123;i:[] <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP)&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">            <span class="keyword">if</span> (i,j) <span class="keyword">in</span> config.EDGES <span class="keyword">or</span> (j,i) <span class="keyword">in</span> config.EDGES:</span><br><span class="line">                skeleton_graph[i].append(j)</span><br><span class="line">                skeleton_graph[j].append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优先级队列</span></span><br><span class="line">    <span class="keyword">while</span> len(keypoints) &gt; <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># pop出最大置信度的候选关键点</span></span><br><span class="line">        kp = keypoints.pop(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 判断该类型(根据id)关键点有没有落入了之前的某个骨架的同类关键点上</span></span><br><span class="line"><span class="comment"># 计算距离是否在r=10(论文里面提到)内</span></span><br><span class="line">        <span class="keyword">if</span> any([np.linalg.norm(kp[<span class="string">'xy'</span>]-s[kp[<span class="string">'id'</span>], :<span class="number">2</span>]) &lt;= <span class="number">10</span> <span class="keyword">for</span> s <span class="keyword">in</span> skeletons]):</span><br><span class="line">            <span class="comment"># 如果是,则抑制该关键点(非极大值抑制)</span></span><br><span class="line">            <span class="comment">#　如果否，则认为该类型关键点属于另外一个人体，那么就开辟新的实例</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment">#　构造新的骨架</span></span><br><span class="line">        this_skel = np.zeros((config.NUM_KP, <span class="number">3</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将该类型关键点的坐标和置信度赋值给该骨架</span></span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], :<span class="number">2</span>] = kp[<span class="string">'xy'</span>]</span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], <span class="number">2</span>] = kp[<span class="string">'conf'</span>]</span><br><span class="line">        </span><br><span class="line"><span class="comment"># 此处 引入该函数</span></span><br><span class="line">    <span class="comment">###########################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">iterative_bfs</span><span class="params">(graph, start, path=[])</span>:</span></span><br><span class="line">    <span class="string">'''iterative breadth first search from start'''</span></span><br><span class="line">    <span class="comment"># 此处的graph是所有的每个关键点与它相连的关键点的构成的多叉树结构</span></span><br><span class="line">    <span class="comment"># start 表示关键点的类型</span></span><br><span class="line">    <span class="comment"># 构造队列</span></span><br><span class="line">    q=[(<span class="literal">None</span>,start)]</span><br><span class="line">    </span><br><span class="line">    visited = []</span><br><span class="line">    <span class="keyword">while</span> q:</span><br><span class="line">        <span class="comment"># 拿出第一个元素v (关键点l（子点）,关键点k（父点）) 初始 关键点l=None</span></span><br><span class="line">        v=q.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 如果其对应的关键点k之前没访问过</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> v[<span class="number">1</span>] <span class="keyword">in</span> visited:</span><br><span class="line">            <span class="comment"># 记录访问</span></span><br><span class="line">            visited.append(v[<span class="number">1</span>])</span><br><span class="line">            <span class="comment"># 记录路径</span></span><br><span class="line">            path=path+[v]</span><br><span class="line">            <span class="comment"># 将在图结构graph中,找到关键点k直接相连的所有子关键点k_1,k_2,...</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 放到队列中[(关键点l_1关键点k_1),(关键点l_2,关键点k_2),...]</span></span><br><span class="line">            q=q+[(v[<span class="number">1</span>], w) <span class="keyword">for</span> w <span class="keyword">in</span> graph[v[<span class="number">1</span>]]]</span><br><span class="line">        <span class="comment">#  然后子点变成父点，下一次迭代生出更多的子点</span></span><br><span class="line">        <span class="comment"># 循环,这样的话,不论初始点是什么,我们都可以找到一个包含完整人体树结构tree-structure的路径,</span></span><br><span class="line">        <span class="comment"># 包含所有 [(None,关键点k),(关键点k,关键点k_1),(关键点1_x,关键点k_2),...,(关键点16_x,关键点16)] 一共会有17个path, 然而就会多余出1个连接,就是初始连接.</span></span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">    <span class="comment">#假如是Lwrist 为起始点的话: path=[('Lwrist', 'Lelbow'), ('Lelbow', 'Lshoulder'), ('Lshoulder', 'nose'), ('Lshoulder', 'Lhip'), ('nose', 'Rshoulder'), ('nose', 'Reye'), ('nose', 'Leye'), ('Lhip', 'Lknee'), ('Rshoulder', 'Relbow'), ('Rshoulder', 'Rhip'), ('Reye', 'Rear'), ('Leye', 'Lear'), ('Lknee', 'Lankle'), ('Relbow', 'Rwrist'), ('Rhip', 'Rknee'), ('Rknee', 'Rankle')]</span></span><br><span class="line"><span class="comment">###########################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 此处对skeleton_graph是每个关键点与它相连的关键点的图结构</span></span><br><span class="line">        path = iterative_bfs(skeleton_graph, kp[<span class="string">'id'</span>])[<span class="number">1</span>:] <span class="comment"># 此处1:开始,即去掉多余的连接</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> path:</span><br><span class="line">            <span class="comment"># 判断第一条边的起始关键点的置信度是不是为0, 因为已经this_skel[kp['id'], 2] = kp['conf'], 所以第一次肯定不为0;但是如果第一次之后没有找到新的关键点的话,那么以后的循环都要continue</span></span><br><span class="line">            <span class="keyword">if</span> this_skel[edge[<span class="number">0</span>],<span class="number">2</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># dir——edges 索引0-31</span></span><br><span class="line">            mid_idx = dir_edges.index(edge)</span><br><span class="line">            <span class="comment"># mid_offsets shape=[h,w,32x2] 输出feature map的每个位置</span></span><br><span class="line">            offsets = mid_offsets[:,:,<span class="number">2</span>*mid_idx:<span class="number">2</span>*mid_idx+<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 计算当前给定的关键点基于grid的位置</span></span><br><span class="line">            from_kp = tuple(np.round(this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>]).astype(<span class="string">'int32'</span>))</span><br><span class="line">            <span class="comment"># 计算当前关键点的位置，加上，此位置针对该类型有向连接的预测的偏移x,y向量，得到的候选位置，</span></span><br><span class="line">            <span class="comment"># 比如我们当前的关键点类型为左肘部，有向连接为（左肘，左手腕），那么根据该位置的mid-offset预测的左手腕的位置就知道了，但是这是根据mid-offset预测得到的位置，如果我们在hough score maps上也同样在该附近位置预测到了左手腕的位置，那么就说明mid-offset的预测也是合理的。</span></span><br><span class="line">            proposal = this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>] + offsets[from_kp[<span class="number">1</span>], from_kp[<span class="number">0</span>], :]</span><br><span class="line"><span class="comment"># 所以接下的matches，是在优先级队列中找到候选的所有左手腕（假设）的关键点，并记录其在优先级队列中的位置i。（找到最佳匹配点后，需要把它从队列中pop出）</span></span><br><span class="line">            matches = [(i, keypoints[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keypoints)) <span class="keyword">if</span> keypoints[i][<span class="string">'id'</span>] == edge[<span class="number">1</span>]] <span class="comment"># edge[1]表示有向线段的末端点（左手腕）</span></span><br><span class="line"><span class="comment"># 通过计算mid-offset预测出的有向线段末端点位置，与score maps 上该类型的位置的距离，我们可以得到很有可能的匹配关键点（&lt;32）</span></span><br><span class="line">            matches = [match <span class="keyword">for</span> match <span class="keyword">in</span> matches <span class="keyword">if</span> np.linalg.norm(proposal-match[<span class="number">1</span>][<span class="string">'xy'</span>]) &lt;= <span class="number">32</span>]</span><br><span class="line">            <span class="comment"># 找不到匹配点的话，就队列中的下一个关键点</span></span><br><span class="line">            <span class="keyword">if</span> len(matches) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 根据匹配到的该类型关键点与预测的距离进行排序，距离越小，排序靠前</span></span><br><span class="line">            matches.sort(key=<span class="keyword">lambda</span> m: np.linalg.norm(m[<span class="number">1</span>][<span class="string">'xy'</span>]-proposal))</span><br><span class="line"><span class="comment"># 排在最前面的关键点，即matches[0]，的坐标作为grid上的位置</span></span><br><span class="line">            to_kp = np.round(matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'xy'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 记录其confidence</span></span><br><span class="line">            to_kp_conf = matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'conf'</span>]</span><br><span class="line">            <span class="comment"># 根据其在队列中的位置，将其pop出来</span></span><br><span class="line">            keypoints.pop(matches[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 把该匹配到的点的位置记录到骨架该处有向线段的末端位置上</span></span><br><span class="line">            </span><br><span class="line">            this_skel[edge[<span class="number">1</span>],:<span class="number">2</span>] = to_kp</span><br><span class="line">            this_skel[edge[<span class="number">1</span>], <span class="number">2</span>] = to_kp_conf</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 此处我们进行path路径上的顺寻，进入下一个有向连接，</span></span><br><span class="line">            <span class="comment"># 这里的path = iterative_bfs(skeleton_graph, kp['id']) 函数是个非常巧妙的扩散方式，</span></span><br><span class="line">            <span class="comment">#它从骨架上的任意一点出发，按照固定的顺序散播到骨架上的所有16个有向连接上（上游点（父），下游点（子））。</span></span><br><span class="line">            <span class="comment">#那么根据起始的任意一种人体关键点，这一个算法就可以在优先级队列中将很有可能属于该人体的所有关键点的候选点group到该人体上。</span></span><br><span class="line"><span class="comment"># 但是,如果考虑到有一种枢纽的关键点，没有找到合适的点，那么不完整的人体骨架将会产生，</span></span><br><span class="line">            <span class="comment"># 但是在队列后面属于人体的关键点还会产生一个候选骨架，那么非极大值抑制skeleton就是必要的一项了</span></span><br><span class="line">        skeletons.append(this_skel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> skeletons</span><br><span class="line"></span><br><span class="line"> <span class="comment">## https://github.com/octiapp/KerasPersonLab/blob/master/post_proc.py</span></span><br><span class="line"> <span class="comment">## https://github.com/senyang-ml/OKS-NMS</span></span><br></pre></td></tr></table></figure> # OpenPIFPAF</p><p>在G-RMI、PersonLab的基础上，引入了PAF和PIF 复合结构，实际上具备显式含义的向量场。</p><p>即在图像每个location的像素位置，寄托更多的复合含义，编码具有直观含义的向量</p><p>PIF针对每一种类型的关键点，PAF针对每一种关联肢体（两个有关part的连接连线）</p><p>对于COCO，有17个关键点，19个连接（论文默认设置）</p><p><strong>PIF和PAF是训练Encoder网络用的监督标签，如何构造这两种标签，来指导监督Encoder网络训练，是本论文很关键的部分，后面的decoder 部分完全依赖于PIF和PAF的预测值。本文的PIF和PAF设计，可谓是将人工先验知识发挥到了极致！</strong></p><h2 id="pif">PIF</h2><p>PIF是个<span class="math inline">\(K\times H \times W \times 5\)</span>的结构， K表示关键点的数量，COCO为17个</p><p>They are composed of a scalar component for confidence, a vector component that points to the closest body part of the particular type and another scalar component for the size of the joint. More formally, at every output location spread <span class="math inline">\(b​\)</span> (details in Section 3.4<span class="math inline">\()​\)</span> and a scale <span class="math inline">\(\sigma​\)</span> and can be written as</p><p><span class="math display">\[\mathbf{p}^{i j}=\left\{p_{c}^{i j}, p_{x}^{i j}, p_{y}^{i j}, p_{b}^{i j}, p_{\sigma}^{i j}\right\}​\]</span></p><p>因为作者主要针对小尺寸人体图片，那么得到的置信度图 confidence map 是非常粗糙的，为了进一步地提升confidence map 的定位精度，作者使用偏量位移maps 来提升confidence map 的分辨率，得到一个<code>高分辨率的confidence map</code>（这个高分辨率的置信图发挥着<code>产生姿态种子点</code>和<code>评价候选点得分</code>的作用），如下公式： <span class="math display">\[f(x, y)=\sum_{i j} p_{c}^{i j} \mathcal{N}\left(x, y | p_{x}^{i j}, p_{y}^{i j}, p_{\sigma}^{i j}\right)\]</span> 这个公式我发现，很大程度上借鉴了G-RMI中的上述公式。用一个未归一化的高斯核，以及可学习的范围因子<span class="math inline">\(\sigma\)</span>来代替G-RMI中的双线性插值核以及归一化的分母，通过上述公式计算一个高分辨率的图（这里的高分辨率尺寸应该是原图尺寸，因为关键点的坐标标签真实值是基于原图的像素大小等级的）的响应值，我个人理解为是一种利用预测值的高斯上采样插值法（<span class="math inline">\(p_{x}^{i j}, p_{y}^{i j}\)</span>是预测出的小尺寸置信图每个位置<span class="math inline">\((i,j)\)</span>基于其自身grid位置<span class="math inline">\((i,j)\)</span>的偏移量，<span class="math inline">\(p_{\sigma}^{i j}\)</span>应该是高分辨图中每个位置的得分受到周围多大范围的预测值的影响，这部分应关注源码）。</p><p>这么做的缘故是，我认为是，<code>想保证不论在何种尺寸（量化等级下）都能克服量化误差的影响，因为heatmap是基于grid的，离散的取值，而真实的位置是不基于grid，并且是连续的位置，我通过预测真实位置与grid位置的偏移、以及grid上的置信度，就能进而获知真实的精确位置</code>。（我个人理解这样的好处就是，定位精度是float级别的，而不是int级别的，这个实际上在小尺寸的图像上是非常重要的一种策略。这种思想源自于G-RMI, 我认为这是一个解决量化误差问题的非常好的方式, 像SimpleBaseline,CPN运用取1/4偏移的方式,是一种人为的假设.）</p><figure><img src="https://img-blog.csdnimg.cn/20190704131509721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="paf">PAF</h2><p>PAF是个<span class="math inline">\(N\times H \times W \times 7\)</span>的结构， N表示关联肢体的数量，默认为19，</p><p>作者使用的bottom-up的方法，必然要解决：关联检测的关键点的位置形成隶属的人体的这一问题，就必须用一定的表示手段和策略来实现。</p><p>作者提出了PAF，来将关键点连接一起形成姿态。</p><p>在输出的每个位置，PAFs预测一个置信度、两个分别指向关联一起的两个part的向量、两个宽度。用下面来表示： <span class="math display">\[\mathbf{a}^{i j}=\left\{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right\}\]</span></p><p>作者接下来说了这样一句话，</p><blockquote><p>Both endpoints are localized with regressions that do not suffer from discretizations as they occur in grid- based methods. This helps to resolve joint locations of close-by persons precisely and to resolve them into distinct annotations。 我目前的理解是，两个端点定位的回归，不再受困于 grid-based方法中出现的离散化问题！这就帮助对于离得很近的关键点精确位置，并区分它们的标注。</p></blockquote><p>在COCO数据集，一共有19个连接关联两种类型的关键点。算法在每个feature map的位置，构造PAFs成分时，采用了两步：</p><p>首先，找到关联的两个关键点中最近的那一个的位置，来决定其向量成分中的一个。</p><p>然后，groundtruth pose决定了另外一个向量成分。第二个点不必是最近的，也可以是很远的。</p><blockquote><p>一开始我没有，怎么理解这么做的含义。后来意识到，这样就相当于，对于每一种类型的关联肢体，比如左肩膀和左屁股连接。对应的PAF中，每个位置都会优先确定理它最近的关键点的位置（考虑多个人体的情况下），然后指向另外一端的向量就自然得到了。</p></blockquote><p>并且在训练的时候，向量成分所指向的parts对必须是相关联的，每个向量的x，y方向必须指向同一个关键点的。</p><figure><img src="https://img-blog.csdnimg.cn/20190704131528767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="adaptive-regression-loss">Adaptive Regression Loss</h2><p>定位偏差可能对于大尺寸人体来讲，是小的影响，但是对于小尺寸人体，这个偏差就会成为主要的问题。本研究通过引入尺度依赖到<span class="math inline">\(L_1 - type​\)</span>的loss函数里，</p><h2 id="greedy-decoding">Greedy Decoding</h2><p>通过PIF和PAF来得到poses。这个快速贪心的算法过程和PersonLab中的相似。</p><p>一个姿态由一个种子点(高分辨率PIF的最高响应位置)开始，一旦一个关键点的估计完成，决策就是最终不变的了。（贪心）</p><p>A new pose is seeded by PIF vectors with the highest values in the high resolution confidence map <span class="math inline">\(f(x, y)\)</span> defined in equation <span class="math inline">\(1 .\)</span> Starting from a seed, connections to other joints are added with the help of PAF fields. The algorithm is fast and greedy. Once a connection to a new joint has been made, this decision is final.</p><p>Multiple PAF associations can form connections between the current and the next joint. Given the loca- tion of a starting joint <span class="math inline">\(\vec{x},\)</span> the scores <span class="math inline">\(s\)</span> of PAF associations a are calculated with</p><p><span class="math display">\[s(\mathbf{a}, \vec{x})=a_{c} \quad \exp \left(-\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\right) f_{2}\left(a_{x 2}, a_{y 2}\right)\]</span></p><p>这个<span class="math inline">\(s(\mathbf{a},\vec{x})\)</span>表示每个location属于part association的得分，得分越高，代表这个更有可能是part association区域部分那么,如果<span class="math inline">\(s(\mathbf{a},\vec{x})\)</span>越大,那么就期望<span class="math inline">\(a_c\)</span>越大,<span class="math inline">\(\left(-\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\right)\)</span>越大,<span class="math inline">\(\frac{\left\|\vec{x}-\vec{a}_{1}\right\|_{2}}{b_{1}}\)</span>越小,那么就期望PAF某位置的<span class="math inline">\(\mathbf{a}\)</span> 对应的<span class="math inline">\(\mathbf{a}=\left\{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right\}\)</span>向量中, 其指向的端点1和当前种子点距离最近, 并且期望该位置指向的另外一个端点2的置信度响应高, 这些期望和该位置是属于这两个关键点(端点)连接肢体的期望是一致的. 一旦我们的初始种子点确立后,我们就可以根据预测的PAF找到其关联的肢体区域和另外一个关键点位置,作为下一次的寻找的种子点.然后,重复这个过程,直到该种子点对应的人体全部找到.(这实际运用了人体躯干的连通性的潜在知识). 作者提倒:</p><blockquote><p><code>To confirm the proposed position of the new joint, we run reverse matching. This process is repeated until a full pose is obtained. We apply non-maximum suppression at the keypoint level as in [34]. The suppression radius is dynamic and based on the predicted scale component ofthe PIF field. We do not refine any fields neither during training nor test time.</code></p></blockquote><p>这个设计是巧妙的,<strong>因为我们在构造PAF的时候,请注意到,<span class="math inline">\((a_{x1},a_{y1})​\)</span> 是PAF输出map的某位置<span class="math inline">\(\mathbf{a}\)</span>最近的关键点的位置（请看Figure 4b），以此来判断离该位置<span class="math inline">\(\mathbf{a}\)</span>最近的关键点是不是<span class="math inline">\(\vec{x}\)</span></strong>。如果当前<span class="math inline">\(\vec{x}​\)</span>和<span class="math inline">\((a_{x1},a_{y1})​\)</span>的距离就可以作为当前位置是不是指向<span class="math inline">\(\vec{x}​\)</span>的判断,因为如果两点重合的话,距离为0,指数取值为最大值1. 并且该位置对应的另外一个端点的取值具有高响应, 那么这就意味着:</p><p><strong><span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>的髙得分位置,很有可能处在指向<span class="math inline">\(\vec{x}\)</span>端点的肢体关联部分的区域！</strong></p><p><strong>换句话说：</strong></p><p><span class="math inline">\(PIF\)</span>是计算得到的<code>高分辨率置信度图</code>负责提供候选的关键点。<span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>得分公式，利用<span class="math inline">\(PAF\)</span>预测值计算在其输出feature map每一个位置的得分，来判断两种关键点之间的连接（如左肘部和左手腕），因为涉及到多人，（参考OpenPose，对于单个人体的单个肢体连接，只有一种连接是合理的），论文提到的To confirm the proposed position of the new joint, we run reverse matching，我认为就是来确定某人体的某个肢体连接的唯一性、合理性的手段，具体还是要看源码。</p><p><del>找到<span class="math inline">\((a_{x2},a_{y2})\)</span>的位置(通过髙响应<span class="math inline">\(s(\mathbf{a}, \vec{x})\)</span>)的位置?还是通过PIF,PAF的预测值得到?这个目前有待考证，我在后面会阅读实现源码,继续更新博客)</del>，<code>高分辨率置信度图</code>负责提供候选的关键点的位置。</p><p>那么，通过这样的一个贪心的快速算法, 我们根据初始的某个关键点就能同时确立多个人体位置,</p><!-- ## 占位符。。。。。。## 占位符。。。。。。 --><h4 id="思考">思考</h4><p>注：可以看出这一系列的论文（GRMI，PersonLab，Openpifpaf，part-based）相比与针对网络结构进行改进（SimpleBaseline，HRNET）的文章看，更加关注几何关系上的问题以及网络的输出表示形式。PersonLab，Openpifpaf面对更加有挑战性的BBOX-FREE方法，以及小尺寸，遮挡问题进行处理，确确实实能给人持续往下深入的启示和实际应用的潜力。针对改网络结构的文章，譬如HRNET，SEU-POSE，SIMPLEBASELINE，CPN等等，致力于寻找最有的卷积结构设计，而不怎么关注一些棘手的问题（用模型本身的能力来克服），为姿态估计行业引领性能的标准，并不断去探索神经网络结构可能发挥的极限。前者更适合去研究新方法，突破现有检测器约束的姿态估计框架，去挑战多人姿态估计的难题，后者给我们提供了，固有框架内可以进一步提升性能的很多实用的经验和技巧，让我们更加洞察神经网络的结构的特性，并充分利用神经网络结构设计的潜在能力。</p><p>哪个才能更好解决人体姿态估计问题的手段呢？</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;g-rmi--personlab---pifpaf-composite-fields-for-human-pose-estimation---cvpr-2019-论文解读&quot;&gt;G-RMI-&amp;gt; PersonLab -&amp;gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;博客地址：https://yangsenius.github.io/blog/2-pifpaf/&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;arxiv地址: https://arxiv.org/abs/1903.06593 github地址: https://github.com/vita-epfl/openpifpaf&lt;/p&gt;
&lt;p&gt;今年的CVPR19的论文最近已经在CVF Openaccess 网站上放出来了。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
  </entry>
  
  <entry>
    <title>Hello, 这是我的新博客</title>
    <link href="http://senyang-ml.github.io/2019/07/17/hello-world/"/>
    <id>http://senyang-ml.github.io/2019/07/17/hello-world/</id>
    <published>2019-07-17T03:28:25.000Z</published>
    <updated>2019-07-17T03:28:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>原来的 https://yangsenius.github.io 网站的所有源码完全是我手工设计的： - 整个网页css的风格来自于DeepMind (https://deepmind.com/) 网站的风格 (0.0) - 用 head.js, body.js, foot.js 制作了每个博客的模板 - html5文件是我用vscode从markdown文本格式转化过来的 - 然后将head.js,body.js,foot.js的文件放到每个html5的指定位置 - 手工设计的博客悄然完成</p><p>然而，网站的风格和博客模板有了，但是维护起来不是特别容易，因为随着博文的增多，分类和分页功能我自己还没开发出来, 另外每次自动修改html源代码的步骤还需要我手动重复操作</p><p>“工欲善其事必先利其器”，既然hexo (https://hexo.io/zh-cn/) 提供了自动设计博客的功能，能够解放我的机械式的博客加工过程，何乐而不为呢？</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;原来的 https://yangsenius.github.io 网站的所有源码完全是我手工设计的： - 整个网页css的风格来自于DeepMind (https://deepmind.com/) 网站的风格 (0.0) - 用 head.js, body.js, foot
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Rethinking Human Pose Estimation  重新思考人体姿态估计</title>
    <link href="http://senyang-ml.github.io/2019/07/13/rethinking_human_pose/"/>
    <id>http://senyang-ml.github.io/2019/07/13/rethinking_human_pose/</id>
    <published>2019-07-13T01:52:16.000Z</published>
    <updated>2020-03-13T10:58:45.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>更多内容在知乎文章: <a href="https://zhuanlan.zhihu.com/p/72561165" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72561165/</a></strong></p><p>浅谈：2D人体姿态估计基本任务、研究问题、意义、应用、研究趋势以及未来方向</p><p><img src="https://ask.qcloudimg.com/http-save/yehe-1000017/1emeto7o6r.gif"></p><a id="more"></a><h4 id="基本定义从单张rgb图像中精确地识别出多个人体的以及其骨架的的稀疏的关键点位置">1.基本定义：从单张RGB图像中，精确地识别出多个人体的以及其骨架的的稀疏的关键点位置。</h4><p><img src="https://tse3-mm.cn.bing.net/th?id=OIP.u3JSsrIZJMhgjdxWK443vwHaHZ&w=221&h=211&c=7&o=5&pid=1.7"> <img src="https://raw.githubusercontent.com/CMU-Perceptual-Computing-Lab/openpose/master/doc/media/keypoints_pose_18.png" width="20%"></p><h4 id="基本任务给定一张rgb图像定位图像中人体的关键点位置并确定其隶属的人体">2.基本任务：给定一张RGB图像，定位图像中人体的关键点位置，并确定其隶属的人体。</h4><blockquote><p>按照人的直观视觉理解的话，主要会涉及到以下问题： - 关键点及周围的局部特征是什么样的？ - 关键点之间、人体肢体的空间约束关系是什么样的，以及层级的人体部件关系是什么样的？ - 不同人体之间的交互关系是什么样的，人体与外界环境之间的交互关系是什么？</p></blockquote><blockquote><p>基于Deep CNN的方法的试图通过神经网络的拟合能力，建立一种隐式的预测模型来避开上述的显式问题： - 基于去显式分析人体姿态问题的方法是有的，传统的Pictorial Structure是其中一个较为经典的算法思路，目前也有少数方法用part-based的层级树结构建立人体姿态模型并利用CNN，来进行学习与预测。 - 当下多数深度CNN回归的方式, 试图用模型强大的拟合能力去回避以上的显式问题，而从大量的图像数据和标签监督信息中用神经网络去学习图像数据与构建的标签信息之间的映射。</p></blockquote><h4 id="当前主流研究的基础问题和难点">3.当前主流研究的基础问题和难点：</h4><ul><li><p>神经网络结构的设计是个永远（当下）都会伴随的问题（假如深度学习的热潮没有退去的话）</p></li><li><p>Top-down：先检测人体，再做单人姿态估计两阶段的方法。</p><ul><li><p>必然受到了目标检测任务的制约。</p></li><li><p>基于bounding box的单人姿态估计问题，在面对遮挡问题容易受到挫折。</p></li><li><p>精度虽然髙实时性能较差</p></li><li><p>小尺寸图像与计算资源限制</p></li></ul></li><li><p>Bottom-up：针对整副图像的多人关键点检测，检测所有关键点候选位置的同时，关联相关人体</p><ul><li>精度不如单人估计的更加精准，但实时性能较好</li><li>面对拥挤问题、遮挡问题仍然容易受到挫折</li><li>小尺寸图像问题</li></ul></li></ul><h4 id="方法分类">4.方法分类：</h4><ul><li><p>标准1：:Top-Down和Bottom-up的方法。</p></li><li><p>标准2：全局的长距离关系的隐式学习问题（大多数）和基于part的中短距离关系（ECCV-18 PersonLab，ECCV-18 Deeply learned compositional models）的学习问题</p></li><li><p>标准3：heatmap回归（大多数），直接坐标回归方法（CVPR-14-DeepPose，ECCV-18的Integral Pose），向量场嵌入（CVPR-17 G-RMI，ECCV-18 PersonLab，CVPR-19 PIFPAF）的方法等等</p></li></ul><h5 id="近几年的代表作">5.近几年的代表作</h5><ul><li>发迹于2014年, CVPR: Google的DeepPose，同年出现了MPII数据集（Max-Planck ）以及MS-COCO数据集。</li><li>16年: CVPR：CMU的Convolutional Pose Machine (CPM)和德国的马克斯普朗克研究所Deepcut以及Stacked Hourglass 网络结构设计的出现。</li><li>17年: CVPR：Google的G-RMI开启基于目标检测的人体姿态估计方法。CMU的OpenPose系统出现，致力于打造实时姿态估计系统。Deepcut的改进版DeeperCut出现。同年ICCV上，Mask RCNN、上海交通大学的RMPE以及随后的AlphaPose崭露头角, NeurIPS17也出现了 Associative Embedding 以新的端到端的方式来避免人体姿态估计多阶段不连续学习的问题。</li><li>18年：CVPR上出现了旷世的CPN拿下了17年COCO挑战赛的冠军, ECCV上微软亚洲研究院的SimpleBaseline用自上而下的方法为姿态估计打造最简单的baseline，并刷新了COCO数据集的新高。ECCV上还出现了来自中东技术大学的Muhammed Kocabas提出了MultiPoseNet，以及Google的自下而上多任务的新作PersonLab, 值得一提的是还有一些开辟新的研究角度的方法如ECCV上美国西北大学part-based的姿态估计方法Deeply learned compositional models 。18年的另外一个趋势就是，新问题新任务的出现，比如CVPR18的DensePose标志着密集关键点人体姿态估计任务的出现, 2D pose track 任务(CVPR18 PoseTrack数据集)的提出, 以及3D 姿态估计问题的兴起......</li><li>19年CVPR, 姿态估计再次呈现一个小爆发（HRNET，PIFPAF，Seu-ByteDance Pose ，Related Parts Help，Crowded Pose , Fast Human Pose，Pose2Reg等等），并且出现了大量新的方向探讨姿态估计问题, 以及 3D 姿态估计成为主流。 当然, 2D姿态估计任务仍然是值得去深入探讨的问题, 因为一些本质上的难题目前还没有完全的洞察和有效的解决方案, 比如严重遮挡,多人重叠问题等等。另外， 数据集MPII, COCO数据集上的&quot;刷性能&quot; 也依然是大家孜孜不倦的追求，性能再次来到了新高。</li></ul><h4 id="研究意义">6.研究意义：</h4><ul><li><p>3D人体姿态估计的铺垫、3维人体重建的必备技术</p></li><li><p>人体关键点的视频追踪问题的基础（从静态到动态）</p></li><li><p>动作识别的信息来源（从关键点的时序空间特征映射到动作语义问题）</p></li></ul><h4 id="应用">7.应用：</h4><ul><li>自动驾驶行业：自动驾驶道路街景中行人的检测以及姿态估计、动作预测等问题</li><li>娱乐产业：动作特效的增加。快手、抖音、微视等视频软件</li><li>安全领域：行人再识别问题，以及特殊场景的特定动作监控，婴儿、老人的照顾。</li><li>影视产业：拍电影特效（复仇者联盟）</li><li>人机交互：AR，VR，以及未来的人机交互方式</li></ul><h4 id="研究趋势的变化以及扩展">8.研究趋势的变化以及扩展：</h4><ul><li>稀疏关键点到密集关键点（CVPR-18 FaceBook DensePose）</li><li>静态图像到视频追踪 （CVPR-18 PoseTrack）</li><li>从关键点定位到肢体的分割预测 （pose parsing,CVPR-19 Pose2Reg）</li><li>从监督学习到弱监督 、半监督，甚至无监督有可能（如, ICLR2019 unsupervised discovery, parts, structure and dynamics）</li><li>当然：神经网络结构的设计（ECCV-18 SimpleBaseline，CVPR-18 CPN， CVPR-19 HRNet，CVPR-19 Enhanced Channel-wise and Spatial Information，ICCV FPN-POSE等等）是个永远都会伴随的问题（假如深度学习的热潮没有退去的话）</li></ul><h4 id="个人思考">个人思考</h4><p>当前所有的姿态估计方法几乎都使用了深度卷积神经网络的强大功能，但个人认为神经网络设计绝不是解决该问题的核心，用力搔靴和脱掉鞋子，哪个才是更好的止痒手段呢？</p><p>人体姿态估计是一个综合的问题，有很多的切入点和难题值得去研究，并且它是一个尚未实际落地的计算机视觉技术。在这个层面上，AI的产品经理们和投机者们应该想想这项技术怎么能更好地服务大众，并带来市场和利润。</p><p>作为科学研究者，赚钱的考虑或应该暂时放到明天。我想讨论的是: 当我们面对一项任务和难题， 我们是应该忽略固有的困难和问题，提出新的问题，给出问题方案，去探索新的研究趋势呢？还是强行深入当前的固有问题，解决当下的难题呢？ 是不是有一些的问题是超前式的，也许放到以后才会有更加合适的方案和角度来解决？</p><p>或者说，我们还可以用另一种粗暴的方案：把这一问题黑箱化或者半黑箱化，然后从神经网络结构设计、数据处理、增强以及其他机器学习数学方法去暴力式的解决。这样的解决方式实际上是，摒弃了人类本身做姿态估计的直观思路（上面所述），而是从更加“机器学习”的角度去处理这个问题。假如，我们寻找到一个“完美”结构的神经网络，让它去达到１００％或者近似１００％的准确率！这样以来，似乎预测问题被完完全全地解决了，但是问题是，我们不知道能不能找到这样的结构或者技术，或者说一旦找到了以后能不能解释性地理解这一技术? 这就又引出了大家探讨争论许久的可解释性问题、显式推理问题。其实今年CVPR19 的PifPaf的工作值得我们去思考，它继续引入复合场(Composite Field)的概念，预测人为得设计好的高维度向量来处理人体姿态预测问题，让模型预测更加巧妙的监督信息。该方法能降低量化误差，设计的关联肢体得分公式巧妙保持了期望的一致性，再加之一个快速贪心算法，利用人体的连通特性就能得到多人姿态。这样的设计与算法，尽管性能比那些注重网络结构设计的略差一些，但却遵循合理的直觉，并且可解释性强，启发性强，是不是需要我们更多的关注？</p><p>另外，今年ICLR2019上，有学者甚至提出了无监督的方式处理人体部件。 我认为这是一种可以去探讨的问题, 因为人体姿态本身其实可以看成图像中的特征簇, 其视觉上的连通特性本身就具备了高维特征上的独特性。那么靠聚类手段、生成模型、无监督学习在直觉上是可行的, 如果再加上视频，光流等辅助信息, 那么就可以从大量无标签的图像数据中, 准确构建人体部件的特征、部件到整体的结构特征以及人体姿态的运动时序特征, 这可能又会是一个新的思路和解决人体姿态估计任务的新手段吗？</p><blockquote><p>知乎文章: <a href="https://zhuanlan.zhihu.com/p/72561165" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/72561165/</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;更多内容在知乎文章: &lt;a href=&quot;https://zhuanlan.zhihu.com/p/72561165&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://zhuanlan.zhihu.com/p/72561165/&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;浅谈：2D人体姿态估计基本任务、研究问题、意义、应用、研究趋势以及未来方向&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://ask.qcloudimg.com/http-save/yehe-1000017/1emeto7o6r.gif&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Object Detection" scheme="http://senyang-ml.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>MetaAnchor - Learning to Detect Objects with Customized Anchors - 2018 NeurIPS 解读</title>
    <link href="http://senyang-ml.github.io/2018/12/20/metaanchor/"/>
    <id>http://senyang-ml.github.io/2018/12/20/metaanchor/</id>
    <published>2018-12-20T01:52:16.000Z</published>
    <updated>2020-03-13T10:59:41.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors" target="_blank" rel="noopener">NeuIPS 2018</a> &gt; 原创博文 转载请注明<a href="https://yangsenius.github.io/blog/MetaAnchor/" target="_blank" rel="noopener" class="uri">https://yangsenius.github.io/blog/MetaAnchor/</a></p><p>一般目标检测方法中的Anchors的生成是来自于人类的先验知识:<span class="math inline">\(b_i\in \mathcal{B} \ which \ is \ predefined \ by \ human\)</span>（<span class="math inline">\(\mathcal{B}\)</span>属于 <span class="math inline">\({prior}\)</span> <span class="math inline">\(i\)</span>代表网格或锚点），即</p><ul><li>通过固定锚点，或者划分网格，生成一定形状和尺寸的Anchor Bboxes 来作为候选检测区域,提取对应位置的图像特征，</li></ul><p>先验往往代表设计人员在构思最初的朴素想法，来源于直觉，并把这种直觉融合在设计者的实现过程与代码中。 <a id="more"></a> 下面举两个例子。</p><h3 id="在faster-rcnn中">在Faster Rcnn中</h3><p>对输出的(W,H,d)维Conv map进行滑动遍历，每个滑窗输出一个特征向量WxH个d维的特征向量</p><p>根据根据感受野中心不变的原理，每个滑窗中心对应原图的anchor锚点或者说anchor bboxes的中心。</p><p>每个锚点映射到原图，实际上对应着来自3x3(3种特定的尺度x3个特定的形状)个的anchor boxes，我们认为这9个anchor bboxes经过特征提取得到的具有尺度不变性的特征向量，这些anchor bboxes意味着proposals。</p><p>然后作者使用先验规定：proposal与GTbbox iou大于某个阈值（0.7）认为是正样本，小于某个阈值（0.3）为负样本，其余的不参与训练！即给这些proposals做标签！</p><p>然后把这些正负样本送入RPN进行训练。</p><p>loss由regression和classification两个loss构成，即预测proposal的中心位置和宽高，以及proposal属于前景or背景</p><p>注意：这里的regression loss包含三个坐标：预测bbox、anchor bboxes、GT——bboxes,loss函数的目标是，缩小 [预测bbox与anchor bboxes相对偏移] 和[gt_bbox与anchor bboxes相对偏移]之间的差距！</p><p>经过RPN筛选后的Proposal的特征图的尺寸大小是不一致的，经过ROIPOOling得到特征维度一致的特征，使用与RPN共享卷积的Fast Rcnn进行进一步的分类和回归。</p><h3 id="在yolo中">在yolo中</h3><p>对任意输入尺寸的图像划分为<span class="math inline">\(s*s\)</span>个网格</p><p>每个网格预测B个bbox的4个位置和1个置信度 - (confidence代表了所预测的box中含有object的置信度和这个box预测的有多准两重信息,object落在一个grid cell里，第一项取1，否则取0。 第二项是预测的bounding box和实际的groundtruth之间的IoU值)</p><p>每个网格同时预测C个类的类别信息(每个网格属于的某类别的条件概率)</p><p>即对于一个输入图像，其输出的张量为 <span class="math inline">\(S*S*（B*5+C）\)</span></p><h2 id="在这里有必要说明这里anchor先验的含义即要把anchor的设计位置尺寸类比蕴含在anchor-function的设计中而不能成为一个独立的模块">在这里，有必要说明，这里“Anchor先验”的含义，即：要把anchor的设计（位置、尺寸、类比）蕴含在anchor function的设计中，而不能成为一个独立的模块</h2><h4 id="作者总结了一个较为一般的形式">作者总结了一个较为一般的形式：</h4><p><span class="math display">\[\mathcal{F}_{b_i}(\mathbf{x};\theta_i)=\left(\mathcal{F}^{cls}_{b_i}(\mathbf{x};\theta^{cls}_i),\mathcal{F}^{reg}_{b_i}(\mathbf{x}; \theta^{reg}_i)\right)\]</span></p><p>判断： 1. 每个候选区域的与真实bbox（如果有）的相对位置 <span class="math inline">\(\mathcal{F}^{reg}_{b_i}(\mathord{\cdot})\)</span> 2. 每个候选区域的类别置信概率 <span class="math inline">\(\mathcal{F}^{cls}_{b_i}(\mathord{\cdot})\)</span></p><p>本篇文章，作者使用的Anchor Function 是从先验的<span class="math inline">\(b_i\)</span>动态生成的,通过如下函数：</p><p><span class="math display">\[\mathcal{F}_{b_i}=\mathcal{G}\left(b_i; w \right)(2)\]</span></p><blockquote><p><span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> is called <span class="math inline">\({anchor \ function \ generator}\)</span> which maps any bounding box prior <span class="math inline">\(b_i\)</span> to the corresponding anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>; and <span class="math inline">\(w\)</span> represents the parameters. Note that in MetaAnchor the prior set <span class="math inline">\(\mathcal{B}\)</span> is not necessarily predefined; instead, it works as a  manner -- during inference, users could specify any anchor boxes, generate the corresponding anchor functions and use the latter to predict object boxes.</p></blockquote><p>上面是作者的原话，我觉得这个想法还是非常具有启发性的。我的理解是：</p><p>我们不是先盲目地生成大量的Anchor来判断是否抛弃，而是根据后面<strong>推理时</strong>的需要，在对应的位置生成特定的anchor boxes，然后生成anchor function来预测物体bbox，这样就避免了大量无关的候选框？这是我的理解，不知道对不对，接着读论文~</p><ul><li><p>“default boxes” , “priors” or “grid cells” 经常作为一个默认的方法。很多任务需要你在设计achor的大小、尺寸、位置时需要小心谨慎，不同数据集之间的物体bbox分布也会影响anchor的选择，但是MetaAnchor的方法就不用考虑这个问题。</p></li><li><p>受到 Learning to learn、few shot learning 、transfer learning的启发：有时候，我们的权重预测不是通过模型本身来学习，而是通过另一个结构（模型）来取预测权重，比如（Learning to learn by gradient descent by gradient descent，hypernetworks等），作者还拿自己的方法和learning to segment everything 作了对比，作者的权重预测是为了生成anchor function。</p></li></ul><p>仿佛，论文最关键的就是如何生成anchor function了，也就是这个函数了：</p><p><span class="math display">\[\mathcal{F}_{b_i}=\mathcal{G}\left(b_i; w \right)\]</span></p><p>下面详细讨论这个机制。</p><h2 id="anchor-function-generator">Anchor Function Generator</h2><blockquote><p>In MetaAnchor framework, <span class="math inline">\({anchor \ function}\)</span> is dynamically generated from the customized box prior (or anchor box) <span class="math inline">\(b_i\)</span> rather than fixed function associated with predefined anchor box. So, <span class="math inline">\({anchor \ function \ generator}\)</span> <span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> (see Equ.2), which maps <span class="math inline">\(b_i\)</span> to the corresponding anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>, plays a key role in the framework.</p></blockquote><p>作者强调了从<span class="math inline">\(b_i\)</span>映射到anchor function <span class="math inline">\(\mathcal{F}_{b_i}\)</span>, 这种映射关系是因为<span class="math inline">\(b_i\)</span>是带着一种随机性</p><blockquote><p>In order to model <span class="math inline">\(\mathcal{G}(\mathord{\cdot})\)</span> with neural work, inspired by <a href>HyperNetworks</a>,<a href>Learning to segment everything</a>, first we assume that for different <span class="math inline">\(b_i\)</span> anchor functions <span class="math inline">\(\mathcal{F}_{b_i}\)</span> share the same formulation <span class="math inline">\(\mathcal{F}(\mathord{\cdot})\)</span> but have different parameters, which means:</p></blockquote><p><span class="math display">\[\mathcal{F}_{b_i}(\mathbf{x}; \theta_i) = \mathcal{F}(\mathbf{x}; \theta_{b_i})\]</span></p><p>作者写这个公式，似乎想给出 无论怎样选择<span class="math inline">\(b_i\)</span> 的anchor function的一般形式。为什么这么做呢？下标的变换有什么意义吗？</p><p>我根据后面的内容，猜测：一般anchor function在设计时是要考虑 anchor<span class="math inline">\(b_i\)</span>的预定义方式，也就是我们要根据不同的anchor先验，具体设计出相对应的anchor function。如果我们anchor function的设计能够独立于anchor<span class="math inline">\(b_i\)</span>的预定义方式，让anchor<span class="math inline">\(b_i\)</span>的设计变成一个函数的可学习的参数形式，那么就把问题转化为一般的超参数学习，或者Meta-learning 的方式。之前我研究Learning to learn by gradient descent by gradient descent，作者就是让人工干预设计的优化方式，变成了可以学习的参数，二者虽然面对的问题的不一样，但是都包含了一个共同的思想：</p><p>让人工设计的先验知识，转化成，可以通过另一个结构或模型学习的，参数形式：</p><p><strong><span class="math display">\[人工先验知识 \rightarrow  可学习的参数形式\]</span></strong></p><p>这个思想和我上一篇<a href="https://blog.csdn.net/senius/article/details/84483329" target="_blank" rel="noopener">博客:learning to learn</a> 所涉及的方法，在理念上不谋而合</p><p>接着看论文。</p><p>论文说道： &gt; each anchor function is distinguished only by its parameters <span class="math inline">\(\theta_{b_i}\)</span>, anchor function generator could be formulated to predict <span class="math inline">\(\theta_{b_i}\)</span> as follows:</p><p><span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; w) \\= \theta^* + \mathcal{R}(b_i; w)\]</span></p><p>就是说，每个anchor function 通过参数 <span class="math inline">\(\theta_{b_i}\)</span> 来唯一确定(我的理解应该没错)，其中<span class="math inline">\(\theta^*\)</span>代表共享参数（独立于<span class="math inline">\({b_i}\)</span>，并且可以学习），残差项<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>依赖于 anchor bbox <span class="math inline">\({b_i}\)</span></p><p>然后<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>使用一个简单的两层全连接网络来表示：</p><p><span class="math display">\[\mathcal{R}(b_i, w) = \mathrm{W}_2 \sigma \left( \mathrm{W}_1 b_i \right)\]</span></p><p>作者还考虑把图像特征引入到参数 <span class="math inline">\(\theta_{b_i}\)</span>的学习中：</p><p><span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; \mathbf{x}, w) \\    = \theta^* + \mathrm{W}_2 \sigma \left(    \mathrm{W}_{11} b_i + \mathrm{W}_{12} r(\mathbf{x})    \right)\]</span></p><p><span class="math inline">\(r(\mathord{\cdot})\)</span> 用来给 <span class="math inline">\(\mathbf{x}\)</span>降维;</p><p>以上就是论文的理论思想了！</p><figure><img src="https://img-blog.csdnimg.cn/2018121317400085.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><h2 id="具体实施细节结合retinanet代码让我们来感受什么是prior什么是meta">具体实施细节，结合RetinaNet代码，让我们来感受什么是“Prior”？什么是“Meta”</h2><blockquote><p>作者没有公布自己的源码是一件令人头疼的事情，这样就不知道，作者是如何把可学习的参数<span class="math inline">\(\theta_{b_i}\)</span>如何融进anchor function，不过我后面会试图写一下。</p></blockquote><p>作者说，这个方法更实用于one-stage的检测方法如 RetinaNet，yolo等，two-stage方法精度似乎受到第二阶段（anchor 不再发挥作用）的学习的影响更大。</p><p>作者主要说明了MetaAnchor在RetinaNet上的使用，先来看看什么是RetianNet，放上一段简介的代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RetinaNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    num_anchors = <span class="number">9</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">20</span>)</span>:</span></span><br><span class="line">        super(RetinaNet, self).__init__()</span><br><span class="line">        self.fpn = FPN50()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.reg_head = self._make_head(self.num_anchors*<span class="number">4</span>)</span><br><span class="line">        self.cls_head = self._make_head(self.num_anchors*self.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        fms = self.fpn(x)</span><br><span class="line">        reg_preds = []</span><br><span class="line">        cls_preds = []</span><br><span class="line">        <span class="keyword">for</span> fm <span class="keyword">in</span> fms:</span><br><span class="line">            loc_pred = self.loc_head(fm)</span><br><span class="line">            cls_pred = self.cls_head(fm)</span><br><span class="line">            loc_pred = loc_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,<span class="number">4</span>)                 <span class="comment"># [N, 9*4,H,W] -&gt; [N,H,W, 9*4] -&gt; [N,H*W*9, 4]</span></span><br><span class="line">            cls_pred = cls_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,self.num_classes)  <span class="comment"># [N,9*20,H,W] -&gt; [N,H,W,9*20] -&gt; [N,H*W*9,20]</span></span><br><span class="line">            loc_preds.append(loc_pred)</span><br><span class="line">            cls_preds.append(cls_pred)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(loc_preds,<span class="number">1</span>), torch.cat(cls_preds,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_head</span><span class="params">(self, out_planes)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            layers.append(nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">256</span>, out_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> nn.Sequential(*layers)</span><br></pre></td></tr></table></figure><blockquote><p>注： 以上代码来自于<a href="https://github.com/kuangliu/pytorch-retinanet/blob/master/retinanet.py" target="_blank" rel="noopener">kuangliu/pytorch-retinanet</a></p></blockquote><p>从以上代码</p><p>_make_head（self, out_planes)</p><p>函数中可以得知：我们必须把anchor的数量考虑并体现在RetinaNet最后一层卷积核的通道数量上。 那么作为RetinaNET网络结构的这个卷积核部分，就包含了我先验的一种设计（Anchor类型数为9）。</p><p>这样做的弊端就是：假如我换了anchor的种类或数量，那么就要重新改变这个卷积核的设计，进而影响了网络的结构和参数学习，那么这就意味着我先前学习的对于9个Anchor的RetinaNet不再具有一般性，不再具备迁移学习的能力。</p><p>如果我想，换一种数据集bbox的分布，或者换一种先验anchor的选择方式，网络依旧能够使用的话，就必须将anchor的先验从原来的设计中剥离出来作为一个独立的结构，从而不影响整体结构的设计，并且可以根据需求自定义不同的anchor设计，这也就是这篇论文要解决的问题，并冠以“MetaAnchor”的称号，并使用了一个<span class="math inline">\(\mathcal{G}(b_i; w)\)</span>的anchor function generator</p><p>在RetianNet 的原设计中，每个detection head模块最后一层，对于预定义的3x3中anchor bboxes ，anchor function中：</p><ul><li>cls模块用3x3x80（类别）=720个通道卷积核，生成720维的预测向量</li><li>reg模块有3x3x4=36个通道卷积核，生成36维的预测向量</li></ul><p>而在使用MetaAnchor后，就降成了：</p><ul><li>cls模块有80（类别）=80个通道卷积核，生成80维的预测向量</li><li>reg模块有4个通道卷积核，生成4维的预测向量</li></ul><p>这就就需要重新设计anchor function。根据自己定制（customized）的anchor bbox<span class="math inline">\({b_i}\)</span>首先，应该考虑如何编码<span class="math inline">\({b_i}\)</span>，它包含了位置、尺寸、类别信息，多亏了RetianNet的全卷积结构，位置坐标信息已经包含在Feature map 中，我们使用<span class="math inline">\(\mathcal{G}(\cdot)\)</span>来预测类别，那么<span class="math inline">\({b_i}\)</span>只需要包含尺寸信息：</p><p><span class="math display">\[b_i = \left(\log \frac{ah_i}{AH}, \log \frac{aw_i}{AW} \right)\]</span></p><p>在一个训练的mini-batch中，我们给定一个二维<span class="math inline">\(b_i\)</span>的数值，分别经过两层的全连接网络<span class="math inline">\(\mathcal{G}(b_i; w_{cls})\)</span>和<span class="math inline">\(\mathcal{G}(b_i; w_{reg})\)</span>的映射，得到一个<span class="math inline">\(W_{cls}\)</span>和<span class="math inline">\(W_{reg}\)</span>维度的参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span></p><p>论文里面没有给出这个参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span>如何写入到Loss function中，我根据作者思路猜测：</p><p>论文提到 <span class="math inline">\(\mathcal{G} \left(b_i, w\right)\)</span> 是一个低秩的子空间</p><p>不过根据论文的权重预测的思想，这里的参数<span class="math inline">\(\theta_{cls,b_i}\)</span>和<span class="math inline">\(\theta_{reg,b_i}\)</span>应该在lossfunction中发挥权重的作用，在训练过程中，通过给定一个位置和尺度下的anchor bbox的输出和标签，乘以相应权重，来计算该anchor点对应的所有anchors总的loss:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Anchor_bbox_size</span><span class="params">(ah_i,aw_i,level)</span>:</span></span><br><span class="line">        minimum_size = <span class="number">20</span></span><br><span class="line">        AH,AW = minimum_size * np.pow(<span class="number">2</span>,level<span class="number">-1</span>)</span><br><span class="line">        b_i=(np.log(ah_i/AH),np.log(aw_i/AW))</span><br><span class="line">        <span class="keyword">return</span> b_i</span><br><span class="line">        </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">anchor_bbox_generator</span><span class="params">(b_i,level=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">'''b_i = (log(ah_i/AH),log(aw_i/AW))</span></span><br><span class="line"><span class="string">       b_t = [N,2]     '''</span></span><br><span class="line">    </span><br><span class="line">    hidden_dim = <span class="number">5</span></span><br><span class="line">    theta_dim = <span class="number">10</span></span><br><span class="line">    theta_standard =torch.randn(theta_dim)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">## two -layer</span></span><br><span class="line">    Residual_theta =F.linear( F.relu (F.linear(bi,(<span class="number">2</span>,hidden_dim))) , (hidden_dim,theta_dim ) )</span><br><span class="line">    </span><br><span class="line">    theta_b_i = theta_standard + Residual_theta</span><br><span class="line">    </span><br><span class="line">    reutrn theta_b_i</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RetinaNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_classes=<span class="number">20</span>)</span>:</span></span><br><span class="line">        super(RetinaNet, self).__init__()</span><br><span class="line">        self.fpn = FPN50()</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        self.reg_head = self._make_head(<span class="number">4</span>)</span><br><span class="line">        self.cls_head = self._make_head(self.num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        fms = self.fpn(x)</span><br><span class="line">        reg_preds = []</span><br><span class="line">        cls_preds = []</span><br><span class="line">        <span class="keyword">for</span> fm <span class="keyword">in</span> fms:</span><br><span class="line">            loc_pred = self.loc_head(fm)</span><br><span class="line">            cls_pred = self.cls_head(fm)</span><br><span class="line">            loc_pred = loc_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,<span class="number">4</span>)            <span class="comment"># [N, 4,H,W] -&gt; [N,H,W, 4] -&gt; [N,H*W, 4]</span></span><br><span class="line">            cls_pred = cls_pred.permute(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>).contiguous().view(x.size(<span class="number">0</span>),<span class="number">-1</span>,self.num_classes)  <span class="comment"># [N,20,H,W] -&gt; [N,H,W,20] -&gt; [N,H*W,20]</span></span><br><span class="line">            loc_preds.append(loc_pred)</span><br><span class="line">            cls_preds.append(cls_pred)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(loc_preds,<span class="number">1</span>), torch.cat(cls_preds,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_head</span><span class="params">(self, out_planes)</span>:</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            layers.append(nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line">            layers.append(nn.ReLU(<span class="literal">True</span>))</span><br><span class="line">        layers.append(nn.Conv2d(<span class="number">256</span>, out_planes, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>))</span><br><span class="line"><span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">focal_loss_meta</span><span class="params">(bi,cls_pred,cls_label,reg_pred,reg_label)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    bi = [N,2]</span></span><br><span class="line"><span class="string">    cls_pred = [N,20]</span></span><br><span class="line"><span class="string">    cls_label = [N,]</span></span><br><span class="line"><span class="string">    reg_pred = [N,4]</span></span><br><span class="line"><span class="string">    reg_label = [N,4]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    alpha = <span class="number">0.25</span></span><br><span class="line">    gamma = <span class="number">2</span></span><br><span class="line">    num_classes = <span class="number">20</span></span><br><span class="line">    </span><br><span class="line">    t = torch.eye（num_classes+<span class="number">1</span>）(cls_label, ）  <span class="comment"># [N,21] 20+背景 </span></span><br><span class="line">    <span class="comment"># t is one-hot vector</span></span><br><span class="line">    t = t[:,<span class="number">1</span>:]  <span class="comment"># 去掉 background 【N，20】 </span></span><br><span class="line">   </span><br><span class="line"></span><br><span class="line">    p = F.logsigmoid(cls_pred)</span><br><span class="line">    pt = p*t + (<span class="number">1</span>-p)*(<span class="number">1</span>-t)         <span class="comment"># pt = p if t &gt; 0 else 1-p</span></span><br><span class="line">    m = alpha*t + (<span class="number">1</span>-alpha)*(<span class="number">1</span>-t)   </span><br><span class="line">    m = m * (<span class="number">1</span>-pt).pow(gamma)   <span class="comment"># focal loss 系数 解决样本不平衡</span></span><br><span class="line">    </span><br><span class="line">    weight = anchor_bbox_generator(bi,) <span class="comment"># [N,W] W维的θ参数，该怎么用？ 还是说这里W=1？？</span></span><br><span class="line">    </span><br><span class="line">    cls_loss = F.binary_cross_entropy_with_logits(x, t, m, size_average=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>以上代码仅代表个人对论文的局限理解</p><p>因为看不到论文的代码，目前我理解最模糊的就是这个θ参数如何与loss function相结合的地方了，还请网友多多交流，欢迎发表更多的见解~</p><p>以上基本就介绍了是论文最主要的想法：</p><ul><li>MetaAnchor对于anchor的设定和bbox的分布更加鲁棒</li><li>MetaAnchor可以缩减不同数据集bbox分布的差异的影响，即更具迁移学习的能力！</li></ul><p>论文的更多的实验细节，我会继续阅读并更新博客~</p><p>=========================================</p><p>上次博客中说道，我理解最模糊的就是这个θ参数如何与ancnhor 的 loss function相结合的地方了</p><p>我重新阅读了论文，作者提到了权重预测的主要受到<strong>HyperNetworks</strong>的启发,然后我找来这篇论文，刚读完摘要，就恍然大悟理解了MetaAnchor里预测权重的思想，即这个θ参数的内涵，<span class="math inline">\(\theta_{b_i}\)</span> 即 <span class="math inline">\(\mathcal{F}_{csl}\left(\cdot\right)\)</span> 和 <span class="math inline">\(\mathcal{F}_{reg}\left(\cdot\right)\)</span> 的中的参数，在RetinaNet中代表了最后一层卷积核的参数！</p><h4 id="原来我在这个点上理解困难的原因是头脑中少了hypernetworks的先验">原来我在这个点上理解困难的原因是头脑中少了“HyperNetworks”的先验！</h4><blockquote><p>看来很多情况下，我们理解的困难源于：少了某些“先验知识”</p></blockquote><p><a href="https://arxiv.org/abs/1609.09106" target="_blank" rel="noopener">HyperNetwork</a> (ICLR2017)</p><p>HyperNetwork是什么呢，简言之：</p><p><strong>用一个网络(A-HyperNetwork)生成另外另一个网络(B-主体网络)的权重</strong></p><p>听起来很神奇，因为我们一般对于网络B的学习，通常经过梯度下降法产生梯度来更新参数。而这个工作可以直接用另一个网络的输出来预测。这样做的好处就是，我们可以将巨大参数量的权重学习，转换为一个小网络的参数学习，并可以通过端到端梯度优化的方法学习！</p><p>这篇论文分析了LSTM和CNN使用HyperNetwork的方法和效果，结合我们主要论述的MetaAnchor，我来简要介绍一下Static HyperNetwork在CNN中的应用</p><h2 id="通过一个两层全连接的小网络用一个layer-embedding来预测表征cnn的卷积核参数值">通过一个两层全连接的小网络，用一个layer embedding来预测（表征）CNN的卷积核参数值</h2><p>对于一个深度的卷积神经网络，其参数主要由卷积核构成</p><p>每个卷积核有 <span class="math inline">\(N_{in} \times N_{out}\)</span> 个滤波器 每个滤波器有 <span class="math inline">\(f_{size} \times f_{size}\)</span>.</p><p>假设这些参数存在一个矩阵 <span class="math inline">\(K^j \in \mathbb{R}^{N_{in}f_{size} \times N_{out}f_{size}}\)</span> for each layer <span class="math inline">\(j = 1,..,D\)</span>, 其中 <span class="math inline">\(D\)</span> 是卷积网络的深度</p><p>对于每一层 <span class="math inline">\(j\)</span>, hypernetwork 接受一个 a layer embedding <span class="math inline">\(z^j \in \mathbb{R}^{N_{z}}\)</span> 作为输入，并预测 <span class="math inline">\(K^j\)</span>, 可以写成:</p><p><span class="math display">\[ {K^j} = g( {z^j} ),\forall j = 1,..., D\]</span></p><p><span class="math display">\[{K} \in \mathbb{R}^{ N_{in}f_{size} \times N_{out}f_{size}}, {z} \in \mathbb{R}^{N_z}\]</span></p><figure><img src="https://img-blog.csdnimg.cn/20181214193114742.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><p>公式中，所有可学习的参数 <span class="math inline">\(W_i\)</span>, <span class="math inline">\(B_i\)</span>, <span class="math inline">\(W_{out}\)</span>, <span class="math inline">\(B_{out}\)</span> 对于所有 <span class="math inline">\(z^{j}\)</span>共享</p><p>在推理时, 模型仅仅将学习到的 the layer embeddings <span class="math inline">\(z^j\)</span> 来生成第 <span class="math inline">\(j\)</span> 层的卷积核权重参数</p><p>这就将可学习的参数量改变了:</p><p><span class="math display">\[D \times N_{in} \times f_{size} \times N_{out}\times f_{size}\]</span></p><p><span class="math display">\[\rightarrow\]</span></p><p><span class="math display">\[N_{z}\times D + d\times (N_z + 1)\times N_i + f_{size}\times N_{out}\times f_{size}\times (d+1)\]</span></p><h4 id="应用到metaanchor中theta_b_i即retinanet的最后一层卷积核的参数">应用到MetaAnchor中：<span class="math inline">\(\theta_{b_i}\)</span>即RetinaNet的最后一层卷积核的参数</h4><p>即，我们用自定义anchor设计<span class="math inline">\({b_i}\)</span>成二维向量，作为“layer embedding”，输入两层的网络，预测了RetinaNet的最后一层卷积核参数的残差，这样就降低了原RetinaNet的卷积核滤波器的数量，就像之前提到的。</p><p><strong>～～2019年6月25日更～～</strong> 最后，可能还需要再次提到<strong>关于<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>这两项的理解</strong>：</p><p>公式提到： <span class="math display">\[\theta_{b_i} = \mathcal{G}(b_i; w) \\= \theta^* + \mathcal{R}(b_i; w)\]</span></p><p>我们想要学习的是<span class="math inline">\(\theta_{b_i}\)</span> ，它有两部分构成：<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>，当进行学习的时候，梯度流到<span class="math inline">\(\theta_{b_i}\)</span> 节点，会产生<span class="math inline">\(\theta^*\)</span> 和<span class="math inline">\(\mathcal{R}(b_i; w)\)</span>的梯度：<span class="math inline">\(\nabla\theta^*\)</span> 和<span class="math inline">\(\nabla\mathcal{R}(b_i; w)\)</span>, 而<span class="math inline">\(\nabla\theta^*\)</span>可以直接用来更新<span class="math inline">\(\theta^*\)</span> ，但<span class="math inline">\(\nabla\mathcal{R}(b_i; w)\)</span>进一步通过HyperNetwork网络流到其参数<span class="math inline">\(\nabla w\)</span>上，然后更新HyperNetwork的参数<span class="math inline">\(w\)</span>。以上是进一步的分析了，如果有问题欢迎提出～</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://papers.nips.cc/paper/7315-metaanchor-learning-to-detect-objects-with-customized-anchors&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;NeuIPS 2018&lt;/a&gt; &amp;gt; 原创博文 转载请注明&lt;a href=&quot;https://yangsenius.github.io/blog/MetaAnchor/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; class=&quot;uri&quot;&gt;https://yangsenius.github.io/blog/MetaAnchor/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一般目标检测方法中的Anchors的生成是来自于人类的先验知识:&lt;span class=&quot;math inline&quot;&gt;\(b_i\in \mathcal{B} \ which \ is \ predefined \ by \ human\)&lt;/span&gt;（&lt;span class=&quot;math inline&quot;&gt;\(\mathcal{B}\)&lt;/span&gt;属于 &lt;span class=&quot;math inline&quot;&gt;\({prior}\)&lt;/span&gt; &lt;span class=&quot;math inline&quot;&gt;\(i\)&lt;/span&gt;代表网格或锚点），即&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过固定锚点，或者划分网格，生成一定形状和尺寸的Anchor Bboxes 来作为候选检测区域,提取对应位置的图像特征，&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先验往往代表设计人员在构思最初的朴素想法，来源于直觉，并把这种直觉融合在设计者的实现过程与代码中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Object Detection" scheme="http://senyang-ml.github.io/tags/Object-Detection/"/>
    
  </entry>
  
  <entry>
    <title>Learning to learn by gradient descent by gradient descent-PyTorch实践</title>
    <link href="http://senyang-ml.github.io/2018/12/17/learning_to_learn/"/>
    <id>http://senyang-ml.github.io/2018/12/17/learning_to_learn/</id>
    <published>2018-12-17T01:52:16.000Z</published>
    <updated>2020-03-13T10:59:56.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>原创博文，转载请注明来源</p></blockquote><h2 id="引言">引言</h2><p>“浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！”</p><p>Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公式符号，初学者很容易看不下去，而比较好的方法就是先浏览目录，掌握一些简单的概念（回归与分类啊，监督与无监督啊），并在按顺序的阅读过程学会“前瞻”与“回顾”，进行快速学习。又比如在早期接受教育的学习阶段，盲目的“题海战术”或死记硬背的“知识灌输”如果不加上恰当的反思和总结，往往会耗时耗力，最后达到的效果却一般，这是因为在接触新东西，掌握新技能时，是需要“技巧性”的。</p><p>从学习知识到学习策略的层面上，总会有“最强王者”在告诉我们，“钻石的操作、黄铜的意识”也许并不能取胜，要“战略上最佳，战术上谨慎”才能更快更好地进步。</p><a id="more"></a><p>这跟本文要讲的内容有什么关系呢？进入正题。 &gt; &gt; 其实读者可以先回顾自己从初高中到大学甚至研究生的整个历程，是不是发现自己已经具备了“learning to learn”的能力？</p><p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png" alt="在这里插入图片描述"> ## Learning to learn by gradient descent by gradient descent</p><p><strong>通过梯度下降来学习如何通过梯度下降学习</strong></p><blockquote><p>是否可以让优化器学会 &quot;为了更好地得到，要先去舍弃&quot; 这样的“策略”？</p></blockquote><p>本博客结合具体实践来解读《Learning to learn by gradient descent by gradient descent》，这是一篇Meta-learning（元学习）领域的<a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">论文</a>,发表在2016年的NIPS。类似“回文”结构的起名，让这篇论文变得有趣，是不是可以再套一层,&quot;Learning to learn to learn by gradient descent by gradient descent by gradient descent&quot;?再套一层？</p><p>首先别被论文题目给误导，==它不是求梯度的梯度，这里不涉及到二阶导的任何操作，而是跟如何学会更好的优化有关==，正确的断句方法为learning to (learn by gradient descent ) by gradient descent 。</p><p>第一次读完后，不禁惊叹作者巧妙的构思--使用LSTM（long short-term memory）优化器来替代传统优化器如（SGD，RMSProp，Adam等），然后使用梯度下降来优化优化器本身。</p><p>虽然明白了作者的出发点，但总感觉一些细节自己没有真正理解。然后就去看原作的代码实现，读起来也是很费劲。查阅了一些博客，但网上对这篇论文解读很少，停留于论文翻译理解上。再次揣摩论文后，打算做一些实验来理解。 在用PyTorch写代码的过程，才恍然大悟，作者的思路是如此简单巧妙，论文名字起的也很恰当，没在故弄玄虚，但是在实现的过程却费劲了周折！</p><h2 id="文章目录">文章目录</h2><p>[TOC]</p><p><strong>如果想看最终版代码和结果，可以直接跳到文档的最后！！</strong></p><p>下面写的一些文字与代码主要站在我自身的角度，记录自己在学习研究这篇论文和代码过程中的所有历程，如何想的，遇到了什么错误，问题在哪里，我把自己理解领悟“learning to learn”这篇论文的过程剖析了一下，也意味着我自己也在“learning to learn”！为了展现自己的心路历程，我基本保留了所有的痕迹，这意味着有些代码不够整洁，不过文档的最后是最终简洁完整版。 ==提醒：看完整个文档需要大量的耐心 : )==</p><p>我默认读者已经掌握了一些必要知识，也希望通过回顾这些经典研究给自己和一些读者带来切实的帮助和启发。</p><p>用Pytorch实现这篇论文想法其实很方便，但是论文作者来自DeepMind，他们用<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow写的项目</a>，读他们的代码你就会领教到最前沿的一线AI工程师们是如何进行工程实践的。</p><p>下面进入正题，我会按照最简单的思路，循序渐进地展开, &lt;0..0&gt;。</p><h2 id="优化问题">优化问题</h2><p>经典的机器学习问题，包括当下的深度学习相关问题，大多可以被表达成一个目标函数的优化问题：</p><p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p><p>一些优化方法可以求解上述问题，最常见的即梯度更新策略： <span class="math display">\[\theta_{t+1}=\theta_{t}-\alpha_{t}*\nabla f\left ( \theta_{t}\right )\]</span></p><p>早期的梯度下降会忽略梯度的二阶信息，而经典的优化技术通过加入<strong>曲率信息</strong>改变步长来纠正，比如Hessian矩阵的二阶偏导数。 <strong>Deep learning</strong>社区的壮大，演生出很多求解高维非凸的优化求解器，如 <strong>momentum</strong>[Nesterov, 1983, Tseng, 1998], <strong>Rprop</strong> [Riedmiller and Braun, 1993], <strong>Adagrad</strong> [Duchi et al., 2011], <strong>RMSprop</strong> [Tieleman and Hinton, 2012], and <strong>ADAM</strong> [Kingma and Ba, 2015].</p><p>目前用于大规模图像识别的模型往往使用卷积网络CNN通过定义一个代价函数来拟合数据与标签，其本质还是一个优化问题。</p><p>这里我们考虑一个简单的优化问题，比如求一个四次非凸函数的最小值点。对于更复杂的模型，下面的方法同样适用。 ### 定义要优化的目标函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">w = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(w,a=<span class="number">0.5</span>,b=<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="comment">#定义要优化的函数，求x的最优解</span></span><br><span class="line">    x= w*(x<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((x+<span class="number">1</span>)*(x+<span class="number">0.5</span>)*x*(x<span class="number">-1</span>)).sum()</span><br></pre></td></tr></table></figure><h3 id="定义常用的优化器如sgd-rmsprop-adam">定义常用的优化器如SGD, RMSProp, Adam。</h3><p>SGD仅仅只是给梯度乘以一个学习率。</p><p>RMSProp的方法是：</p><p><span class="math display">\[E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t\]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\]</span></p><p>当前时刻下，用当前梯度和历史梯度的平方加权和（越老的历史梯度，其权重越低）来重新调节学习率(如果历史梯度越低，“曲面更平坦”，那么学习率越大，梯度下降更“激进”一些，如果历史梯度越高，“曲面更陡峭”那么学习率越小，梯度下降更“谨慎”一些)，来更快更好地朝着全局最优解收敛。</p><p>Adam是RMSProp的变体： <span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p><p><span class="math display">\[\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\ \hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}\]</span></p><p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</span> 即通过估计当前梯度的一阶矩估计和二阶矩估计来代替，梯度和梯度的平方，然后更新策略和RMSProp一样。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.1</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">    </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br></pre></td></tr></table></figure><p>这里的Adam优化器直接用了Pytorch里定义的。然后我们通过优化器来求解极小值x，通过梯度下降的过程，我们期望的函数值是逐步下降的。 这是我们一般人为设计的学习策略，即==逐步梯度下降法，以“每次都比上一次进步一些” 为原则进行学习！==</p><h3 id="接下来-构造优化算法">接下来 构造优化算法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_STEPS = <span class="number">15</span></span><br><span class="line">theta = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(theta,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">theta_init = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   PyTorch 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i]))+1)*loss</span></span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#print(loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            x = x.detach_()</span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#x.retain_grad()</span></span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph</span><br></pre></td></tr></table></figure><h3 id="对比不同优化器的优化效果">对比不同优化器的优化效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">T = np.arange(TRAINING_STEPS)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201209548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="100"><figcaption>100</figcaption></figure><pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211</code></pre><p>通过上述实验可以发现，这些优化器都可以发挥作用，似乎RMS表现更加优越一些，不过这并不代表RMS就比其他的好,可能这个优化问题还是较为简单，调整要优化的函数，可能就会看到不同的结果。</p><h2 id="meta-optimizer-从手工设计优化器迈步到自动设计优化器">Meta-optimizer ：从手工设计优化器迈步到自动设计优化器</h2><p>上述这些优化器的更新策略是根据人的经验主观设计，要来解决一般的优化问题的。</p><p><strong>No Free Lunch Theorems for Optimization</strong> [Wolpert and Macready, 1997] 表明组合优化设置下，==没有一个算法可以绝对好过一个随机策略==。这暗示，一般来讲，对于一个子问题，特殊化其优化方法是提升性能的唯一方法。</p><p>而针对一个特定的优化问题，也许一个特定的优化器能够更好的优化它，我们是否可以不根据人工设计，而是让优化器本身根据模型与数据，自适应地调节，这就涉及到了meta-learning ### 用一个可学习的梯度更新规则，替代手工设计的梯度更新规则</p><p><span class="math display">\[\theta_{t+1}=\theta_{t}+g\textit{}_{t}\left (f\left ( \theta_{t}\right ),\phi \right)\]</span></p><p>这里的<span class="math inline">\(g(\cdot)\)</span>代表其梯度更新规则函数，通过参数<span class="math inline">\(\phi\)</span>来确定，其输出为目标函数f当前迭代的更新梯度值，<span class="math inline">\(g\)</span>函数通过RNN模型来表示，保持状态并动态迭代</p><p>假如一个优化器可以根据历史优化的经验来自身调解自己的优化策略，那么就一定程度上做到了自适应，这个不是说像Adam，momentum，RMSprop那样自适应地根据梯度调节学习率，（其梯度更新规则还是不变的），而是说自适应地改变其梯度更新规则，而Learning to learn 这篇论文就使用LSTM（RNN）优化器做到了这一点，毕竟RNN存在一个可以保存历史信息的隐状态，LSTM可以从一个历史的全局去适应这个特定的优化过程，做到论文提到的所谓的“CoordinateWise”，我的理解是：LSTM的参数对每个时刻节点都保持“聪明”，是一种“全局性的聪明”，适应每分每秒。</p><h4 id="构建lstm优化器">构建LSTM优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line">lstm=torch.nn.LSTM(Input_DIM,Hidden_nums ,Layers)</span><br><span class="line">Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span><br><span class="line">batchsize = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(lstm)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM_Optimizee</span><span class="params">(gradients, state)</span>:</span></span><br><span class="line">    <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">    <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">    gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)   </span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                 torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">   </span><br><span class="line">    update, state = lstm(gradients, state) <span class="comment"># 用optimizee_lstm代替 lstm</span></span><br><span class="line">    update = Linear(update)</span><br><span class="line">    <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">    <span class="keyword">return</span> update.squeeze().squeeze(), state</span><br></pre></td></tr></table></figure><pre><code>LSTM(10, 20, num_layers=2)</code></pre><p>从上面LSTM优化器的设计来看，我们几乎没有加入任何先验的人为经验在里面，只是用了长短期记忆神经网络的架构</p><h4 id="优化器本身的参数即lstm的参数代表了我们的更新策略">优化器本身的参数即LSTM的参数，代表了我们的更新策略</h4><p><strong>这个优化器的参数代表了我们的更新策略，后面我们会学习这个参数，即学习用什么样的更新策略</strong></p><p>对了如果你不太了解LSTM的话，我就放这个网站 http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 博客的几个图，它很好解释了什么是RNN和LSTM：</p><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="600"></center><center><img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="600"></center><h5 id="好了看一下我们使用刚刚初始化的lstm优化器后的优化结果">好了，看一下我们使用刚刚初始化的LSTM优化器后的优化结果</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(TRAINING_STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,TRAINING_STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201313760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211,lstm=554.2158203125</code></pre><h5 id="咦为什么lstm优化器那么差根本没有优化效果">咦，为什么LSTM优化器那么差，根本没有优化效果？</h5><p><strong>先别着急质疑！因为我们还没有学习LSTM优化器！</strong></p><p>用到的LSTM模型完全是随机初始化的！并且LSTM的参数在TRAIN_STEPS=[0,T]中的每个节点都是保持不变的！</p><h4 id="下面我们就来优化lstm优化器的参数">下面我们就来优化LSTM优化器的参数！</h4><p>不论是原始优化问题，还是隶属元学习的LSTM优化目标，我们都一个共同的学习目标：</p><p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p><p>或者说我们希望迭代后的loss值变得很小，传统方法，是基于每个迭代周期，一步一步，让loss值变小，可以说，传统优化器进行梯度下降时所站的视角是在某个周期下的，那么，我们其实可以换一个视角，更全局的视角，即，我们希望所有周期迭代的loss值都很小，这和传统优化是不违背的，并且是全局的，这里做个比喻，优化就像是下棋，优化器就是</p><h5 id="下棋手">“下棋手 ”</h5><p>如果一个棋手，在每走一步之前，都能看未来很多步被这一步的影响，那么它就能在当前步做出最佳策略，而LSTM的优化过程，就是把一个历史全局的“步”放在一起进行优化，所以LSTM的优化就具备了“瞻前顾后”的能力！</p><p>关于这一点，论文给出了一个期望loss的定义： <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]\]</span></p><p>但这个实现起来并不现实，我们只需要将其思想具体化。</p><ul><li><p>Meta-optimizer优化：目标函数“所有周期的loss都要很小！”，而且这个目标函数是独立同分布采样的（比如，这里意味着任意初始化一个优化问题模型的参数，我们都希望这个优化器能够找到一个优化问题的稳定的解）</p></li><li><p>传统优化器：&quot;对于当前的目标函数，只要这一步的loss比上一步的loss值要小就行”</p></li></ul><h5 id="特点-2.考虑优化器优化过程的历史全局性信息-3.独立同分布地采样优化问题目标函数的参数">特点 ： 2.考虑优化器优化过程的历史全局性信息 3.独立同分布地采样优化问题目标函数的参数</h5><p>接下来我们就站在更全局的角度，来优化LSTM优化器的参数</p><p>LSTM是循环神经网络，它可以连续记录并传递所有周期时刻的信息，其每个周期循环里的子图共同构建一个巨大的图，然后使用Back-Propagation Through Time (BPTT)来求导更新</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lstm_losses,global_graph_loss= learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span>) <span class="comment"># [loss1,loss2,...lossT] 所有周期的loss</span></span><br><span class="line"><span class="comment"># 因为这里要保留所有周期的计算图所以retain_graph_flag =True</span></span><br><span class="line">all_computing_graph_loss = torch.tensor(lstm_losses).sum() </span><br><span class="line"><span class="comment">#构建一个所有周期子图构成的总计算图,使用BPTT来梯度更新LSTM参数</span></span><br><span class="line"></span><br><span class="line">print(all_computing_graph_loss,global_graph_loss )</span><br><span class="line">print(global_graph_loss)</span><br></pre></td></tr></table></figure><pre><code>tensor(554.2158) tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)</code></pre><p>可以看到，变量<strong>global_graph_loss</strong>保留了所有周期产生的计算图grad_fn=<ThAddBackward></ThAddBackward></p><p>下面针对LSTM的参数进行全局优化,优化目标：“所有周期之和的loss都很小”。 值得说明一下：在LSTM优化时的参数，是在所有Unroll_TRAIN_STEPS=[0,T]中保持不变的，在进行完所有Unroll_TRAIN_STEPS以后，再整体优化LSTM的参数。</p><p>这也就是论文里面提到的coordinate-wise，即“对每个时刻点都保持‘全局聪明’”，即学习到LSTM的参数是全局最优的了。因为我们是站在所有TRAIN_STEPS=[0,T]的视角下进行的优化！</p><p>优化LSTM优化器选择的是Adam优化器进行梯度下降</p><h4 id="通过梯度下降法来优化-优化器">通过梯度下降法来优化 优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Global_Train_Steps = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(global_loss_1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        <span class="comment">#adam_global_optimizer.zero_grad()</span></span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#print(global_loss)</span></span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure><pre><code>xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]</code></pre><h5 id="为什么loss值没有改变为什么lstm参数的梯度不存在的">为什么loss值没有改变？为什么LSTM参数的梯度不存在的？</h5><p>通过分析推理，我发现了LSTM参数的梯度为None,那么反向传播就完全没有更新LSTM的参数！</p><p>为什么参数的梯度为None呢，优化器并没有更新指定的LSTM的模型参数，一定是什么地方出了问题，我想了好久，还是做一些简单的实验来找一找问题吧。</p><blockquote><p>ps: 其实写代码做实验的过程，也体现了人类本身学会学习的高级能力，那就是：通过实验来实现想法时，实验结果往往和预期差别很大，那一定有什么地方出了问题，盲目地大量试错法可能找不到真正问题所在，如何找到问题所在并解决，就是一种学会如何学习的能力，也是一种强化学习的能力。这里我采用的人类智能是：以小见大法。 In a word , if we want the machine achieving to AGI, it must imiate human's ability of reasoning and finding where the problem is and figuring out how to solve the problem. Meta Learning contains this idea.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">z= torch.empty(<span class="number">2</span>)</span><br><span class="line">torch.nn.init.uniform_(z , <span class="number">-2</span>, <span class="number">2</span>)</span><br><span class="line">z.requires_grad = <span class="literal">True</span></span><br><span class="line">z.retain_grad()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (z*z).sum()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam([z],lr=<span class="number">0.01</span>)</span><br><span class="line">grad =[]</span><br><span class="line">losses= []</span><br><span class="line">zgrad =[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    q = f(z)</span><br><span class="line">    loss = q**<span class="number">2</span></span><br><span class="line">    <span class="comment">#z.retain_grad()</span></span><br><span class="line">    loss.backward(retain_graph = <span class="literal">True</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment">#print(x,x.grad,loss,)</span></span><br><span class="line">    </span><br><span class="line">    loss.retain_grad()</span><br><span class="line">    print(q.grad,q.requires_grad)</span><br><span class="line">    grad.append((z.grad))</span><br><span class="line">    losses.append(loss)</span><br><span class="line">    zgrad.append(q.grad)</span><br><span class="line">    </span><br><span class="line">print(grad)</span><br><span class="line">print(losses)</span><br><span class="line">print(zgrad)</span><br></pre></td></tr></table></figure><pre><code>None TrueNone True[tensor([-44.4396, -36.7740]), tensor([-44.4396, -36.7740])][tensor(35.9191, grad_fn=&lt;PowBackward0&gt;), tensor(35.0999, grad_fn=&lt;PowBackward0&gt;)][None, None]</code></pre><h5 id="问题出在哪里">问题出在哪里？</h5><p>经过多方面的实验修改，我发现LSTM的参数在每个周期内BPTT的周期内，并没有产生梯度！！怎么回事呢？我做了上面的小实验。</p><p>可以看到z.grad = None,但是z.requres_grad = True，z变量作为x变量的子节点，其在计算图中的梯度没有被保留或者没办法获取，那么我就应该通过修改一些PyTorch的代码，使得计算图中的叶子节点的梯度得以存在。然后我找到了retain_grad()这个函数，实验证明，它必须在backward()之前使用才能保存中间叶子节点的梯度！这样的方法也就适合于LSTM优化器模型参数的更新了吧？</p><p>那么如何保留LSTM的参数在每个周期中产生的梯度是接下来要修改的！</p><p>这是因为我计算loss = f(x)，然后loss.backward() 这里的loss计算并没有和LSTM产生关系，我先来想一想loss和LSTM的关系在哪里？</p><p>论文里有一张图，可以作为参考：</p><center>图2<img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/blog/learn-to-learn/graph.PNG" width="600"></center><p><strong>LSTM参数的梯度来自于每次输出的“update”的梯度 update的梯度包含在生成的下一次迭代的参数x的梯度中</strong></p><p>哦!因为参数<span class="math inline">\(x_t = x_{t-1}+update_{t-1}\)</span> 在BPTT的每个周期里<span class="math inline">\(\frac{\partial loss_t}{\partial \theta_{LSTM}}=\frac{\partial loss_t}{\partial update_{t-1}}*\frac{\partial update_{t-1}}{\partial \theta_{LSTM}}\)</span>,那么我们想通过<span class="math inline">\(loss_0,loss_1,..,loss_t\)</span>之和来更新<span class="math inline">\(\theta_{LSTM}\)</span>的话，就必须让梯度经过<span class="math inline">\(x_t\)</span> 中 的<span class="math inline">\(update_{t-1}\)</span>流回去，那么每次得到的<span class="math inline">\(x_t\)</span>就必须包含了上一次更新产生的图（可以想像，这个计算图是越来越大的），想一想我写的代码，似乎没有保留上一次的计算图在<span class="math inline">\(x_t\)</span>节点中，因为我用了x = x.detach_() 把x从图中拿了下来！这似乎是问题最关键所在！！！（而tensorflow静态图的构建，直接建立了一个完整所有周期的图，似乎Pytorch的动态图不合适？no，no）</p><p>（注：以上来自代码中的<span class="math inline">\(x_t\)</span>对应上图的<span class="math inline">\(\theta_t\)</span>，<span class="math inline">\(update_{t}\)</span>对应上图的<span class="math inline">\(g_t\)</span>）</p><p>我为什么会加入x = x.detach_() 是因为不加的话，x变成了子节点，下一次求导pytorch不允许，其实只需要加一行x.retain_grad()代码就行了,并且总的计算图的globa_graph_loss在逐步降低！问题解决！</p><p>目前在运行global_training(lstm)函数的话，就会发现LSTM的参数已经根据计算图中的梯度回流产生了梯度，每一步可以更新参数了， 但是这个BPTT算法用cpu算起来，有点慢了~</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># x = x.detach_()</span></span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            x.retain_grad()</span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam([&#123;<span class="string">'params'</span>:optimizee.parameters()&#125;,&#123;<span class="string">'params'</span>:Linear.parameters()&#125;],lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure><pre><code>tensor(193.6147, grad_fn=&lt;ThAddBackward&gt;)tensor(7.6411)</code></pre><h5 id="计算图不再丢失了lstm的参数的梯度经过计算图的流动已经产生了">计算图不再丢失了，LSTM的参数的梯度经过计算图的流动已经产生了！</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/2018112320175122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">15</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201810120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123201822472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre><p>可以看出来，经过优化后的LSTM优化器，似乎已经开始掌握如何优化的方法，即我们基本训练出了一个可以训练模型的优化器！</p><p><strong>但是效果并不是很明显！</strong></p><p>不过代码编写取得一定进展 (0 ..0) /，接下就是让效果更明显，性能更稳定了吧？</p><p><strong>不过我先再多测试一些周期看看LSTM的优化效果！先别高兴太早！</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">50</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201835829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=150.99790954589844,rms=5.940474033355713,adam=14.17563247680664,lstm=268.0199279785156</code></pre><h4 id="又出了什么幺蛾子">又出了什么幺蛾子？</h4><p>即我们基本训练出了一个可以训练模型的优化器！但是 经过更长周期的测试，发现训练好的优化器只是优化了指定的周期的loss，而并没有学会“全局优化”的本领，这个似乎是个大问题！</p><h5 id="不同周期下输入lstm的梯度幅值数量级不在一个等级上面">不同周期下输入LSTM的梯度幅值数量级不在一个等级上面</h5><p>要处理一下！</p><p>论文里面提到了对梯度的预处理，即处理不同数量级别的梯度，来进行BPTT，因为每个周期的产生的梯度幅度是完全不在一个数量级，前期梯度下降很快，中后期梯度下降平缓，这个对于LSTM的输入，变化裕度太大，应该归一化，但这个我并没有考虑，接下似乎该写这个部分的代码了</p><p>论文里提到了： One potential challenge in training optimizers is that different input coordinates (i.e. the gradients w.r.t. different optimizee parameters) can have very different magnitudes. This is indeed the case e.g. when the optimizee is a neural network and different parameters correspond to weights in different layers. This can make training an optimizer difficult, because neural networks naturally disregard small variations in input signals and concentrate on bigger input values.</p><h5 id="用梯度的归一化幅值方向二元组替代原梯度作为lstm的输入">用梯度的（归一化幅值，方向）二元组替代原梯度作为LSTM的输入</h5><p>To this aim we propose to preprocess the optimizer's inputs. One solution would be to give the optimizer <span class="math inline">\(\left(\log(|\nabla|),\,\operatorname{sgn}(\nabla)\right)\)</span> as an input, where <span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p><p><span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p><p><span class="math display">\[\nabla^k\rightarrow  \left\{\begin{matrix} \left(\frac{\log(|\nabla|)}{p}\,, \operatorname{sgn}(\nabla)\right)&amp; \text{if } |\nabla| \geq e^{-p}\\ (-1, e^p \nabla)   &amp; \text{otherwise}\end{matrix}\right.\]</span></p><p>作者将不同幅度和方向下的梯度，用一个标准化到<span class="math inline">\([-1,1]\)</span>的幅值和符号方向二元组来表示原来的梯度张量！这样将有助于LSTM的参数学习！ 那我就重新定义LSTM优化器！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line"><span class="comment">#lstm1=torch.nn.LSTM(Input_DIM*2,Hidden_nums ,Layers)</span></span><br><span class="line"><span class="comment">#Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size)</span><br><span class="line">        <span class="comment">#elf.lstm = torch.nn.LSTM(10, 20,2)</span></span><br><span class="line">        <span class="comment">#elf.Linear = torch.nn.Linear(20,10)</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作"""</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                         torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        </span><br><span class="line">        update = Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,gradients, prev_state)</span>:</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line"></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM*<span class="number">2</span>, Output_DIM, Hidden_nums ,Layers , batchsize=<span class="number">1</span>,)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">grads = torch.randn(<span class="number">10</span>)*<span class="number">10</span></span><br><span class="line">print(grads.size())</span><br><span class="line"></span><br><span class="line">update,state =  LSTM_Optimizee(grads,<span class="literal">None</span>)</span><br><span class="line">print(update.size(),)</span><br></pre></td></tr></table></figure><pre><code>torch.Size([10])torch.Size([10])</code></pre><p>编写成功！ 执行！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):     </span><br><span class="line">            loss = f(x)    </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">           <span class="comment"># print('loss&#123;&#125;:'.format(i),loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            </span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">           <span class="comment">#print(update)</span></span><br><span class="line">            losses.append(loss)     </span><br><span class="line">            x = x + update  </span><br><span class="line">            x.retain_grad()</span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(LSTM_Optimizee)</span><br></pre></td></tr></table></figure><pre><code>tensor(239.6029, grad_fn=&lt;ThAddBackward&gt;)tensor(158.0625)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">30</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span>  </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><figure><img src="https://img-blog.csdnimg.cn/20181123201922189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=94.19924926757812,rms=5.705832481384277,adam=13.772469520568848,lstm=1224.2393798828125</code></pre><p><strong>为什么loss出现了NaN？？？为什么优化器的泛化性能很差？？很烦</strong></p><p>即超过Unroll周期以后，LSTM优化器不再具备优化性能？？？</p><p>然后我又回顾论文，发现，对优化器进行优化的每个周期开始，要重新随机化，优化问题的参数，即确保我们的LSTM不针对一个特定优化问题过拟合，什么？优化器也会过拟合！是的！l</p><p>论文里面其实也有提到：</p><p>Given a distribution of functions <span class="math inline">\(f\)</span> we will write the expected loss as: <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]（1）\]</span></p><p>其中f是随机分布的，那么就需要在Unroll的初始进行从IID 标准Gaussian分布随机采样函数的参数</p><p>另外我完善了代码，根据原作代码实现，把LSTM的输出乘以一个系数0.01，那么LSTM的学习变得更加快速了。</p><p>还有一个地方，就是作者优化optimiee用了100个周期，即5个连续的Unroll周期，这一点似乎我之前也没有考虑到！</p><hr><hr><h2 id="以上是代码编写遇到的种种问题下面就是最完整的有效代码了">以上是代码编写遇到的种种问题，下面就是最完整的有效代码了！！！</h2><p><strong>我们考虑优化论文中提到的Quadratic函数，并且用论文中完全一样的实验条件！</strong></p><p><span class="math display">\[f(\theta) = \|W\theta - y\|_2^2\]</span> for different 10x10 matrices <span class="math inline">\(W\)</span> and 10-dimensional vectors <span class="math inline">\(y\)</span> whose elements are drawn from an IID Gaussian distribution. Optimizers were trained by optimizing random functions from this family and tested on newly sampled functions from the same distribution. Each function was optimized for 100 steps and the trained optimizers were unrolled for 20 steps. We have not used any preprocessing, nor postprocessing.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning to learn by gradient descent by gradient descent</span></span><br><span class="line"><span class="comment"># =========================#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1611.03824</span></span><br><span class="line"><span class="comment"># https://yangsenius.github.io/blog/LSTM_Meta/</span></span><br><span class="line"><span class="comment"># https://github.com/yangsenius/learning-to-learn-by-pytorch</span></span><br><span class="line"><span class="comment"># author：yangsen</span></span><br><span class="line"><span class="comment"># #### “通过梯度下降来学习如何通过梯度下降学习”</span></span><br><span class="line"><span class="comment"># #### 要让优化器学会这样   "为了更好地得到，要先去舍弃"  这样类似的知识！</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> timeit <span class="keyword">import</span> default_timer <span class="keyword">as</span> timer</span><br><span class="line"><span class="comment">#####################      优化问题   ##########################</span></span><br><span class="line">USE_CUDA = <span class="literal">False</span></span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">batchsize = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    USE_CUDA = <span class="literal">True</span>  </span><br><span class="line">USE_CUDA = <span class="literal">False</span>  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'\n\nUSE_CUDA = &#123;&#125;\n\n'</span>.format(USE_CUDA))</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(W,Y,x)</span>:</span></span><br><span class="line">    <span class="string">"""quadratic function : f(\theta) = \|W\theta - y\|_2^2"""</span></span><br><span class="line">    <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">        W = W.cuda()</span><br><span class="line">        Y = Y.cuda()</span><br><span class="line">        x = x.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ((torch.matmul(W,x.unsqueeze(<span class="number">-1</span>)).squeeze()-Y)**<span class="number">2</span>).sum(dim=<span class="number">1</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################    手工的优化器   ###################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.01</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA == <span class="literal">True</span>:</span><br><span class="line">            state = state.cuda()</span><br><span class="line">            </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################    自动 LSTM 优化器模型  ##########################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.input_flag = <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> preprocess != <span class="literal">True</span>:</span><br><span class="line">             self.input_flag = <span class="number">1</span></span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size*self.input_flag, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size) <span class="comment">#1-&gt; output_size</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作</span></span><br><span class="line"><span class="string">        coordinate-wise LSTM """</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">            <span class="keyword">if</span> USE_CUDA :</span><br><span class="line">                 prev_state = (torch.zeros(Layers,batchsize,Hidden_nums).cuda(),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums).cuda())</span><br><span class="line">         </span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        update = self.Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            input_gradients = input_gradients.cuda()</span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = input_gradients.unsqueeze(<span class="number">0</span>)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">      </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line">    </span><br><span class="line"><span class="comment">#################   优化器模型参数  ##############################</span></span><br><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line">output_scale_value=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   构造一个优化器  #######</span></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM, Output_DIM, Hidden_nums ,Layers , batchsize=batchsize,\</span><br><span class="line">                preprocess=<span class="literal">False</span>,output_scale=output_scale_value)</span><br><span class="line">print(LSTM_Optimizee)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    LSTM_Optimizee = LSTM_Optimizee.cuda()</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">######################  优化问题目标函数的学习过程   ###############</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Learner</span><span class="params">( object )</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args :</span></span><br><span class="line"><span class="string">        `f` : 要学习的问题</span></span><br><span class="line"><span class="string">        `optimizee` : 使用的优化器</span></span><br><span class="line"><span class="string">        `train_steps` : 对于其他SGD,Adam等是训练周期，对于LSTM训练时的展开周期</span></span><br><span class="line"><span class="string">        `retain_graph_flag=False`  : 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">        `reset_theta = False `  :  默认每次学习前 不随机初始化参数</span></span><br><span class="line"><span class="string">        `reset_function_from_IID_distirbution = True` : 默认从分布中随机采样函数 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return :</span></span><br><span class="line"><span class="string">        `losses` : reserves each loss value in each iteration</span></span><br><span class="line"><span class="string">        `global_loss_graph` : constructs the graph of all Unroll steps for LSTM's BPTT </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,    f ,   optimizee,  train_steps ,  </span></span></span><br><span class="line"><span class="function"><span class="params">                                            eval_flag = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            retain_graph_flag=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_theta = False ,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_function_from_IID_distirbution = True)</span>:</span></span><br><span class="line">        self.f = f</span><br><span class="line">        self.optimizee = optimizee</span><br><span class="line">        self.train_steps = train_steps</span><br><span class="line">        <span class="comment">#self.num_roll=num_roll</span></span><br><span class="line">        self.eval_flag = eval_flag</span><br><span class="line">        self.retain_graph_flag = retain_graph_flag</span><br><span class="line">        self.reset_theta = reset_theta</span><br><span class="line">        self.reset_function_from_IID_distirbution = reset_function_from_IID_distirbution  </span><br><span class="line">        self.init_theta_of_f()</span><br><span class="line">        self.state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">        self.losses = []   <span class="comment"># 保存每个训练周期的loss值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_theta_of_f</span><span class="params">(self,)</span>:</span>  </span><br><span class="line">        <span class="string">''' 初始化 优化问题 f 的参数 '''</span></span><br><span class="line">        self.DIM = <span class="number">10</span></span><br><span class="line">        self.batchsize = <span class="number">128</span></span><br><span class="line">        self.W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">        self.Y = torch.randn(batchsize,DIM)</span><br><span class="line">        self.x = torch.zeros(self.batchsize,self.DIM)</span><br><span class="line">        self.x.requires_grad = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            self.W = self.W.cuda()</span><br><span class="line">            self.Y = self.Y.cuda()</span><br><span class="line">            self.x = self.x.cuda()</span><br><span class="line">        </span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Reset_Or_Reuse</span><span class="params">(self , x , W , Y , state, num_roll)</span>:</span></span><br><span class="line">        <span class="string">''' re-initialize the `W, Y, x , state`  at the begining of each global training</span></span><br><span class="line"><span class="string">            IF `num_roll` == 0    '''</span></span><br><span class="line"></span><br><span class="line">        reset_theta =self.reset_theta</span><br><span class="line">        reset_function_from_IID_distirbution = self.reset_function_from_IID_distirbution</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">            theta = torch.zeros(batchsize,DIM)</span><br><span class="line">           </span><br><span class="line">            theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">            x = theta_init_new</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">################   每次全局训练迭代，从独立同分布的Normal Gaussian采样函数     ##################</span></span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_function_from_IID_distirbution == <span class="literal">True</span> :</span><br><span class="line">            W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">            Y = torch.randn(batchsize,DIM)     <span class="comment">#代表 数据的标签 #  独立同分布的标准正太分布</span></span><br><span class="line">         </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span>:</span><br><span class="line">            state = <span class="literal">None</span></span><br><span class="line">            print(<span class="string">'reset W, x , Y, state '</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            W = W.cuda()</span><br><span class="line">            Y = Y.cuda()</span><br><span class="line">            x = x.cuda()</span><br><span class="line">            x.retain_grad()</span><br><span class="line">          </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span>  x , W , Y , state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, num_roll=<span class="number">0</span>)</span> :</span> </span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Total Training steps = Unroll_Train_Steps * the times of  `Learner` been called</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        SGD,RMS,LSTM 用上述定义的</span></span><br><span class="line"><span class="string">         Adam优化器直接使用pytorch里的，所以代码上有区分 后面可以完善！'''</span></span><br><span class="line">        f  = self.f </span><br><span class="line">        x , W , Y , state =  self.Reset_Or_Reuse(self.x , self.W , self.Y , self.state , num_roll )</span><br><span class="line">        self.global_loss_graph = <span class="number">0</span>   <span class="comment">#每个unroll的开始需要 重新置零</span></span><br><span class="line">        optimizee = self.optimizee</span><br><span class="line">        print(<span class="string">'state is None = &#123;&#125;'</span>.format(state == <span class="literal">None</span>))</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):     </span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                <span class="comment">#self.global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">              </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">              </span><br><span class="line">                update, state = optimizee(x.grad, state)</span><br><span class="line">              </span><br><span class="line">                self.losses.append(loss)</span><br><span class="line">             </span><br><span class="line">                x = x + update  </span><br><span class="line">                x.retain_grad()</span><br><span class="line">                update.retain_grad()</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.state = (state[<span class="number">0</span>].detach(),state[<span class="number">1</span>].detach())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses ,self.global_loss_graph </span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#Pytorch Adam</span></span><br><span class="line"></span><br><span class="line">            x.detach_()</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):</span><br><span class="line">                </span><br><span class="line">                optimizee.zero_grad()</span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                </span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">                </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag)</span><br><span class="line">                optimizee.step()</span><br><span class="line">                self.losses.append(loss.detach_())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses, self.global_loss_graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   LSTM 优化器的训练过程 Learning to learn   ###############</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Learning_to_learn_global_training</span><span class="params">(optimizee, global_taining_steps, Optimizee_Train_Steps, UnRoll_STEPS, Evaluate_period ,optimizer_lr=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Training the LSTM optimizee . Learning to learn</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:   </span></span><br><span class="line"><span class="string">        `optimizee` : DeepLSTMCoordinateWise optimizee model</span></span><br><span class="line"><span class="string">        `global_taining_steps` : how many steps for optimizer training o可以ptimizee</span></span><br><span class="line"><span class="string">        `Optimizee_Train_Steps` : how many step for optimizee opimitzing each function sampled from IID.</span></span><br><span class="line"><span class="string">        `UnRoll_STEPS` :: how many steps for LSTM optimizee being unrolled to construct a computing graph to BPTT.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    global_loss_list = []</span><br><span class="line">    Total_Num_Unroll = Optimizee_Train_Steps // UnRoll_STEPS</span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = optimizer_lr)</span><br><span class="line"></span><br><span class="line">    LSTM_Learner = Learner(f, optimizee, UnRoll_STEPS, retain_graph_flag=<span class="literal">True</span>, reset_theta=<span class="literal">True</span>,)</span><br><span class="line">  <span class="comment">#这里考虑Batchsize代表IID的话，那么就可以不需要每次都重新IID采样</span></span><br><span class="line">  <span class="comment">#即reset_function_from_IID_distirbution = False 否则为True</span></span><br><span class="line"></span><br><span class="line">    best_sum_loss = <span class="number">999999</span></span><br><span class="line">    best_final_loss = <span class="number">999999</span></span><br><span class="line">    best_flag = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps): </span><br><span class="line"></span><br><span class="line">        print(<span class="string">'\n=======&gt; global training steps: &#123;&#125;'</span>.format(i))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> range(Total_Num_Unroll):</span><br><span class="line">            </span><br><span class="line">            start = timer()</span><br><span class="line">            _,global_loss = LSTM_Learner(num)   </span><br><span class="line"></span><br><span class="line">            adam_global_optimizer.zero_grad()</span><br><span class="line">            global_loss.backward() </span><br><span class="line">       </span><br><span class="line">            adam_global_optimizer.step()</span><br><span class="line">            <span class="comment"># print('xxx',[(z.grad,z.requires_grad) for z in optimizee.lstm.parameters()  ])</span></span><br><span class="line">            global_loss_list.append(global_loss.detach_())</span><br><span class="line">            time = timer() - start</span><br><span class="line">            <span class="comment">#if i % 10 == 0:</span></span><br><span class="line">            print(<span class="string">'-&gt; time consuming [&#123;:.1f&#125;s] optimizee train steps :  [&#123;&#125;] | Global_Loss = [&#123;:.1f&#125;] '</span>\</span><br><span class="line">                  .format(time,(num +<span class="number">1</span>)* UnRoll_STEPS,global_loss,))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % Evaluate_period == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            best_sum_loss, best_final_loss, best_flag  = evaluate(best_sum_loss,best_final_loss,best_flag , optimizer_lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> global_loss_list,best_flag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(best_sum_loss,best_final_loss, best_flag,lr)</span>:</span></span><br><span class="line">    print(<span class="string">'\n --&gt; evalute the model'</span>)</span><br><span class="line">    STEPS = <span class="number">100</span></span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>, retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses, sum_loss = LSTM_learner()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        best = torch.load(<span class="string">'best_loss.txt'</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'can not find best_loss.txt'</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        best_sum_loss = best[<span class="number">0</span>]</span><br><span class="line">        best_final_loss = best[<span class="number">1</span>]</span><br><span class="line">        print(<span class="string">"load_best_final_loss and sum_loss"</span>)</span><br><span class="line">    <span class="keyword">if</span> lstm_losses[<span class="number">-1</span>] &lt; best_final_loss <span class="keyword">and</span>  sum_loss &lt; best_sum_loss:</span><br><span class="line">        best_final_loss = lstm_losses[<span class="number">-1</span>]</span><br><span class="line">        best_sum_loss =  sum_loss</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'\n\n===&gt; update new best of final LOSS[&#123;&#125;]: =  &#123;&#125;, best_sum_loss =&#123;&#125;'</span>.format(STEPS, best_final_loss,best_sum_loss))</span><br><span class="line">        torch.save(LSTM_Optimizee.state_dict(),<span class="string">'best_LSTM_optimizer.pth'</span>)</span><br><span class="line">        torch.save([best_sum_loss ,best_final_loss,lr ],<span class="string">'best_loss.txt'</span>)</span><br><span class="line">        best_flag = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_sum_loss, best_final_loss, best_flag</span><br></pre></td></tr></table></figure><pre><code>USE_CUDA = FalseLSTM_Optimizee_Model(  (lstm): LSTM(10, 20, num_layers=2)  (Linear): Linear(in_features=20, out_features=10, bias=True))</code></pre><h3 id="我们先来看看随机初始化的lstm优化器的效果">我们先来看看随机初始化的LSTM优化器的效果</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#############  注意：接上一片段的代码！！   #######################3#</span></span><br><span class="line"><span class="comment">##########################   before learning LSTM optimizee ###############################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">   </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><pre><code>reset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123202026137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=945.0716552734375,rms=269.4500427246094,adam=134.2750244140625,lstm=562912.125</code></pre><h5 id="随机初始化的lstm优化器没有任何效果loss发散了因为还没训练优化器">随机初始化的LSTM优化器没有任何效果，loss发散了，因为还没训练优化器</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#######   注意：接上一段的代码！！</span></span><br><span class="line"><span class="comment">#################### Learning to learn (优化optimizee) ######################</span></span><br><span class="line">Global_Train_Steps = <span class="number">1000</span> <span class="comment">#可修改</span></span><br><span class="line">Optimizee_Train_Steps = <span class="number">100</span></span><br><span class="line">UnRoll_STEPS = <span class="number">20</span></span><br><span class="line">Evaluate_period = <span class="number">1</span> <span class="comment">#可修改</span></span><br><span class="line">optimizer_lr = <span class="number">0.1</span> <span class="comment">#可修改</span></span><br><span class="line">global_loss_list ,flag = Learning_to_learn_global_training(   LSTM_Optimizee,</span><br><span class="line">                                                        Global_Train_Steps,</span><br><span class="line">                                                        Optimizee_Train_Steps,</span><br><span class="line">                                                        UnRoll_STEPS,</span><br><span class="line">                                                        Evaluate_period,</span><br><span class="line">                                                          optimizer_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show learning process results </span></span><br><span class="line"><span class="comment">#torch.load('best_LSTM_optimizer.pth'))</span></span><br><span class="line"><span class="comment">#import numpy as np</span></span><br><span class="line"><span class="comment">#import matplotlib</span></span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Global_T = np.arange(len(global_loss_list))</span></span><br><span class="line"><span class="comment">#p1, = plt.plot(Global_T, global_loss_list, label='Global_graph_loss')</span></span><br><span class="line"><span class="comment">#plt.legend(handles=[p1])</span></span><br><span class="line"><span class="comment">#plt.title('Training LSTM optimizee by gradient descent ')</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure><pre><code>=======&gt; global training steps: 0reset W, x , Y, state state is None = True-&gt; time consuming [0.2s] optimizee train steps :  [20] | Global_Loss = [4009.4] state is None = False-&gt; time consuming [0.3s] optimizee train steps :  [40] | Global_Loss = [21136.7] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [60] | Global_Loss = [136640.5] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [80] | Global_Loss = [4017.9] state is None = False-&gt; time consuming [0.2s] optimizee train steps :  [100] | Global_Loss = [9107.1]  --&gt; evalute the modelreset W, x , Y, state state is None = True......................</code></pre><p>输出结果已经省略大部分</p><h5 id="接下来看一下优化好的lstm优化器模型和sgdrmspropadam的优化性能对比表现吧">接下来看一下优化好的LSTM优化器模型和SGD，RMSProp，Adam的优化性能对比表现吧~</h5><p>鸡冻</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############  **注意： **接上一片段的代码****</span></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show contrast results SGD,ADAM, RMS ,LSTM ###############################</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> flag ==<span class="literal">True</span> :</span><br><span class="line">    print(<span class="string">'\n==== &gt; load best LSTM model'</span>)</span><br><span class="line">    last_state_dict = copy.deepcopy(LSTM_Optimizee.state_dict())</span><br><span class="line">    torch.save(LSTM_Optimizee.state_dict(),<span class="string">'final_LSTM_optimizer.pth'</span>)</span><br><span class="line">    LSTM_Optimizee.load_state_dict( torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line">    </span><br><span class="line">LSTM_Optimizee.load_state_dict(torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line"><span class="comment">#LSTM_Optimizee.load_state_dict(torch.load('final_LSTM_optimizer.pth'))</span></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): <span class="comment">#可以多试几次测试实验，LSTM不稳定</span></span><br><span class="line">    </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#p4.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure><pre><code>==== &gt; load best LSTM modelreset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123204117756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=967.908935546875,rms=257.03814697265625,adam=122.87742614746094,lstm=105.06891632080078reset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = Truereset W, x , Y, state state is None = True</code></pre><figure><img src="https://img-blog.csdnimg.cn/20181123202329192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption></figure><pre><code>sum_loss:sgd=966.5319213867188,rms=277.1605224609375,adam=143.6751251220703,lstm=109.35062408447266</code></pre><p>（以上代码在一个文件里面执行。复制粘贴格式代码好像需要Chrome或者IE浏览器打开才行？？？）</p><h2 id="实验结果分析与结论">实验结果分析与结论</h2><p>可以看到：==<strong>SGD 优化器</strong>对于这个问题已经<strong>不具备优化能力</strong>，RMSprop优化器表现良好，<strong>Adam优化器表现依旧突出，LSTM优化器能够媲美甚至超越Adam（Adam已经是业界认可并大规模使用的优化器了）</strong>==</p><h3 id="请注意lstm优化器最终优化策略是没有任何人工设计的经验">请注意：LSTM优化器最终优化策略是没有任何人工设计的经验</h3><p><strong>是自动学习出的一种学习策略！并且这种方法理论上可以应用到任何优化问题</strong></p><p>换一个角度讲，针对给定的优化问题，LSTM可以逼近或超越现有的任何人工优化器，不过对于大型的网络和复杂的优化问题，这个方法的优化成本太大，优化器性能的稳定性也值得考虑，所以这个工作的创意是独特的，实用性有待考虑~~</p><p>以上代码参考Deepmind的<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow版本</a>，遵照论文思路，加上个人理解，力求最简，很多地方写得不够完备，如果有问题，还请多多指出！</p><p><img src="https://img-blog.csdnimg.cn/20181125191428527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"> 上图是论文中给出的结果，作者取最好的实验结果的平均表现（试出了最佳学习率？）展示，我用下面和论文中一样的实验条件（不过没使用NAG优化器），基本达到了论文中所示的同样效果？性能稳定性较差一些</p><p>（我怀疑论文的实验Batchsize不是代码中的128？又或者作者把batchsize当作函数的随机采样？我这里把batchsize当作确定的参数，随机采样单独编写）。</p><h2 id="实验条件">实验条件：</h2><ul><li>PyTorch-0.4.1 cpu</li><li>优化问题为Quadratic函数:</li><li>W : [128,10,10] Y: [128 , 10] x: [128, 10] 从IID的标准Gaussian分布中采样，初始 x = 0</li><li><p>全局优化器optimizer使用Adam优化器， 学习率为0.1（或许有更好的选择，没有进行对比实验）</p></li><li>CoordinateWise LSTM 使用LSTM_Optimizee_Model：<ul><li>(lstm): LSTM(10, 20, num_layers=2)</li><li>(Linear): Linear(in_features=20, out_features=10, bias=True)</li></ul></li><li>==未使用任何数据预处理==（LogAndSign）和后处理</li><li>UnRolled Steps = 20 Optimizee_Training_Steps = 100</li><li>*Global_Traing_steps = 1000 原代码=10000，或许进一步优化LSTM优化器，能够到达更稳定的效果。</li><li><p>另外，原论文进行了mnist和cifar10的实验，本篇博客没有进行实验，代码部分还有待完善，还是希望读者多读原论文和原代码，多动手编程实验！</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">()</span>:</span></span><br><span class="line"><span class="keyword">return</span> <span class="string">"知识"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_to</span><span class="params">(learn)</span>:</span></span><br><span class="line">    print(learn())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"学习策略"</span></span><br><span class="line"></span><br><span class="line">print(learning_to(learn))</span><br></pre></td></tr></table></figure><h2 id="后叙">后叙</h2><blockquote><p>人可以从自身认识与客观存在的差异中学习，来不断的提升认知能力，这是最基本的学习能力。而另一种潜在不容易发掘，但却是更强大的能力--在学习中不断调整适应自身与外界的学习技巧或者规则--其实构建了我们更高阶的智能。比如，我们在学习知识时，我们总会先接触一些简单容易理解的基本概念，遇到一些理解起来困难或者十分抽象的概念时，我们往往不是采取强行记忆，即我们并不会立刻学习跟我们当前认知的偏差非常大的事物，而是把它先放到一边，继续学习更多简单的概念，直到有所“领悟”发现先前的困难概念变得容易理解</p></blockquote><blockquote><p>心理学上，元认知被称作反省认知，指人对自我认知的认知。弗拉威尔称，元认知是关于个人自己认知过程的知识和调节这些过程的能力：对思维和学习活动的知识和控制。那么学会适应性地调整学习策略，也成为机器学习的一个研究课题，a most ordinary problem for machine learning is that although we expect to find the invariant pattern in all data, for an individual instance in a specified dataset，it has its own unique attribute, which requires the model taking different policy to understand them seperately .</p></blockquote><blockquote><p>以上均为原创，转载请注明来源<br>https://blog.csdn.net/senius/article/details/84483329 or https://yangsenius.github.io/blog/LSTM_Meta/ 溜了溜了</p></blockquote><h2 id="下载地址与参考">下载地址与参考</h2><p><strong><a href="https://yangsenius.github.io/blog/LSTM_Meta/learning_to_learn_by_pytorch.py" target="_blank" rel="noopener">下载代码: learning_to_learn_by_pytorch.py</a></strong></p><p><a href="https://github.com/yangsenius/learning-to-learn-by-pytorch" target="_blank" rel="noopener">Github地址</a></p><p>参考： <a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">1.Learning to learn by gradient descent by gradient descent</a> <a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">2. Learning to learn in Tensorflow by DeepMind</a> <a href="https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2" target="_blank" rel="noopener">3.learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2</a></p><p><em>目录</em> [TOC]</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;原创博文，转载请注明来源&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;“浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！”&lt;/p&gt;
&lt;p&gt;Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公式符号，初学者很容易看不下去，而比较好的方法就是先浏览目录，掌握一些简单的概念（回归与分类啊，监督与无监督啊），并在按顺序的阅读过程学会“前瞻”与“回顾”，进行快速学习。又比如在早期接受教育的学习阶段，盲目的“题海战术”或死记硬背的“知识灌输”如果不加上恰当的反思和总结，往往会耗时耗力，最后达到的效果却一般，这是因为在接触新东西，掌握新技能时，是需要“技巧性”的。&lt;/p&gt;
&lt;p&gt;从学习知识到学习策略的层面上，总会有“最强王者”在告诉我们，“钻石的操作、黄铜的意识”也许并不能取胜，要“战略上最佳，战术上谨慎”才能更快更好地进步。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Meta Learning" scheme="http://senyang-ml.github.io/tags/Meta-Learning/"/>
    
      <category term="Reproduce" scheme="http://senyang-ml.github.io/tags/Reproduce/"/>
    
      <category term="PyTorch" scheme="http://senyang-ml.github.io/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Associative embedding End-to-End Learning for Joint Detection and Grouping</title>
    <link href="http://senyang-ml.github.io/2018/07/12/AE-hourglass/"/>
    <id>http://senyang-ml.github.io/2018/07/12/AE-hourglass/</id>
    <published>2018-07-12T01:52:16.000Z</published>
    <updated>2020-05-29T05:30:46.446Z</updated>
    
    <content type="html"><![CDATA[<p>2018.04.07：</p><h2 id="论文阅读">论文阅读</h2><p>这篇文章的核心思想是比较精炼概括的，它的亮点是用一个框架解决了在计算机视觉中常见的任务中经常遇到的两个通用环节：Detection and Grouping，用中文来讲就是，检测（小的视觉单元作为候选）和（根据得分）重组（一个合理的结构）。 <a id="more"></a></p><p>从以下的视觉任务中可以体现：</p><ul><li><p><strong>人体姿态估计问题</strong>：一般按照bottom-to-up的方式，先检测出body key points然后按照约束来组合完整的人体，但多人姿态估计的问题又衍生出另一种up-down的方式，就是先检测出单个人体再识别其姿态，比如Mask R-CNN, RMPE等方法。</p></li><li><strong>目标检测</strong>：往往先寻找不同位置和尺度下的bounding boxes，然后打分筛选</li><li><strong>实例分割</strong>：寻找相关联的像素，然后将像素合理重组成物体实例（mask）。</li><li><p><strong>多目标追踪</strong>：检测物体实例，重组其运动轨迹</p></li></ul><p>这些方式，本质上都符合人类自身视觉从部分认识整体，以整体推理部分的直觉。以往的工作都认识到这一点，只是这篇论文做了一次提炼概括了，并指出了一个问题：</p><p>以往的两步策略（detection ﬁrst and grouping second）忽略了两个环节之间内在的紧密耦合。</p><blockquote><p>（在之前看的CMU的Realtime Multi-Person2D Pose Estimation using Part Affinity Fields, 他们的论文当中，除了人体关键点作为监督信息外，还引入了Part Affinity Fields，也就是和肢体方向保持一致的单位向量作为监督信号，我的感觉是，这实际上就是没有充分利用两个环节上的耦合性，或者说是人体关键点与肢体连接的耦合性信息，毕竟人体的关节与整体的关系是统一的， 而OpenPose用的是寻找最佳的图匹配的方式，但同时将关键点位置，和肢体向量同时作为监督信息，会导致信息冗余，增加复杂度吧？所以我觉得作者这种融合两步的思想就很实际，很前卫）</p></blockquote><p>所以作者针对多人体姿态估计，将两步工作融入到一个框架里，即在一般的输出Heatmap层，附加了一层作为“tag“（也就是论文提到的embedding的含义），并设计了一个grouping loss作为监督关键点是否分配给了正确的人体的函数。论文巧妙的地方就是没有给“tag”赋予”ground-truth”来作为强监督，而是用“tag“值的相似与差异来表示多个人体。用于预测Heatmap的网络架构基于作者之前的工作“Stacked Hourglass”.</p><p>论文中Related work中的Perceptual Organization的叙述部分，给我了比较多的启示：</p><blockquote><p>Perceptual Organization是感知组织的意思，我理解成人类在认识事物或概念所遵循的层级组织关系。所谓的强人工智能，就需要解决这一棘手问题吧。作者提到了这一工作涉及到的许多任务，有Figure–ground segmentation (perception)，hierarchical image parsing， spectral clustering，conditional random ﬁelds，generative probabilistic models等等一系列问题，这些方法都遵循，从pre-detect visual units到measure affinity，再到grouping，但是目前没有统一到一个统一的架构上来，作者就是从这角度出发，不加一些额外的设计来完成一个端到端的网络架构。作者提到了图像层级解析，特别符合人类认知图像，所以，作者的Hourglass模块设计成沙漏状，先是压缩图像，获得全局信息，然后利用全局信息与低层特征融合，输出一个与同样大小的heatmap，其实就是想将这样的层级解析的思想间接地蕴含在内，只不过网络的训练将这些信息都隐含在了参数里，无法与人的解析思路类比.</p></blockquote><p>详情查看博客 <a href="https://yangsenius.github.io/blog/embedding/" target="_blank" rel="noopener" class="uri">https://yangsenius.github.io/blog/embedding/</a></p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原创, 禁止转载</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;2018.04.07：&lt;/p&gt;
&lt;h2 id=&quot;论文阅读&quot;&gt;论文阅读&lt;/h2&gt;
&lt;p&gt;这篇文章的核心思想是比较精炼概括的，它的亮点是用一个框架解决了在计算机视觉中常见的任务中经常遇到的两个通用环节：Detection and Grouping，用中文来讲就是，检测（小的视觉单元作为候选）和（根据得分）重组（一个合理的结构）。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Pose machine：Articulated Pose Estimation via Inference Machines</title>
    <link href="http://senyang-ml.github.io/2018/06/01/pose-machine-eccv2014/"/>
    <id>http://senyang-ml.github.io/2018/06/01/pose-machine-eccv2014/</id>
    <published>2018-06-01T01:52:16.000Z</published>
    <updated>2020-05-24T04:06:10.042Z</updated>
    
    <content type="html"><![CDATA[<h1 id="pose-machinearticulated-pose-estimation-via-inference-machines">Pose machine：Articulated Pose Estimation via Inference Machines</h1><p>由显式的图模型推断人体铰链模型到隐式地表达丰富的多部件关系的转折点！</p><h2 id="介绍">介绍</h2><p>估计铰链式人体姿态的复杂度来源有两个：</p><ul><li>潜在骨架模型的自由度（20大约）导致的高位配置空间的搜索范围</li><li>图像中人体姿态的多变性</li></ul><p>传统的图模型，基于树结构或者星状结构的简单图模型难以捕捉多部件之间的关系和依赖性，而且容易导致双重计数错误产生。 非树模型需要近似推断来解决，参数学习非常复杂！ <a id="more"></a> 图模型的第二个困难的地方是： &gt; A second limitation of graphical models is that deﬁning the potential functions requires careful consideration when specifying the types of interactions. This choice is usually dominated by parametric forms such as simple quadratic models in order to enable tractable inference [1]. Finally, to further enable eﬃcient inference in practice, many approaches are also restricted to use simple classiﬁers such as mixtures of linear models for part detection [5]. These are choices guided by tractabilty of inference rather than the complexity of the data. Such tradeoﬀs result in a restrictive model that do not address the inherent complexity of the problem.</p><p>论文建立了<strong>一个类似于场景解析的层级推断机制来估计人体姿态</strong></p><h2 id="pose-machine">Pose machine</h2><p>它是一个序列预测算法，它<strong>效仿了信息传递的机制</strong>来预测每一个人体部件的置信度，在每个阶段迭代提升它的预测能力</p><ul><li><p>1.它联合了多变量之间的交互关系</p></li><li><p>2.它从数据中学习空间关系，而不需要精确的参数模型</p></li><li><p>3.它模块化的架构能够允许使用高承载的预测器来处理不同形状的姿态</p></li></ul><p>解决在传统模型人体姿态估计的两大难点。</p><p>可以这么说，pose machine最重要的想法就是：运用级联，将对每个单独部件的估计信息结合起来，从中提取语义特征再进一步地精确估计下个阶段的每个部件位置。这个思想是开创性的，它不仅符合人类的直觉，并且实验证明是有提升效果的，后来的CPM，用卷积网络的架构准确实现了这一方法，取得了当时的最好效果，对各种形状的人体姿态都有帮组，在2016年，CPM在MPII,LSP,FLIC数据库上都到达了最好表现，而近年来成功的关于姿态估计的所有论文都几乎蕴含了这一思想：试图归纳出图中全局的信息作为下个级联阶段的预测，比如stacker hourglass、cpn</p><p>本博客详见 <a href="https://yangsenius.github.io/blog/pose-machine/" target="_blank" rel="noopener">Pose-Machine</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;pose-machinearticulated-pose-estimation-via-inference-machines&quot;&gt;Pose machine：Articulated Pose Estimation via Inference Machines&lt;/h1&gt;
&lt;p&gt;由显式的图模型推断人体铰链模型到隐式地表达丰富的多部件关系的转折点！&lt;/p&gt;
&lt;h2 id=&quot;介绍&quot;&gt;介绍&lt;/h2&gt;
&lt;p&gt;估计铰链式人体姿态的复杂度来源有两个：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;潜在骨架模型的自由度（20大约）导致的高位配置空间的搜索范围&lt;/li&gt;
&lt;li&gt;图像中人体姿态的多变性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;传统的图模型，基于树结构或者星状结构的简单图模型难以捕捉多部件之间的关系和依赖性，而且容易导致双重计数错误产生。 非树模型需要近似推断来解决，参数学习非常复杂！&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Causal inference(因果推断)</title>
    <link href="http://senyang-ml.github.io/2018/05/20/inference/"/>
    <id>http://senyang-ml.github.io/2018/05/20/inference/</id>
    <published>2018-05-20T01:52:16.000Z</published>
    <updated>2020-05-24T03:47:45.003Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推断的介入">推断的介入</h1><p>善有善报，恶有恶报。</p><p>虽然世间并没有将这份准则执行地那么恰如其分，但是万物归根结底，必然有着因果之间的联系。</p><p>而人生匆匆，为什么愚蠢的我们总是在苦苦寻觅事情的答案，why？</p><p>剪不断，理还乱，只好奉上因果推断</p><a id="more"></a><h2 id="因果推断">因果推断</h2><p>在机器学习领域，因果推断，是在图模型中被着重研究的一种理论。</p><p>2017年NIPS会议上，<a href="https://mp.weixin.qq.com/s/ytt8Hwv4JHZdV4649XG6Xw" target="_blank" rel="noopener">Judael Pearl</a>发表了机器学习的局限性演讲，并提出了因果推断的三个层次：</p><table><colgroup><col style="width: 43%"><col style="width: 37%"><col style="width: 18%"></colgroup><thead><tr class="header"><th></th><th>公式</th><th>含义</th></tr></thead><tbody><tr class="odd"><td><strong>观察</strong></td><td><span class="math inline">\(P\left( y\mid x \right)\)</span></td><td>Seeing: what is？ 观察到x时，会如何影响到y的信念</td></tr><tr class="even"><td><strong>介入</strong></td><td><span class="math inline">\(P\left(y\mid do(x) \right)\)</span></td><td>Doing: what if？ 如果做出对x的介入时，会如何影响到y的信念</td></tr><tr class="odd"><td><strong>反事实</strong></td><td><span class="math inline">\(P\left(y_{x}\mid x^{,},y^{,} \right)\)</span></td><td>Imaging: why? 是x影响到y的吗，基于当下，假设发生的是x^，会对y产生何种影响</td></tr></tbody></table><p>为来形象直观地理解Judael Pearl因果推断三层次，我举一个简单的追求异性的例子，它深刻蕴含着这三个推断层次：</p><p><strong>观察</strong>：在追求一位异性时，不管你是情场高手，还是恋爱小白，你必须承认，我们都会收集足够多的关于他/她的信息x，来作为判断能否成功追求到他/她的依据，也许有的人会认真考虑这个问题，（如果发现成功率低就会直接选择放弃，而选择不再追，其实有点可惜了，因为这仅仅处于<strong>观察</strong>阶段），而有的人在直觉上就已经做出了估计。这也就是我们基于已经观察到的信息x，来评估成功追到他/她的概率<span class="math inline">\(P\left( y\mid x \right)\)</span></p><p><strong>介入</strong>：在追求异性时，我们必须有所行动，仅仅是观察并不能影响到成功追到他/她的概率。理所当然，我们所作出的行动一般都会遵循一个显而易见的行动目标或指南：做一些有意义的事情 <span class="math inline">\(do(x)\)</span>来增加能成功追求到他/她的概率<span class="math inline">\(P\left(y\mid do(x) \right)\)</span>，我们会尝试一些方法，来判断这样做是会引起对方的反感，还是能增加对自己的好感，进而影响成功追求到他/她的概率（如果你发现无论 <span class="math inline">\(do(whatever)\)</span>，总不能提高<span class="math inline">\(P\left( y\mid do(whatever) \right)\)</span>，那么事实就会证明，放弃可能是比较明智的选择）</p><p><strong>反事实</strong>： 在追求异性遭遇挫败时，或者反思自己的追求历程时，我们往往会基于当前的态势做出一些假设，要是当初我那样去做<span class="math inline">\(x^{,}\)</span>而不是做<span class="math inline">\(x\)</span>，会不会就更有可能把他/她给追到呢 <span class="math inline">\(P\left( y_{x}\mid x^{,},y^{,} \right)\)</span> 。这样的假设才会让我们总结失败或者成功的真正原因，找出哪些才是真正影响我们最后能否追求成功的因素。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">然而在现实生活中，上述的推断分析还是考虑地过于单纯、局限，因为情感问题不是靠推断就能解决的。</span><br></pre></td></tr></table></figure><p>这样的推断，在大多数情况下，会更多地出现在我们的直觉当中，所以见到这样的理论你也会见怪不怪，但是对于机器而言，如何向用数学的语言来描述这个推断过程，确实一件极其困难的事情，更何况想让计算机能够自动计算这种推断。<strong>而当下首先要解决的是如何用合理的计算机语言来描述这个推断过程</strong>。</p><p>基于这三个层次的因果推断属于较为高级的人类智能，而目前的机器学习仅仅停留在第一个阶段，就是由观察到信念的阶段，这就是当下机器学习的局限性所在：可解释差，没有引入人类的合理性推断在里面。那么，是否可以在这方面做出更多的思考呢？</p><h2 id="概念假设推断">概念假设推断</h2><p>下表是我根据反事实理论设想的<strong>概念假设推断表</strong>，可以将概念层级联系在一起。</p><table><colgroup><col style="width: 41%"><col style="width: 23%"><col style="width: 17%"><col style="width: 17%"></colgroup><thead><tr class="header"><th></th><th>原始数据</th><th>K个表示中间概念的特征</th><th>高层概念表示</th></tr></thead><tbody><tr class="odd"><td>Groud-truth</td><td>X</td><td><span class="math inline">\(Z^{*}:z^{*} \left( 1 \right)=g^{*}_1,...,z^{*} \left( K \right)=g^{*}_K\)</span></td><td><span class="math inline">\(G^{*}\)</span></td></tr><tr class="even"><td>Learning</td><td>X</td><td>观察单个显著特征 <span class="math inline">\(z \left( i \right)=g_i\)</span></td><td>显著特征下最容易激活的高层概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span></td></tr><tr class="odd"><td>Inference</td><td>X</td><td><span class="math inline">\(\max_{\Theta }P(Z=Z^{*}\mid z^{*}\left( i \right)=z \left( i \right)=g_i,G^{*}=G_{z \left( i \right)=g_i};\Theta)\)</span></td><td><span class="math inline">\(G\)</span></td></tr></tbody></table><p><span class="math display">\[\max_{\Theta }P\left(Z=Z^{*}\mid z^{*}\left( i \right)=z \left( i \right)=g_i,G^{*}=G_{z \left( i \right)=g_i};\Theta\right)\]</span></p><p>公式的含义，表示：数据信息X在传递到中间层时，如果某一属性实体 <span class="math inline">\(z \left(i \right)=g_i\)</span>（弱概念）处于激活态时，那么这一属性实体会导致具有相似属性的更高层概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 处于活跃状态，来进行反向印证概念 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 激活时对应的 的中间层其他属性的一致性： 即此时真实观测的 <span class="math inline">\(z \left( 1 \right),...,z \left( K \right)\)</span> 与 <span class="math inline">\(G_{z \left( i \right)=g_i}\)</span> 激活时其他属性值的期望值 <span class="math inline">\(z^{*}\left( 1 \right),...z^{*} \left( K \right)\)</span> 之间的差异程度，差异越小，那么 <span class="math inline">\(Z\)</span> 越接近于<span class="math inline">\(Z^{*}\)</span>，即优化目标，满足了假设推断。</p><h2 id="怎么将推断应用到当前的机器学习">怎么将推断应用到当前的机器学习？</h2><p>用一个实际的例子来说明，比如我们在一幅图像中检测到了手指的存在，那么我们会推断是一个手臂会大概率的存在，然后我们检测图像中是否有在手臂激活时肩膀、上臂、前臂、肘关节的存在，如果这几个中级概念属性一致，那么我们可以大致推断出这里有张手臂。这种做法的好处，就是概念可以类似积木式地向上堆叠，比如我们观测到手臂后，可以推断是否为人类，来进一步观察是否有毛发、手臂颜色如何、是否有躯干形状、头部形状等等，同理可以类似推断其他物体。而关于中间概念的检测，可以依赖神经网络预测。至于推断层面，不需要神经网络来解决！</p><p>而问题的难点在于中间概念的检测，目前的机器学习都没有去解决这个问题，往往是低层像素程度的特征抽取和高层标签信息的分类，介于两者之间的中间概念是被忽略的。</p><p>而目前有工作，通过CNN可视化在尝试恢复中间层的特征信息。需要思考的两点是：1.如何表示中间特征可视化得到的信息 2.信息从前到后是一个压缩、去噪的有损失过程，怎么能够继续用图像可视化，这是违背了信息传递的基本事实，除非引入了新的信息或者融合了原始信息。</p><p><strong>这是一个开放式的问题</strong></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;推断的介入&quot;&gt;推断的介入&lt;/h1&gt;
&lt;p&gt;善有善报，恶有恶报。&lt;/p&gt;
&lt;p&gt;虽然世间并没有将这份准则执行地那么恰如其分，但是万物归根结底，必然有着因果之间的联系。&lt;/p&gt;
&lt;p&gt;而人生匆匆，为什么愚蠢的我们总是在苦苦寻觅事情的答案，why？&lt;/p&gt;
&lt;p&gt;剪不断，理还乱，只好奉上因果推断&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Causal Inference" scheme="http://senyang-ml.github.io/tags/Causal-Inference/"/>
    
      <category term="Probability Graph Model" scheme="http://senyang-ml.github.io/tags/Probability-Graph-Model/"/>
    
  </entry>
  
  <entry>
    <title>The limitation of thinking with &#39;label&#39; in Deep Learning (深度学习中标签思维的局限)</title>
    <link href="http://senyang-ml.github.io/2018/04/29/label_limitation/"/>
    <id>http://senyang-ml.github.io/2018/04/29/label_limitation/</id>
    <published>2018-04-29T03:52:16.000Z</published>
    <updated>2020-05-24T05:42:57.740Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度学习中标签思维的局限">深度学习中标签思维的局限</h1><p>在生活中，聪明的人常常告诫我们，不要给事物轻易下定义，不要轻易给他人打上标签，因为“标签”的一种贬义替代词汇是“偏见”。然而目前的机器学习正利用人类的标签思维进行学习，所以机器学习到头来，学习到的都是我们的偏见吗？</p><h2 id="端到端是标签思维的一种表现">端到端是标签思维的一种表现</h2><p>我们首先来谈论一下端到端的学习。什么是端到端学习？简单来讲就是，机器学习一个从特定输入到特定输出的过程。端到端的学习应该在深度学习产生后进入了巅峰状态。对物体特征的提取技术由手工设计，如（HOG、Har、SIFT、LBP等），被自动提取技术（CNN）所替代了，少了人工的介入，那么端到端就能够轻易实现了。但实际上，端到端是一种局限的智能，为什么说它是局限的？一方面，它导致了模型的黑箱化，不可解释性，即我们不能认知中间特征数据所表示的内涵，另外，一个样本的信息量是巨大的，若是从不同的角度去解读，提取它的内涵就会得到不同的“标签”，甚至可以提取出内涵对立的标签（<strong>比如，如果我们特意地让机器去区分人类和动物的差异，机器就不能学习到，人类本身就是动物，这一基本事实</strong>），然而机器将大量相似的图像进行归纳学习为一个单独的标签，那么模型学到的是一种呆板的、偏见的抽象，这绝对不是真正的智能，就像Judeal Pearl说的如今的机器学习就是曲线拟合而已。</p><h2 id="标签思维导致对抗样本的产生">标签思维导致对抗样本的产生</h2><p><a href="https://www.jiqizhixin.com/articles/2018-01-06-6" target="_blank" rel="noopener">近来的研究表明</a> DNN 容易受到对抗样本（adversarial example）的影响：在输入中加入精心设计的对抗扰动（adversarial perturbation）可以误导目标 DNN，使其在运行中给该输入加标签时出错。</p><p>一个常见的对抗攻击来自于2014年Goodfellow提出的Fast Gradient Sign Method(FGSM),它通过在梯度方向上进行添加增量来诱导网络对生成的图片X’进行误分类 <span class="math display">\[ x^{,}=x+\varepsilon sign(\bigtriangledown _{x} J(\theta ,x,y)) \]</span></p><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/FGSM.png" width="800"></center><a id="more"></a><p>在图片样本加上精心设计的噪声干扰后，机器会以99.3%的置信度认为图片中动物是<strong>长臂猿</strong>，而不是<strong>熊猫</strong>！</p><p>当在实际世界中应用 DNN 时，这样的对抗样本就会带来安全问题。比如，加上了对抗扰动的输入可以误导自动驾驶汽车的感知系统，使其在分类道路标志时出错，从而可能造成灾难性的后果。</p><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/label_stop.png" width="800"></center><p>谷歌团队的这篇论文<a href="https://arxiv.org/pdf/1801.02774.pdf" target="_blank" rel="noopener">Adversarial Spheres</a>，从<a href="http://mp.weixin.qq.com/s/x5rTpvvCfABWWkjpgnJ5BA" target="_blank" rel="noopener">流形的角度</a>探讨了对抗样本的产生</p><center><img src="http://mmbiz.qpic.cn/mmbiz_png/KmXPKA19gW8XROWibCiaB0victgLYCDcbYyynbLxe0svCF0f7ePuIOpTicBH4wDGS2kibYXJ0HVrQSuOwcAFQIgPjRA/640?wx_fmt=png&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1" width="800"></center><p>对抗样本的破坏能力之所以强大，是因为DNN就没有根本理解图像的内涵，它把标签实际当成了图像中一种可以进行量化的真实属性值，学到的是这个“属性”和其他层次属性（如颜色，纹理，布局，边缘等）的映射关系，这种手工标签和智能所理解的“标签”是有本质的区别的。</p><ul><li><p>手工设计的标签:表示通过设定阈值得到的属性概念的对立程度。比如即图像中一只狗，这种手工标签表示的是“是狗”和“不是狗”这两种概念的对立程度</p></li><li><p>人类智能的“标签”：表示和很多低层信息，中层概念紧密相连，甚至同时存在的一种整体概念连接关系，而不是单纯的表示概率的标量。我们提到标签“狗”时，这个“狗”绝不是单个标量，而是和动物、摇尾巴、宠物、躯干、四肢、舌头、毛发等并行存在的概念。</p></li></ul><p>对于同一种“标签”来说，它的图像差异可能非常大，甚至出现对立，如果你用深度学习去学习像素到标签的映射是非常困难，难以收敛的，或者容易过拟合，所以一些被认为是正常的标签，模型并不认识。</p><p>还有，幽默理论认为：人之所以会笑，是因为突然觉察到概念和实物之间关系的不协调，笑就是对这种不协调之间的反映，所以事物的概念并不是一种固化的东西。</p><h2 id="对抗样本与标签空间">对抗样本与“标签”空间</h2><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/duikang.png" width="50%"></center><p>对抗样本的产生函数：就是利用了标签思维的局限性，利用这一弱点，设计出标签为 A，但是与B（C,D……)类的像素度量距离更近的样本（即，肉眼观察更接近B,C,D,或者指定如C）。</p><p>一般的神经网络其实也是在高维数据空间，寻找多个分界面，作为类与类的区别</p><p>无论模型训练出何种程度，一定可以在高维空间的分界面处寻找到A的对抗样本,数据维度越高，对抗样本在展开维空间上越相似于A，但实际却属于B，所以增加模型参数，有利于寻找对抗样本</p><p>标签思维，就是在扭曲、折叠客观实在的空间中，寻找一个高维流形球，球内球外，作为一个“标签”的判断依据，所以你在图像上，描一个轻微的线，只要你能将被加了一条线的图像样本在扭曲空间中实现一次在分界流形球面的“超球面穿越”，那么这个加工后的图片就能成为对抗样本。所以对抗样本的存在与标签思维训练是一个硬币的正反面，是共同存在的，当下所有用标签训练出来的深度学习模型，无论它声称的鲁棒性有多好，在理论上都是存在一些对抗样本能够完全地混淆模型的预测，如<a href="https://arxiv.org/pdf/1801.02774.pdf" target="_blank" rel="noopener">Adversarial Spheres</a>中所述。</p><h2 id="从标签到实体概念如何表征">从标签到实体，概念如何表征？</h2><p>智能需要多种定义，一种泛化的概念，打破局限的标签思维。标签应该从概念实体中产生，角度不同，标签不同 Hinton2017年提出的capsules，最后一层用活动向量来表示实体，实际上就是从标签思维到概念实体思维的一种突破。另外，深度学习去自主学习低层特征，而不是人工设计特征，这一点是可取的，但是对特征的利用环节，应当引入更多可以设计的接口。这是因为我们人脑的处理信息过程中，从底层的像素信息不是直接映射到概念，中间存在很多层次化的结构，在DNN中是一种粗暴的学习，所以对抗样本才会抓住这一漏洞。而加入一些可介入的中层概念接口，就会提升网络的鲁棒性。</p><h2 id="单一标签描述图片是否可靠">单一标签描述图片是否可靠？</h2>我们先看下列6幅图片，从一般的标签思维出发，其中有4张可以被认为是狗，另外2张被认为是海狮，但是其中几张并不是纯粹的动物照片，而是加入了人为的干扰因素，比如狗穿上了人的服装或者场景的差异。<center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/1.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/2.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/3.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/4.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/5.png" width="30%"></center><center><img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/721df7d9196ccdf5949cc55a694165fdece5448e/dog_otarriinae/6.png" width="50%"></center><p>以上的所有图片，如果按照 标签思维 来分类的话，那么一共有两类：狗和海狮，这两类是如何产生的呢？</p><p><strong>首先</strong>，我们最关注的是图像的主题信息，也就是包含信息最多的部分，一般占据了图像大部分面积，然后我们发现主题信息都是动物，那么动物的种类就完全作为了我们给图像标签的依据（<strong>这是机器学习不到的依据</strong>）</p><p><strong>然后</strong>根据我们的经验，我们通过，耳朵，四肢，毛发长度，脸部五官的分布来区分两个种类。但是其中两幅图有干扰信息，比如人的衣服，帽子，眼镜，这样的干扰对小样本学习来说是致命的，机器很难去忽略此类明显信息而只关注主体，它们让类内差异变得巨大，而第一幅狗的图和两张海狮图，主体信息很明确，基本没有什么干扰，而让机器直接去关注主体的特征之间的差异，甚至可能将其归为一类物体。只有在样本足够足够大时，机器才能学会忽略次要信息，来关注主题信息，这也就是为什么目前的机器学习是需要大数据作支撑的，而非人类的小样本学习。 &gt; 它们都属于动物的这一基本事实，却完全不能用深度网络所学习到的标签表示，也就是说，深度学习训练用到的标签思维，完全是，先利用人类认识事物的基本属性的先验后，再利用图像像素层面寻找数据集中数据分布的差异，然后用几个标量值来表示这种差异的程度，</p><p>这也就是我们 标签思维 去训练物体的固有缺陷，总结一下人类所谓的“标签思维”：</p><ul><li><p>人类的标签思维，是忽略次要信息，只关注主体特征间的类内差异和类间差异，但我们很难去告诉机器，要去忽略哪种次要信息，尤其是当次要信息与主体特征信息难以分开时，次要信息会增加类间差异，导致训练模型过拟合</p></li><li>人类的标签思维，本质是从事物的单一属性出发，是刻板僵化的，是智能的片面反映。图像信息在不同角度的解读下，具有不同属性，如果你将上述图像分为：人类装扮的 和 非人类装扮的，那么可以将其中的两张图归为人类装扮类，剩余的为非人类装扮类。那么这种分类标准和动物种类有什么本质区别吗？</li><li><p>人类的标签思维，是概念层级某一层上的实例描述。这句话什么意思呢？举个例子，在生物分类学上，存在一个层级型的划分依据，把自然界的生物按照：<strong>界门纲目科属种</strong>的层次结构划分，比如，黑猩猩物种 界：动物界 门：脊索动物门 纲：哺乳纲 目：灵长目 科：猩猩科 属：黑猩猩属 种：黑猩猩。而我们脑海中“黑猩猩”的标签，是在一个最常见的认知层次上去认知的，如果它和一个汽车放在一起，我们也可以认为它属于“动物”标签，如果它和牛放在一起，我们可以认为它是“哺乳动物”标签，它和猿猴放在一起，我们认为它是“黑猩猩”标签。那么机器学习，该如何胜任这种划分？它能同时学到，黑猩猩，既有“动物”标签，也有“哺乳动物”标签，还有“黑猩猩”标签吗？</p></li></ul><p>我想是不太容易的。其实事物本身的存在是客观的，而标签是主观产生的，标签来源客观事物从不同角度出发的一种属性。如果按照标签思维去训练第一种分类标准，是得不到第二种分类的判断，反之亦成立，所以打破这种固有缺陷，就必须从实体角度出发，而就是将多种概念属性与实体联合，让机器学到的是一个戴眼镜穿衣服的狗，一个不戴眼镜不穿衣服游泳的海狮。</p><p><strong>概括地讲，人类的标签思维是为视觉感知与语言表达需要的一种折衷，标签最大的用处在于交流与储存，而对于视觉感知来讲，“标签”是一种片面的图像表征，对于智能机器来讲，它应该学到编码更多信息的层级表征，而不单纯是标签</strong></p><h2 id="faster-rcnn系列深度网络成功的直观原因">Faster RCNN系列深度网络成功的直观原因</h2><p>2018年5月23日补充： Faster-RCNN的设计成功很大程度上取决于，多个环节利用了人类视觉本身处理目标的先验能力，比如：</p><ul><li>我们通过大致扫一眼来看到目标物体大概在那个位置--RPN网络</li><li>我们在候选出的区域具体判定区域内的物体是什么类别--Fast RCNN网络</li></ul><p>在faster-rcnn里面，Vgg-16网络预训练出的 特征图(feature map)实际表征了一种数值化的中间抽象概念，而从表征中间抽象概念到标签概念的信息传递是一个有损失的信息降噪、压缩编码过程。这符合人类本身的对事物的认知：试图从事物中取概括一般的内涵，这种内涵在丰富的外延下，保持着鲁棒不变性，反映在图像的像素到抽象概念上，就是从数值表示含义上的不断抽象。</p><p>端到端的学习，让中间产生的语义性、概念性的特征或者表征，不在可见，限制了网络推理的功能，faster-rcnn所具备的思想实际在端到端的过程上利用了一些人类的先验知识，它将先验知识反映在网络架构的设计上，所以下一步的网络应该是一种特征概念开放式的网络，在此基础上可以进行因果推断</p><p><strong>总结一下，标签本质是一种偏见，是一种局限的智能，标签思维的引入使得机器学习面临着对抗样本的考验。而要解决这一问题，一方面需要重新定义事物的概念，另一方面，需要人的推断来介入</strong></p><hr>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;深度学习中标签思维的局限&quot;&gt;深度学习中标签思维的局限&lt;/h1&gt;
&lt;p&gt;在生活中，聪明的人常常告诫我们，不要给事物轻易下定义，不要轻易给他人打上标签，因为“标签”的一种贬义替代词汇是“偏见”。然而目前的机器学习正利用人类的标签思维进行学习，所以机器学习到头来，学习到的都是我们的偏见吗？&lt;/p&gt;
&lt;h2 id=&quot;端到端是标签思维的一种表现&quot;&gt;端到端是标签思维的一种表现&lt;/h2&gt;
&lt;p&gt;我们首先来谈论一下端到端的学习。什么是端到端学习？简单来讲就是，机器学习一个从特定输入到特定输出的过程。端到端的学习应该在深度学习产生后进入了巅峰状态。对物体特征的提取技术由手工设计，如（HOG、Har、SIFT、LBP等），被自动提取技术（CNN）所替代了，少了人工的介入，那么端到端就能够轻易实现了。但实际上，端到端是一种局限的智能，为什么说它是局限的？一方面，它导致了模型的黑箱化，不可解释性，即我们不能认知中间特征数据所表示的内涵，另外，一个样本的信息量是巨大的，若是从不同的角度去解读，提取它的内涵就会得到不同的“标签”，甚至可以提取出内涵对立的标签（&lt;strong&gt;比如，如果我们特意地让机器去区分人类和动物的差异，机器就不能学习到，人类本身就是动物，这一基本事实&lt;/strong&gt;），然而机器将大量相似的图像进行归纳学习为一个单独的标签，那么模型学到的是一种呆板的、偏见的抽象，这绝对不是真正的智能，就像Judeal Pearl说的如今的机器学习就是曲线拟合而已。&lt;/p&gt;
&lt;h2 id=&quot;标签思维导致对抗样本的产生&quot;&gt;标签思维导致对抗样本的产生&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.jiqizhixin.com/articles/2018-01-06-6&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;近来的研究表明&lt;/a&gt; DNN 容易受到对抗样本（adversarial example）的影响：在输入中加入精心设计的对抗扰动（adversarial perturbation）可以误导目标 DNN，使其在运行中给该输入加标签时出错。&lt;/p&gt;
&lt;p&gt;一个常见的对抗攻击来自于2014年Goodfellow提出的Fast Gradient Sign Method(FGSM),它通过在梯度方向上进行添加增量来诱导网络对生成的图片X’进行误分类 &lt;span class=&quot;math display&quot;&gt;\[ x^{,}=x+\varepsilon sign(\bigtriangledown _{x} J(\theta ,x,y)) \]&lt;/span&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/FGSM.png&quot; width=&quot;800&quot;&gt;
&lt;/center&gt;
    
    </summary>
    
    
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Label" scheme="http://senyang-ml.github.io/tags/Label/"/>
    
      <category term="Adversarial Sample" scheme="http://senyang-ml.github.io/tags/Adversarial-Sample/"/>
    
  </entry>
  
  <entry>
    <title>Capsule Network and Human Pose</title>
    <link href="http://senyang-ml.github.io/2018/04/25/capsule-pose/"/>
    <id>http://senyang-ml.github.io/2018/04/25/capsule-pose/</id>
    <published>2018-04-25T01:52:16.000Z</published>
    <updated>2020-05-29T03:00:12.586Z</updated>
    
    <content type="html"><![CDATA[<h1 id="capsules-network">Capsules Network</h1><h2 id="section">2018.4.25</h2><h2 id="引言">引言</h2><p>因为最近同时在看Matrix Capsules with EM routing (https://openreview.net/pdf?id=HJWLfGWRb)和人体姿态估计相关的论文如Associative Embedding, stacked hourglass等，我渐渐发现了这两类研究方向在核心思想上的一些共通之处，即自下而上地获得全局信息，利用全局信息去解析局部特征，并进行预测。</p><a id="more"></a><p>概括地讲，近年来很多取得成功并广泛应用的深度网络设计都蕴含着这一思想，比如Deep Residual Net中的残差模块，Densely Connected Network卷积层的全连接，Fully Connected Network中的特征融合，U-net中的反卷积，还有Stacked Hourglass，其中 hourglass的设计更是淋漓尽致地表达了这一思想，不断pooling获得全局信息，然后结合局部信息进行unsampling，方法简单粗暴，通过关节点位置的监督约束来回归出heatmap。</p><center><img src="https://raw.githubusercontent.com/yangsenius/images/master/stacked%20hourglass.PNG" width="800"></center><p>上述网络设计之所以能够成功一方面和网络层数愈来愈深，还有所谓的Trick有关，另一方面很大程度上是因为它们的设计都在把较为原始的特征信息与加工后的特征信息相融合。</p><p><strong>为什么这样的设计是有效的？或者说如果不这样设计，网络还会起作用吗？</strong></p><p>思考这样的问题能让我们看到问题的本质，那就是这些网络全都是基于卷积网络的架构，而卷积网络中的Pooling层的设计初衷为了得到粗略的响应位置，这会导致精确特征信息的丢失，信息丢失是不可逆的，那在一些需要精确信息的视觉任务中，而能够挽救的方式就是结合原始信息与加工后的信息。可以说，这些设计方式的成功在于它们解决了信息损失的问题。</p><p>然而在上世纪80年代卷积网络中的pooling层被设计出来时，要解决的是图像目标在平移，变换后时依然能够有着相同的输出响应。它并没有考虑日后的一些极其复杂的视觉任务，比如图像分割需要产生一个pixels wise的热值图，分割任务面临的是像素级别的预测，而大多数研究者遇到此类问题，往往是在宏观层面去设计一个更复杂的网络结构去解决问题，这样做是在使模型复杂化，针对他们特定的任务或者数据集而发挥作用，虽然说一定程度弥补了基础网络的能力，但却牺牲了设计对不同视觉任务的泛化能力。如果我们能从基础CNN固有存在的问题出发，那么是否能达到事半功倍的效果呢？</p><p>直到在2017年Hinton指出了在CNN中的固有缺陷，pooling层会导致精确特征信息的丢失，在较低层，这种丢失反映在空间信息上，如果在较高层，语义特征信息的丢失，就会让网络发掘不到更抽象的信息。所以Hinton提出了一种新的网络设计方式capsules net，并证明其能用更少量参数超过CNN性能，这种网络具有仿射变换鲁棒性，更擅长处理重叠问题。而在我的理解看来，他实际上提出的是一种新的信息无损失传递计算方式。从一定程度上讲，这种新的计算方式可以应用到已有的任何卷积网络框架中。</p><p>如下是matrix capsules with EM routing的核心思想，我按照论文给出的思路，形象地画出了一幅前向计算的示意图。还有论文核心路由算法的公式精简版。</p><center><img src="https://raw.githubusercontent.com/yangsenius/images/master/%E8%B4%A1%E7%8C%AE.PNG" width="800"></center><center><img src="https://raw.githubusercontent.com/yangsenius/images/master/em.PNG" width="800"></center><center><img src="https://raw.githubusercontent.com/yangsenius/images/master/%E5%85%AC%E5%BC%8F.PNG" width="800"></center><p>#人体姿态估计</p><p>目前人体姿态估计任务面临的都是图像中人体2D姿态的关键点的回归问题，如果考虑到多人姿态估计，就要涉及到图像的解析问题，即某个关节点到底要分给谁，这是多人重叠问题的难点。当下研究者一般从两个角度去分析这一问题：bottom-up和top-down，即自下而上和自上而下。 在人体姿态估计领域内的研究，自上而下或者自下而上的图像解析都反映在了研究者的设计技巧上和网络结构的设计上，基础网络块都是CNN，还有一些更高层次的设计。而capsules网络独特的一点是，其本身的计算方式就蕴含了一种解析思想。</p><p>它将连续两个featuremap层中，较高层的capsules单元视为“因”（先验-隐变量），较低层的单元视为“果”（数据），然后建立高斯混合分布，它从数据生成角度出发，可以理解为在某一层次的特征级别中，去寻找整体特征与局部特征的关系，即每个低层的capsules试图去在高层寻找一个可以解释（生成）它的母capsule,即似然：P(子capsule|母capsule)。同时，高层的capsules也在寻找并不是偶然产生的来自低层capsules投票产生的紧密簇，这个紧密簇代表许多跟它有强烈关系的低层capsules，如果高层的某个capsule发现了低层capsules越趋于一致（越聚集，就是更服从该capsule的高斯分布），这就意味该capsule与低层capsules的关系总不确定度越低，信息熵越低，那么这个capsule越容易激活。这一过程实际上同时蕴含了bottom-up和top-down，而且基本不存在信息丢失的问题。所以这是我在开头提到为什么Matrix Capsules with EM routing和人体姿态估计的hourglass思想上相通，所以我自然地考虑想把capsule运用到人体姿态估计。</p><ul><li>补充：4月26日</li></ul><p>Facebook提出了DensePose，密集人体姿势估计是指将一个RGB图像中的所有人体像素点映射到人体的3D表面</p><p>#潜力 Capsules被设计的初衷并没有考虑复杂的视觉任务，比如人体姿态估计，那么如何去将它运用到人体姿态估计呢？</p><p>一个简单粗暴的方式就是，直接那它的路由计算去替代CNN的pooling计算，然后网络设计上去模仿当下流行的网络架构 &gt; 补充：4月27日 2018CVPR 提出了Detail-presering pooling in Deep network（https://arxiv.org/pdf/1804.04076.pdf） ，它直接指出来CNN maxl-pooling或者average pooling存在的只选取最大而忽略与周围像素的关联性，一个重视关联性却又直接抹平，并且在实际梯度计算中也有一些drawback，所以该文提出了这个新方法，一句话概括，就是在池化过程中学了一个动态的weight （来自德国学者，我觉得我的想法跟他有点像）</p><p>另一种就是利用capsules本身能够保留精确的空间特征信息，来设计与人体姿态估计匹配的网络结构，比如设计一个heatmap层去加入到capsules网络中去，整个网络即不存在分辨率下降的问题。</p><p>值得提到的一点是，hinton在提出capsules时，他强调了这种设计方式对于处理2D数据和3D数据，是性能上一致的，而matrix capsules with em routing的实验数据集就是一个3D数据集smallNorb，数据集是5类玩具，5个实例，在18个不同方位角，在9种不同高度下，在6种不同光照条件一共下采集了24300的照片，算法在该数据集上达到state-of-art。而人体2D姿态估计是很容易产生在3D场景下姿态的变换问题，所以我个人认为，capsules能够从人体2D姿态在全方位视角下捕捉产生的数据中去寻找一种3D上的全局信息，因为在matrix capsules with em routing这篇论文就在强调capsule中的 4x4 pose matrix 就在学习观察者和物体实例之间的视角关系。</p><p>#未来工作和难点 实现hourglass相关代码和capsulesEM相关代码，DPP的cuda代码也可以考虑相，将算法融合</p><p>实现hourglass相关代码和capsulesEM相关代码，将两者算法融合。</p><p>目前在人体姿态数据集上，性能排名：</p><p><strong>MPII</strong></p>①单人姿态估计 <ceter><img src="https://raw.githubusercontent.com/yangsenius/images/master/mpii1.png" width="800"><p>表现最好的是2018 ArXiv上来自中国科学院的Multi-Scale Structure-Aware Network for Human Pose Estimation论文 https://arxiv.org/pdf/1803.09894.pdf</p><p>（这篇论文包含Hourglass的设计）</p>②多人姿态估计 <ceter><img src="https://raw.githubusercontent.com/yangsenius/images/master/mpii2.png" width="800"><p>表现最好的是NIPS2017 Alejandro Newell发表的Associative Embedding: End-to-End Learning for Joint Detection and Grouping论文 https://arxiv.org/pdf/1611.05424.pdf （hourglass原作者的多人姿态估计设计）</p><p><strong>MS COCO</strong></p><ceter><img src="https://raw.githubusercontent.com/yangsenius/images/master/mscoco.png" width="800"><p>2017年在COCO数据集上性能表现最好的来自Face++提出的Cascaded Pyramid Network for Multi-Person Pose Estimation https://arxiv.org/pdf/1711.07319.pdf</p><p>其设计思想和stacked hourglass有异曲同工之妙，它包含了GlobalNet和RefineNet，不过CPN采用的是top-down的设计方式，FPN网络先检测bouding boxes，然后进行single pose estimation，它可以处理multi-person的姿态估计</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">原创, 禁止转载</span><br></pre></td></tr></table></figure></ceter></ceter></ceter>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;capsules-network&quot;&gt;Capsules Network&lt;/h1&gt;
&lt;h2 id=&quot;section&quot;&gt;2018.4.25&lt;/h2&gt;
&lt;h2 id=&quot;引言&quot;&gt;引言&lt;/h2&gt;
&lt;p&gt;因为最近同时在看Matrix Capsules with EM routing (https://openreview.net/pdf?id=HJWLfGWRb)和人体姿态估计相关的论文如Associative Embedding, stacked hourglass等，我渐渐发现了这两类研究方向在核心思想上的一些共通之处，即自下而上地获得全局信息，利用全局信息去解析局部特征，并进行预测。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Human Pose Estimation" scheme="http://senyang-ml.github.io/tags/Human-Pose-Estimation/"/>
    
      <category term="Deep Learning" scheme="http://senyang-ml.github.io/tags/Deep-Learning/"/>
    
      <category term="Capsule Network" scheme="http://senyang-ml.github.io/tags/Capsule-Network/"/>
    
  </entry>
  
</feed>
