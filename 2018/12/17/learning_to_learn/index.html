<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-minimal.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Meta Learning,Reproduce,PyTorch," />




  


  <link rel="alternate" href="/atom.xml" title="Sen Yang" type="application/atom+xml" />






<meta name="description" content="原创博文，转载请注明来源  引言 “浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！” Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公">
<meta property="og:type" content="article">
<meta property="og:title" content="Learning to learn by gradient descent by gradient descent-PyTorch实践">
<meta property="og:url" content="http://senyang-ml.github.io/2018/12/17/learning_to_learn/index.html">
<meta property="og:site_name" content="Sen Yang">
<meta property="og:description" content="原创博文，转载请注明来源  引言 “浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！” Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公">
<meta property="og:image" content="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201209548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png">
<meta property="og:image" content="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201313760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/blog/learn-to-learn/graph.PNG">
<meta property="og:image" content="https://img-blog.csdnimg.cn/2018112320175122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201810120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201822472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201835829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123201922189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123202026137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123204117756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181123202329192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20181125191428527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2018-12-17T01:52:16.000Z">
<meta property="article:modified_time" content="2020-03-13T10:59:56.000Z">
<meta property="article:author" content="yangsenius">
<meta property="article:tag" content="Meta Learning">
<meta property="article:tag" content="Reproduce">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":2},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://senyang-ml.github.io/2018/12/17/learning_to_learn/"/>





  <title>Learning to learn by gradient descent by gradient descent-PyTorch实践 | Sen Yang</title>
  








<meta name="generator" content="Hexo 4.2.1"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sen Yang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-blogs">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Blogs
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/research/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://senyang-ml.github.io/2018/12/17/learning_to_learn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yangsenius">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sen Yang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Learning to learn by gradient descent by gradient descent-PyTorch实践</h1>
        

        <div class="post-meta">
                  

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">posted on</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-17T09:52:16+08:00">
                2018-12-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          
             <span id="/2018/12/17/learning_to_learn/" class="leancloud_visitors" data-flag-title="Learning to learn by gradient descent by gradient descent-PyTorch实践">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>原创博文，转载请注明来源</p>
</blockquote>
<h2 id="引言">引言</h2>
<p>“浪费75金币买控制守卫有什么用，还不是让人给拆了？我要攒钱！早晚憋出来我的灭世者的死亡之帽！”</p>
<p>Learning to learn，即学会学习，是每个人都具备的能力，具体指的是一种在学习的过程中去反思自己的学习行为来进一步提升学习能力的能力。这在日常生活中其实很常见，比如在通过一本书来学习某个陌生专业的领域知识时（如《机器学习》），面对大量的专业术语与陌生的公式符号，初学者很容易看不下去，而比较好的方法就是先浏览目录，掌握一些简单的概念（回归与分类啊，监督与无监督啊），并在按顺序的阅读过程学会“前瞻”与“回顾”，进行快速学习。又比如在早期接受教育的学习阶段，盲目的“题海战术”或死记硬背的“知识灌输”如果不加上恰当的反思和总结，往往会耗时耗力，最后达到的效果却一般，这是因为在接触新东西，掌握新技能时，是需要“技巧性”的。</p>
<p>从学习知识到学习策略的层面上，总会有“最强王者”在告诉我们，“钻石的操作、黄铜的意识”也许并不能取胜，要“战略上最佳，战术上谨慎”才能更快更好地进步。</p>
<a id="more"></a>
<p>这跟本文要讲的内容有什么关系呢？进入正题。 &gt; &gt; 其实读者可以先回顾自己从初高中到大学甚至研究生的整个历程，是不是发现自己已经具备了“learning to learn”的能力？</p>
<p><img src="https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/0fbfea6bf315e9609bd2baf28c145b6390f8de1c/2-Figure1-1.png" alt="在这里插入图片描述"> ## Learning to learn by gradient descent by gradient descent</p>
<p><strong>通过梯度下降来学习如何通过梯度下降学习</strong></p>
<blockquote>
<p>是否可以让优化器学会 &quot;为了更好地得到，要先去舍弃&quot; 这样的“策略”？</p>
</blockquote>
<p>本博客结合具体实践来解读《Learning to learn by gradient descent by gradient descent》，这是一篇Meta-learning（元学习）领域的<a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">论文</a>,发表在2016年的NIPS。类似“回文”结构的起名，让这篇论文变得有趣，是不是可以再套一层,&quot;Learning to learn to learn by gradient descent by gradient descent by gradient descent&quot;?再套一层？</p>
<p>首先别被论文题目给误导，==它不是求梯度的梯度，这里不涉及到二阶导的任何操作，而是跟如何学会更好的优化有关==，正确的断句方法为learning to (learn by gradient descent ) by gradient descent 。</p>
<p>第一次读完后，不禁惊叹作者巧妙的构思--使用LSTM（long short-term memory）优化器来替代传统优化器如（SGD，RMSProp，Adam等），然后使用梯度下降来优化优化器本身。</p>
<p>虽然明白了作者的出发点，但总感觉一些细节自己没有真正理解。然后就去看原作的代码实现，读起来也是很费劲。查阅了一些博客，但网上对这篇论文解读很少，停留于论文翻译理解上。再次揣摩论文后，打算做一些实验来理解。 在用PyTorch写代码的过程，才恍然大悟，作者的思路是如此简单巧妙，论文名字起的也很恰当，没在故弄玄虚，但是在实现的过程却费劲了周折！</p>
<h2 id="文章目录">文章目录</h2>
<p>[TOC]</p>
<p><strong>如果想看最终版代码和结果，可以直接跳到文档的最后！！</strong></p>
<p>下面写的一些文字与代码主要站在我自身的角度，记录自己在学习研究这篇论文和代码过程中的所有历程，如何想的，遇到了什么错误，问题在哪里，我把自己理解领悟“learning to learn”这篇论文的过程剖析了一下，也意味着我自己也在“learning to learn”！为了展现自己的心路历程，我基本保留了所有的痕迹，这意味着有些代码不够整洁，不过文档的最后是最终简洁完整版。 ==提醒：看完整个文档需要大量的耐心 : )==</p>
<p>我默认读者已经掌握了一些必要知识，也希望通过回顾这些经典研究给自己和一些读者带来切实的帮助和启发。</p>
<p>用Pytorch实现这篇论文想法其实很方便，但是论文作者来自DeepMind，他们用<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow写的项目</a>，读他们的代码你就会领教到最前沿的一线AI工程师们是如何进行工程实践的。</p>
<p>下面进入正题，我会按照最简单的思路，循序渐进地展开, &lt;0..0&gt;。</p>
<h2 id="优化问题">优化问题</h2>
<p>经典的机器学习问题，包括当下的深度学习相关问题，大多可以被表达成一个目标函数的优化问题：</p>
<p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p>
<p>一些优化方法可以求解上述问题，最常见的即梯度更新策略： <span class="math display">\[\theta_{t+1}=\theta_{t}-\alpha_{t}*\nabla f\left ( \theta_{t}\right )\]</span></p>
<p>早期的梯度下降会忽略梯度的二阶信息，而经典的优化技术通过加入<strong>曲率信息</strong>改变步长来纠正，比如Hessian矩阵的二阶偏导数。 <strong>Deep learning</strong>社区的壮大，演生出很多求解高维非凸的优化求解器，如 <strong>momentum</strong>[Nesterov, 1983, Tseng, 1998], <strong>Rprop</strong> [Riedmiller and Braun, 1993], <strong>Adagrad</strong> [Duchi et al., 2011], <strong>RMSprop</strong> [Tieleman and Hinton, 2012], and <strong>ADAM</strong> [Kingma and Ba, 2015].</p>
<p>目前用于大规模图像识别的模型往往使用卷积网络CNN通过定义一个代价函数来拟合数据与标签，其本质还是一个优化问题。</p>
<p>这里我们考虑一个简单的优化问题，比如求一个四次非凸函数的最小值点。对于更复杂的模型，下面的方法同样适用。 ### 定义要优化的目标函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">w = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(w,a=<span class="number">0.5</span>,b=<span class="number">1.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span> <span class="comment">#定义要优化的函数，求x的最优解</span></span><br><span class="line">    x= w*(x<span class="number">-1</span>)</span><br><span class="line">    <span class="keyword">return</span> ((x+<span class="number">1</span>)*(x+<span class="number">0.5</span>)*x*(x<span class="number">-1</span>)).sum()</span><br></pre></td></tr></table></figure>
<h3 id="定义常用的优化器如sgd-rmsprop-adam">定义常用的优化器如SGD, RMSProp, Adam。</h3>
<p>SGD仅仅只是给梯度乘以一个学习率。</p>
<p>RMSProp的方法是：</p>
<p><span class="math display">\[E[g^2]_t = 0.9 E[g^2]_{t-1} + 0.1 g^2_t\]</span></p>
<p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]_t + \epsilon}} g_{t}\]</span></p>
<p>当前时刻下，用当前梯度和历史梯度的平方加权和（越老的历史梯度，其权重越低）来重新调节学习率(如果历史梯度越低，“曲面更平坦”，那么学习率越大，梯度下降更“激进”一些，如果历史梯度越高，“曲面更陡峭”那么学习率越小，梯度下降更“谨慎”一些)，来更快更好地朝着全局最优解收敛。</p>
<p>Adam是RMSProp的变体： <span class="math display">\[m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t \\ 
v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2\]</span></p>
<p><span class="math display">\[\hat{m}_t = \dfrac{m_t}{1 - \beta^t_1} \\ 
\hat{v}_t = \dfrac{v_t}{1 - \beta^t_2}\]</span></p>
<p><span class="math display">\[\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t\]</span> 即通过估计当前梯度的一阶矩估计和二阶矩估计来代替，梯度和梯度的平方，然后更新策略和RMSProp一样。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.1</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">    </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br></pre></td></tr></table></figure>
<p>这里的Adam优化器直接用了Pytorch里定义的。然后我们通过优化器来求解极小值x，通过梯度下降的过程，我们期望的函数值是逐步下降的。 这是我们一般人为设计的学习策略，即==逐步梯度下降法，以“每次都比上一次进步一些” 为原则进行学习！==</p>
<h3 id="接下来-构造优化算法">接下来 构造优化算法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">TRAINING_STEPS = <span class="number">15</span></span><br><span class="line">theta = torch.empty(DIM)</span><br><span class="line">torch.nn.init.uniform_(theta,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">theta_init = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   PyTorch 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i]))+1)*loss</span></span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#print(loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            x = x.detach_()</span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment">#x.retain_grad()</span></span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph</span><br></pre></td></tr></table></figure>
<h3 id="对比不同优化器的优化效果">对比不同优化器的优化效果</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">T = np.arange(TRAINING_STEPS)</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss ))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201209548.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="100"><figcaption>100</figcaption>
</figure>
<pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211</code></pre>
<p>通过上述实验可以发现，这些优化器都可以发挥作用，似乎RMS表现更加优越一些，不过这并不代表RMS就比其他的好,可能这个优化问题还是较为简单，调整要优化的函数，可能就会看到不同的结果。</p>
<h2 id="meta-optimizer-从手工设计优化器迈步到自动设计优化器">Meta-optimizer ：从手工设计优化器迈步到自动设计优化器</h2>
<p>上述这些优化器的更新策略是根据人的经验主观设计，要来解决一般的优化问题的。</p>
<p><strong>No Free Lunch Theorems for Optimization</strong> [Wolpert and Macready, 1997] 表明组合优化设置下，==没有一个算法可以绝对好过一个随机策略==。这暗示，一般来讲，对于一个子问题，特殊化其优化方法是提升性能的唯一方法。</p>
<p>而针对一个特定的优化问题，也许一个特定的优化器能够更好的优化它，我们是否可以不根据人工设计，而是让优化器本身根据模型与数据，自适应地调节，这就涉及到了meta-learning ### 用一个可学习的梯度更新规则，替代手工设计的梯度更新规则</p>
<p><span class="math display">\[\theta_{t+1}=\theta_{t}+g\textit{}_{t}\left (f\left ( \theta_{t}\right ),\phi \right)\]</span></p>
<p>这里的<span class="math inline">\(g(\cdot)\)</span>代表其梯度更新规则函数，通过参数<span class="math inline">\(\phi\)</span>来确定，其输出为目标函数f当前迭代的更新梯度值，<span class="math inline">\(g\)</span>函数通过RNN模型来表示，保持状态并动态迭代</p>
<p>假如一个优化器可以根据历史优化的经验来自身调解自己的优化策略，那么就一定程度上做到了自适应，这个不是说像Adam，momentum，RMSprop那样自适应地根据梯度调节学习率，（其梯度更新规则还是不变的），而是说自适应地改变其梯度更新规则，而Learning to learn 这篇论文就使用LSTM（RNN）优化器做到了这一点，毕竟RNN存在一个可以保存历史信息的隐状态，LSTM可以从一个历史的全局去适应这个特定的优化过程，做到论文提到的所谓的“CoordinateWise”，我的理解是：LSTM的参数对每个时刻节点都保持“聪明”，是一种“全局性的聪明”，适应每分每秒。</p>
<h4 id="构建lstm优化器">构建LSTM优化器</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line">lstm=torch.nn.LSTM(Input_DIM,Hidden_nums ,Layers)</span><br><span class="line">Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span><br><span class="line">batchsize = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">print(lstm)</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">LSTM_Optimizee</span><span class="params">(gradients, state)</span>:</span></span><br><span class="line">    <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">    <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">    gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)   </span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                 torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">   </span><br><span class="line">    update, state = lstm(gradients, state) <span class="comment"># 用optimizee_lstm代替 lstm</span></span><br><span class="line">    update = Linear(update)</span><br><span class="line">    <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">    <span class="keyword">return</span> update.squeeze().squeeze(), state</span><br></pre></td></tr></table></figure>
<pre><code>LSTM(10, 20, num_layers=2)</code></pre>
<p>从上面LSTM优化器的设计来看，我们几乎没有加入任何先验的人为经验在里面，只是用了长短期记忆神经网络的架构</p>
<h4 id="优化器本身的参数即lstm的参数代表了我们的更新策略">优化器本身的参数即LSTM的参数，代表了我们的更新策略</h4>
<p><strong>这个优化器的参数代表了我们的更新策略，后面我们会学习这个参数，即学习用什么样的更新策略</strong></p>
<p>对了如果你不太了解LSTM的话，我就放这个网站 http://colah.github.io/posts/2015-08-Understanding-LSTMs/ 博客的几个图，它很好解释了什么是RNN和LSTM：</p>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png" width="600">
</center>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-C-line.png" width="600">
</center>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-f.png" width="600">
</center>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-i.png" width="600">
</center>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-C.png" width="600">
</center>
<center>
<img src="http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png" width="600">
</center>
<h5 id="好了看一下我们使用刚刚初始化的lstm优化器后的优化结果">好了，看一下我们使用刚刚初始化的LSTM优化器后的优化结果</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">x = np.arange(TRAINING_STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,TRAINING_STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,TRAINING_STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(T, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(T, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(T, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201313760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=289.9213562011719,rms=60.56287384033203,adam=117.2123031616211,lstm=554.2158203125</code></pre>
<h5 id="咦为什么lstm优化器那么差根本没有优化效果">咦，为什么LSTM优化器那么差，根本没有优化效果？</h5>
<p><strong>先别着急质疑！因为我们还没有学习LSTM优化器！</strong></p>
<p>用到的LSTM模型完全是随机初始化的！并且LSTM的参数在TRAIN_STEPS=[0,T]中的每个节点都是保持不变的！</p>
<h4 id="下面我们就来优化lstm优化器的参数">下面我们就来优化LSTM优化器的参数！</h4>
<p>不论是原始优化问题，还是隶属元学习的LSTM优化目标，我们都一个共同的学习目标：</p>
<p><span class="math display">\[\theta ^{*}= \arg\min_{\theta\in \Theta }f\left ( \theta  \right )\]</span></p>
<p>或者说我们希望迭代后的loss值变得很小，传统方法，是基于每个迭代周期，一步一步，让loss值变小，可以说，传统优化器进行梯度下降时所站的视角是在某个周期下的，那么，我们其实可以换一个视角，更全局的视角，即，我们希望所有周期迭代的loss值都很小，这和传统优化是不违背的，并且是全局的，这里做个比喻，优化就像是下棋，优化器就是</p>
<h5 id="下棋手">“下棋手 ”</h5>
<p>如果一个棋手，在每走一步之前，都能看未来很多步被这一步的影响，那么它就能在当前步做出最佳策略，而LSTM的优化过程，就是把一个历史全局的“步”放在一起进行优化，所以LSTM的优化就具备了“瞻前顾后”的能力！</p>
<p>关于这一点，论文给出了一个期望loss的定义： <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]\]</span></p>
<p>但这个实现起来并不现实，我们只需要将其思想具体化。</p>
<ul>
<li><p>Meta-optimizer优化：目标函数“所有周期的loss都要很小！”，而且这个目标函数是独立同分布采样的（比如，这里意味着任意初始化一个优化问题模型的参数，我们都希望这个优化器能够找到一个优化问题的稳定的解）</p></li>
<li><p>传统优化器：&quot;对于当前的目标函数，只要这一步的loss比上一步的loss值要小就行”</p></li>
</ul>
<h5 id="特点-2.考虑优化器优化过程的历史全局性信息-3.独立同分布地采样优化问题目标函数的参数">特点 ： 2.考虑优化器优化过程的历史全局性信息 3.独立同分布地采样优化问题目标函数的参数</h5>
<p>接下来我们就站在更全局的角度，来优化LSTM优化器的参数</p>
<p>LSTM是循环神经网络，它可以连续记录并传递所有周期时刻的信息，其每个周期循环里的子图共同构建一个巨大的图，然后使用Back-Propagation Through Time (BPTT)来求导更新</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">lstm_losses,global_graph_loss= learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span>) <span class="comment"># [loss1,loss2,...lossT] 所有周期的loss</span></span><br><span class="line"><span class="comment"># 因为这里要保留所有周期的计算图所以retain_graph_flag =True</span></span><br><span class="line">all_computing_graph_loss = torch.tensor(lstm_losses).sum() </span><br><span class="line"><span class="comment">#构建一个所有周期子图构成的总计算图,使用BPTT来梯度更新LSTM参数</span></span><br><span class="line"></span><br><span class="line">print(all_computing_graph_loss,global_graph_loss )</span><br><span class="line">print(global_graph_loss)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(554.2158) tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)
tensor(554.2158, grad_fn=&lt;ThAddBackward&gt;)</code></pre>
<p>可以看到，变量<strong>global_graph_loss</strong>保留了所有周期产生的计算图grad_fn=<ThAddBackward></ThAddBackward></p>
<p>下面针对LSTM的参数进行全局优化,优化目标：“所有周期之和的loss都很小”。 值得说明一下：在LSTM优化时的参数，是在所有Unroll_TRAIN_STEPS=[0,T]中保持不变的，在进行完所有Unroll_TRAIN_STEPS以后，再整体优化LSTM的参数。</p>
<p>这也就是论文里面提到的coordinate-wise，即“对每个时刻点都保持‘全局聪明’”，即学习到LSTM的参数是全局最优的了。因为我们是站在所有TRAIN_STEPS=[0,T]的视角下进行的优化！</p>
<p>优化LSTM优化器选择的是Adam优化器进行梯度下降</p>
<h4 id="通过梯度下降法来优化-优化器">通过梯度下降法来优化 优化器</h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Global_Train_Steps = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#print(global_loss_1)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        <span class="comment">#adam_global_optimizer.zero_grad()</span></span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        print(<span class="string">'xxx'</span>,[(z.grad,z.requires_grad) <span class="keyword">for</span> z <span class="keyword">in</span> optimizee.parameters()  ])</span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#print(global_loss)</span></span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure>
<pre><code>xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]
xxx [(None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True), (None, True)]</code></pre>
<h5 id="为什么loss值没有改变为什么lstm参数的梯度不存在的">为什么loss值没有改变？为什么LSTM参数的梯度不存在的？</h5>
<p>通过分析推理，我发现了LSTM参数的梯度为None,那么反向传播就完全没有更新LSTM的参数！</p>
<p>为什么参数的梯度为None呢，优化器并没有更新指定的LSTM的模型参数，一定是什么地方出了问题，我想了好久，还是做一些简单的实验来找一找问题吧。</p>
<blockquote>
<p>ps: 其实写代码做实验的过程，也体现了人类本身学会学习的高级能力，那就是：通过实验来实现想法时，实验结果往往和预期差别很大，那一定有什么地方出了问题，盲目地大量试错法可能找不到真正问题所在，如何找到问题所在并解决，就是一种学会如何学习的能力，也是一种强化学习的能力。这里我采用的人类智能是：以小见大法。 In a word , if we want the machine achieving to AGI, it must imiate human's ability of reasoning and finding where the problem is and figuring out how to solve the problem. Meta Learning contains this idea.</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">z= torch.empty(<span class="number">2</span>)</span><br><span class="line">torch.nn.init.uniform_(z , <span class="number">-2</span>, <span class="number">2</span>)</span><br><span class="line">z.requires_grad = <span class="literal">True</span></span><br><span class="line">z.retain_grad()</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(z)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (z*z).sum()</span><br><span class="line"></span><br><span class="line">optimizer = torch.optim.Adam([z],lr=<span class="number">0.01</span>)</span><br><span class="line">grad =[]</span><br><span class="line">losses= []</span><br><span class="line">zgrad =[]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    q = f(z)</span><br><span class="line">    loss = q**<span class="number">2</span></span><br><span class="line">    <span class="comment">#z.retain_grad()</span></span><br><span class="line">    loss.backward(retain_graph = <span class="literal">True</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    <span class="comment">#print(x,x.grad,loss,)</span></span><br><span class="line">    </span><br><span class="line">    loss.retain_grad()</span><br><span class="line">    print(q.grad,q.requires_grad)</span><br><span class="line">    grad.append((z.grad))</span><br><span class="line">    losses.append(loss)</span><br><span class="line">    zgrad.append(q.grad)</span><br><span class="line">    </span><br><span class="line">print(grad)</span><br><span class="line">print(losses)</span><br><span class="line">print(zgrad)</span><br></pre></td></tr></table></figure>
<pre><code>None True
None True
[tensor([-44.4396, -36.7740]), tensor([-44.4396, -36.7740])]
[tensor(35.9191, grad_fn=&lt;PowBackward0&gt;), tensor(35.0999, grad_fn=&lt;PowBackward0&gt;)]
[None, None]</code></pre>
<h5 id="问题出在哪里">问题出在哪里？</h5>
<p>经过多方面的实验修改，我发现LSTM的参数在每个周期内BPTT的周期内，并没有产生梯度！！怎么回事呢？我做了上面的小实验。</p>
<p>可以看到z.grad = None,但是z.requres_grad = True，z变量作为x变量的子节点，其在计算图中的梯度没有被保留或者没办法获取，那么我就应该通过修改一些PyTorch的代码，使得计算图中的叶子节点的梯度得以存在。然后我找到了retain_grad()这个函数，实验证明，它必须在backward()之前使用才能保存中间叶子节点的梯度！这样的方法也就适合于LSTM优化器模型参数的更新了吧？</p>
<p>那么如何保留LSTM的参数在每个周期中产生的梯度是接下来要修改的！</p>
<p>这是因为我计算loss = f(x)，然后loss.backward() 这里的loss计算并没有和LSTM产生关系，我先来想一想loss和LSTM的关系在哪里？</p>
<p>论文里有一张图，可以作为参考：</p>
<center>
图2<img src="https://raw.githubusercontent.com/yangsenius/yangsenius.github.io/master/blog/learn-to-learn/graph.PNG" width="600">
</center>
<p><strong>LSTM参数的梯度来自于每次输出的“update”的梯度 update的梯度包含在生成的下一次迭代的参数x的梯度中</strong></p>
<p>哦!因为参数<span class="math inline">\(x_t = x_{t-1}+update_{t-1}\)</span> 在BPTT的每个周期里<span class="math inline">\(\frac{\partial loss_t}{\partial \theta_{LSTM}}=\frac{\partial loss_t}{\partial update_{t-1}}*\frac{\partial update_{t-1}}{\partial \theta_{LSTM}}\)</span>,那么我们想通过<span class="math inline">\(loss_0,loss_1,..,loss_t\)</span>之和来更新<span class="math inline">\(\theta_{LSTM}\)</span>的话，就必须让梯度经过<span class="math inline">\(x_t\)</span> 中 的<span class="math inline">\(update_{t-1}\)</span>流回去，那么每次得到的<span class="math inline">\(x_t\)</span>就必须包含了上一次更新产生的图（可以想像，这个计算图是越来越大的），想一想我写的代码，似乎没有保留上一次的计算图在<span class="math inline">\(x_t\)</span>节点中，因为我用了x = x.detach_() 把x从图中拿了下来！这似乎是问题最关键所在！！！（而tensorflow静态图的构建，直接建立了一个完整所有周期的图，似乎Pytorch的动态图不合适？no，no）</p>
<p>（注：以上来自代码中的<span class="math inline">\(x_t\)</span>对应上图的<span class="math inline">\(\theta_t\)</span>，<span class="math inline">\(update_{t}\)</span>对应上图的<span class="math inline">\(g_t\)</span>）</p>
<p>我为什么会加入x = x.detach_() 是因为不加的话，x变成了子节点，下一次求导pytorch不允许，其实只需要加一行x.retain_grad()代码就行了,并且总的计算图的globa_graph_loss在逐步降低！问题解决！</p>
<p>目前在运行global_training(lstm)函数的话，就会发现LSTM的参数已经根据计算图中的梯度回流产生了梯度，每一步可以更新参数了， 但是这个BPTT算法用cpu算起来，有点慢了~</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee.__name__ !=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">            losses.append(loss)</span><br><span class="line">           </span><br><span class="line">            x = x + update</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># x = x.detach_()</span></span><br><span class="line">            <span class="comment">#这个操作 直接把x中包含的图给释放了，</span></span><br><span class="line">            <span class="comment">#那传递给下次训练的x从子节点变成了叶节点，那么梯度就不能沿着这个路回传了，        </span></span><br><span class="line">            <span class="comment">#之前写这一步是因为这个子节点在下一次迭代不可以求导，那么应该用x.retain_grad()这个操作，</span></span><br><span class="line">            <span class="comment">#然后不需要每次新的的开始给x.requires_grad = True</span></span><br><span class="line">            </span><br><span class="line">            x.retain_grad()</span><br><span class="line">            <span class="comment">#print(x.retain_grad())</span></span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam([&#123;<span class="string">'params'</span>:optimizee.parameters()&#125;,&#123;<span class="string">'params'</span>:Linear.parameters()&#125;],lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(LSTM_Optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(lstm)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(193.6147, grad_fn=&lt;ThAddBackward&gt;)
tensor(7.6411)</code></pre>
<h5 id="计算图不再丢失了lstm的参数的梯度经过计算图的流动已经产生了">计算图不再丢失了，LSTM的参数的梯度经过计算图的流动已经产生了！</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/2018112320175122.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">15</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201810120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201822472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=48.513607025146484,rms=5.537945747375488,adam=11.781242370605469,lstm=15.151629447937012</code></pre>
<p>可以看出来，经过优化后的LSTM优化器，似乎已经开始掌握如何优化的方法，即我们基本训练出了一个可以训练模型的优化器！</p>
<p><strong>但是效果并不是很明显！</strong></p>
<p>不过代码编写取得一定进展 (0 ..0) /，接下就是让效果更明显，性能更稳定了吧？</p>
<p><strong>不过我先再多测试一些周期看看LSTM的优化效果！先别高兴太早！</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">50</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    <span class="comment">#plt.yscale('log')</span></span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201835829.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=150.99790954589844,rms=5.940474033355713,adam=14.17563247680664,lstm=268.0199279785156</code></pre>
<h4 id="又出了什么幺蛾子">又出了什么幺蛾子？</h4>
<p>即我们基本训练出了一个可以训练模型的优化器！但是 经过更长周期的测试，发现训练好的优化器只是优化了指定的周期的loss，而并没有学会“全局优化”的本领，这个似乎是个大问题！</p>
<h5 id="不同周期下输入lstm的梯度幅值数量级不在一个等级上面">不同周期下输入LSTM的梯度幅值数量级不在一个等级上面</h5>
<p>要处理一下！</p>
<p>论文里面提到了对梯度的预处理，即处理不同数量级别的梯度，来进行BPTT，因为每个周期的产生的梯度幅度是完全不在一个数量级，前期梯度下降很快，中后期梯度下降平缓，这个对于LSTM的输入，变化裕度太大，应该归一化，但这个我并没有考虑，接下似乎该写这个部分的代码了</p>
<p>论文里提到了： One potential challenge in training optimizers is that different input coordinates (i.e. the gradients w.r.t. different optimizee parameters) can have very different magnitudes. This is indeed the case e.g. when the optimizee is a neural network and different parameters correspond to weights in different layers. This can make training an optimizer difficult, because neural networks naturally disregard small variations in input signals and concentrate on bigger input values.</p>
<h5 id="用梯度的归一化幅值方向二元组替代原梯度作为lstm的输入">用梯度的（归一化幅值，方向）二元组替代原梯度作为LSTM的输入</h5>
<p>To this aim we propose to preprocess the optimizer's inputs. One solution would be to give the optimizer <span class="math inline">\(\left(\log(|\nabla|),\,\operatorname{sgn}(\nabla)\right)\)</span> as an input, where <span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p>
<p><span class="math inline">\(\nabla\)</span> is the gradient in the current timestep. This has a problem that <span class="math inline">\(\log(|\nabla|)\)</span> diverges for <span class="math inline">\(\nabla \rightarrow 0\)</span>. Therefore, we use the following preprocessing formula</p>
<p><span class="math display">\[\nabla^k\rightarrow  \left\{\begin{matrix}
 \left(\frac{\log(|\nabla|)}{p}\,, \operatorname{sgn}(\nabla)\right)&amp; \text{if } |\nabla| \geq e^{-p}\\ 
(-1, e^p \nabla)   &amp; \text{otherwise}
\end{matrix}\right.\]</span></p>
<p>作者将不同幅度和方向下的梯度，用一个标准化到<span class="math inline">\([-1,1]\)</span>的幅值和符号方向二元组来表示原来的梯度张量！这样将有助于LSTM的参数学习！ 那我就重新定义LSTM优化器！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line"></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line"><span class="comment"># "coordinate-wise" RNN </span></span><br><span class="line"><span class="comment">#lstm1=torch.nn.LSTM(Input_DIM*2,Hidden_nums ,Layers)</span></span><br><span class="line"><span class="comment">#Linear = torch.nn.Linear(Hidden_nums,Output_DIM)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size)</span><br><span class="line">        <span class="comment">#elf.lstm = torch.nn.LSTM(10, 20,2)</span></span><br><span class="line">        <span class="comment">#elf.Linear = torch.nn.Linear(20,10)</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作"""</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                         torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        </span><br><span class="line">        update = Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,gradients, prev_state)</span>:</span></span><br><span class="line">       </span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = gradients.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">        </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line"></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM*<span class="number">2</span>, Output_DIM, Hidden_nums ,Layers , batchsize=<span class="number">1</span>,)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">grads = torch.randn(<span class="number">10</span>)*<span class="number">10</span></span><br><span class="line">print(grads.size())</span><br><span class="line"></span><br><span class="line">update,state =  LSTM_Optimizee(grads,<span class="literal">None</span>)</span><br><span class="line">print(update.size(),)</span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([10])
torch.Size([10])</code></pre>
<p>编写成功！ 执行！</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">(optimizee,unroll_train_steps,retain_graph_flag=False,reset_theta = False)</span>:</span> </span><br><span class="line">    <span class="string">"""retain_graph_flag=False   默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">    #  reset_theta = False     默认每次学习前 不随机初始化参数"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">        theta_new = torch.empty(DIM)</span><br><span class="line">        torch.nn.init.uniform_(theta_new,a=<span class="number">-1</span>,b=<span class="number">1.0</span>) </span><br><span class="line">        theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">        x = theta_init_new</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        x = theta_init</span><br><span class="line">        </span><br><span class="line">    global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">    state = <span class="literal">None</span></span><br><span class="line">    x.requires_grad = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):     </span><br><span class="line">            loss = f(x)    </span><br><span class="line">            <span class="comment">#global_loss_graph += torch.exp(torch.Tensor([-i/20]))*loss</span></span><br><span class="line">            <span class="comment">#global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">           <span class="comment"># print('loss&#123;&#125;:'.format(i),loss)</span></span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">            </span><br><span class="line">            update, state = optimizee(x.grad, state)</span><br><span class="line">           <span class="comment">#print(update)</span></span><br><span class="line">            losses.append(loss)     </span><br><span class="line">            x = x + update  </span><br><span class="line">            x.retain_grad()</span><br><span class="line">        <span class="keyword">return</span> losses ,global_loss_graph </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        losses = []</span><br><span class="line">        x.requires_grad = <span class="literal">True</span></span><br><span class="line">        optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(unroll_train_steps):</span><br><span class="line">            </span><br><span class="line">            optimizee.zero_grad()</span><br><span class="line">            loss = f(x)</span><br><span class="line">            </span><br><span class="line">            global_loss_graph += loss</span><br><span class="line">            </span><br><span class="line">            loss.backward(retain_graph=retain_graph_flag)</span><br><span class="line">            optimizee.step()</span><br><span class="line">            losses.append(loss.detach_())</span><br><span class="line">        <span class="comment">#print(x)</span></span><br><span class="line">        <span class="keyword">return</span> losses,global_loss_graph </span><br><span class="line">    </span><br><span class="line">Global_Train_Steps = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">global_training</span><span class="params">(optimizee)</span>:</span></span><br><span class="line">    global_loss_list = []    </span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = <span class="number">0.0001</span>)</span><br><span class="line">    _,global_loss_1 = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">True</span>)</span><br><span class="line">    print(global_loss_1)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps):    </span><br><span class="line">        _,global_loss = learn(optimizee,TRAINING_STEPS,retain_graph_flag =<span class="literal">True</span> ,reset_theta = <span class="literal">False</span>)       </span><br><span class="line">        adam_global_optimizer.zero_grad()</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#print(i,global_loss)</span></span><br><span class="line">        global_loss.backward() <span class="comment">#每次都是优化这个固定的图，不可以释放动态图的缓存</span></span><br><span class="line">        <span class="comment">#print('xxx',[(z,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        adam_global_optimizer.step()</span><br><span class="line">        <span class="comment">#print('xxx',[(z.grad,z.requires_grad) for z in optimizee.parameters()  ])</span></span><br><span class="line">        global_loss_list.append(global_loss.detach_())</span><br><span class="line">        </span><br><span class="line">    print(global_loss)</span><br><span class="line">    <span class="keyword">return</span> global_loss_list</span><br><span class="line"></span><br><span class="line"><span class="comment"># 要把图放进函数体内，直接赋值的话图会丢失</span></span><br><span class="line"><span class="comment"># 优化optimizee</span></span><br><span class="line">global_loss_list = global_training(LSTM_Optimizee)</span><br></pre></td></tr></table></figure>
<pre><code>tensor(239.6029, grad_fn=&lt;ThAddBackward&gt;)
tensor(158.0625)</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Global_T = np.arange(Global_Train_Steps)</span><br><span class="line"></span><br><span class="line">p1, = plt.plot(Global_T, global_loss_list, label=<span class="string">'Global_graph_loss'</span>)</span><br><span class="line">plt.legend(handles=[p1])</span><br><span class="line">plt.title(<span class="string">'Training LSTM optimizee by gradient descent '</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201904948.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">STEPS = <span class="number">30</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span>  </span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = learn(SGD,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    rms_losses, rms_sum_loss = learn(RMS,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    adam_losses, adam_sum_loss = learn(Adam,STEPS,reset_theta=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses,lstm_sum_loss = learn(LSTM_Optimizee,STEPS,reset_theta=<span class="literal">True</span>,retain_graph_flag = <span class="literal">True</span>)</span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"sum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123201922189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=94.19924926757812,rms=5.705832481384277,adam=13.772469520568848,lstm=1224.2393798828125</code></pre>
<p><strong>为什么loss出现了NaN？？？为什么优化器的泛化性能很差？？很烦</strong></p>
<p>即超过Unroll周期以后，LSTM优化器不再具备优化性能？？？</p>
<p>然后我又回顾论文，发现，对优化器进行优化的每个周期开始，要重新随机化，优化问题的参数，即确保我们的LSTM不针对一个特定优化问题过拟合，什么？优化器也会过拟合！是的！l</p>
<p>论文里面其实也有提到：</p>
<p>Given a distribution of functions <span class="math inline">\(f\)</span> we will write the expected loss as: <span class="math display">\[ L\left(\phi \right) =E_f \left[ f \left ( \theta ^{*}\left ( f,\phi  \right )\right ) \right]（1）\]</span></p>
<p>其中f是随机分布的，那么就需要在Unroll的初始进行从IID 标准Gaussian分布随机采样函数的参数</p>
<p>另外我完善了代码，根据原作代码实现，把LSTM的输出乘以一个系数0.01，那么LSTM的学习变得更加快速了。</p>
<p>还有一个地方，就是作者优化optimiee用了100个周期，即5个连续的Unroll周期，这一点似乎我之前也没有考虑到！</p>
<hr>
<hr>
<h2 id="以上是代码编写遇到的种种问题下面就是最完整的有效代码了">以上是代码编写遇到的种种问题，下面就是最完整的有效代码了！！！</h2>
<p><strong>我们考虑优化论文中提到的Quadratic函数，并且用论文中完全一样的实验条件！</strong></p>
<p><span class="math display">\[f(\theta) = \|W\theta - y\|_2^2\]</span> for different 10x10 matrices <span class="math inline">\(W\)</span> and 10-dimensional vectors <span class="math inline">\(y\)</span> whose elements are drawn from an IID Gaussian distribution. Optimizers were trained by optimizing random functions from this family and tested on newly sampled functions from the same distribution. Each function was optimized for 100 steps and the trained optimizers were unrolled for 20 steps. We have not used any preprocessing, nor postprocessing.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding: utf-8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Learning to learn by gradient descent by gradient descent</span></span><br><span class="line"><span class="comment"># =========================#</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># https://arxiv.org/abs/1611.03824</span></span><br><span class="line"><span class="comment"># https://yangsenius.github.io/blog/LSTM_Meta/</span></span><br><span class="line"><span class="comment"># https://github.com/yangsenius/learning-to-learn-by-pytorch</span></span><br><span class="line"><span class="comment"># author：yangsen</span></span><br><span class="line"><span class="comment"># #### “通过梯度下降来学习如何通过梯度下降学习”</span></span><br><span class="line"><span class="comment"># #### 要让优化器学会这样   "为了更好地得到，要先去舍弃"  这样类似的知识！</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> timeit <span class="keyword">import</span> default_timer <span class="keyword">as</span> timer</span><br><span class="line"><span class="comment">#####################      优化问题   ##########################</span></span><br><span class="line">USE_CUDA = <span class="literal">False</span></span><br><span class="line">DIM = <span class="number">10</span></span><br><span class="line">batchsize = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    USE_CUDA = <span class="literal">True</span>  </span><br><span class="line">USE_CUDA = <span class="literal">False</span>  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'\n\nUSE_CUDA = &#123;&#125;\n\n'</span>.format(USE_CUDA))</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(W,Y,x)</span>:</span></span><br><span class="line">    <span class="string">"""quadratic function : f(\theta) = \|W\theta - y\|_2^2"""</span></span><br><span class="line">    <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">        W = W.cuda()</span><br><span class="line">        Y = Y.cuda()</span><br><span class="line">        x = x.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ((torch.matmul(W,x.unsqueeze(<span class="number">-1</span>)).squeeze()-Y)**<span class="number">2</span>).sum(dim=<span class="number">1</span>).mean(dim=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">###############################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment">######################    手工的优化器   ###################</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SGD</span><span class="params">(gradients, state, learning_rate=<span class="number">0.001</span>)</span>:</span></span><br><span class="line">   </span><br><span class="line">    <span class="keyword">return</span> -gradients*learning_rate, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">RMS</span><span class="params">(gradients, state, learning_rate=<span class="number">0.01</span>, decay_rate=<span class="number">0.9</span>)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> state <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        state = torch.zeros(DIM)</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA == <span class="literal">True</span>:</span><br><span class="line">            state = state.cuda()</span><br><span class="line">            </span><br><span class="line">    state = decay_rate*state + (<span class="number">1</span>-decay_rate)*torch.pow(gradients, <span class="number">2</span>)</span><br><span class="line">    update = -learning_rate*gradients / (torch.sqrt(state+<span class="number">1e-5</span>))</span><br><span class="line">    <span class="keyword">return</span> update, state</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> torch.optim.Adam()</span><br><span class="line"></span><br><span class="line"><span class="comment">##########################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#####################    自动 LSTM 优化器模型  ##########################</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTM_Optimizee_Model</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""LSTM优化器"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,input_size,output_size, hidden_size, num_stacks, batchsize, preprocess = True ,p = <span class="number">10</span> ,output_scale = <span class="number">1</span>)</span>:</span></span><br><span class="line">        super(LSTM_Optimizee_Model,self).__init__()</span><br><span class="line">        self.preprocess_flag = preprocess</span><br><span class="line">        self.p = p</span><br><span class="line">        self.input_flag = <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> preprocess != <span class="literal">True</span>:</span><br><span class="line">             self.input_flag = <span class="number">1</span></span><br><span class="line">        self.output_scale = output_scale <span class="comment">#论文</span></span><br><span class="line">        self.lstm = torch.nn.LSTM(input_size*self.input_flag, hidden_size, num_stacks)</span><br><span class="line">        self.Linear = torch.nn.Linear(hidden_size,output_size) <span class="comment">#1-&gt; output_size</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">LogAndSign_Preprocess_Gradient</span><span class="params">(self,gradients)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          gradients: `Tensor` of gradients with shape `[d_1, ..., d_n]`.</span></span><br><span class="line"><span class="string">          p       : `p` &gt; 0 is a parameter controlling how small gradients are disregarded </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          `Tensor` with shape `[d_1, ..., d_n-1, 2 * d_n]`. The first `d_n` elements</span></span><br><span class="line"><span class="string">          along the nth dimension correspond to the `log output` \in [-1,1] and the remaining</span></span><br><span class="line"><span class="string">          `d_n` elements to the `sign output`.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        p  = self.p</span><br><span class="line">        log = torch.log(torch.abs(gradients))</span><br><span class="line">        clamp_log = torch.clamp(log/p , min = <span class="number">-1.0</span>,max = <span class="number">1.0</span>)</span><br><span class="line">        clamp_sign = torch.clamp(torch.exp(torch.Tensor(p))*gradients, min = <span class="number">-1.0</span>, max =<span class="number">1.0</span>)</span><br><span class="line">        <span class="keyword">return</span> torch.cat((clamp_log,clamp_sign),dim = <span class="number">-1</span>) <span class="comment">#在gradients的最后一维input_dims拼接</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Output_Gradient_Increment_And_Update_LSTM_Hidden_State</span><span class="params">(self, input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="string">"""LSTM的核心操作</span></span><br><span class="line"><span class="string">        coordinate-wise LSTM """</span></span><br><span class="line">        <span class="keyword">if</span> prev_state <span class="keyword">is</span> <span class="literal">None</span>: <span class="comment">#init_state</span></span><br><span class="line">            prev_state = (torch.zeros(Layers,batchsize,Hidden_nums),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums))</span><br><span class="line">            <span class="keyword">if</span> USE_CUDA :</span><br><span class="line">                 prev_state = (torch.zeros(Layers,batchsize,Hidden_nums).cuda(),</span><br><span class="line">                            torch.zeros(Layers,batchsize,Hidden_nums).cuda())</span><br><span class="line">         			</span><br><span class="line">        update , next_state = self.lstm(input_gradients, prev_state)</span><br><span class="line">        update = self.Linear(update) * self.output_scale <span class="comment">#因为LSTM的输出是当前步的Hidden，需要变换到output的相同形状上 </span></span><br><span class="line">        <span class="keyword">return</span> update, next_state</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,input_gradients, prev_state)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            input_gradients = input_gradients.cuda()</span><br><span class="line">        <span class="comment">#LSTM的输入为梯度，pytorch要求torch.nn.lstm的输入为（1，batchsize,input_dim）</span></span><br><span class="line">        <span class="comment">#原gradient.size()=torch.size[5] -&gt;[1,1,5]</span></span><br><span class="line">        gradients = input_gradients.unsqueeze(<span class="number">0</span>)</span><br><span class="line">      </span><br><span class="line">        <span class="keyword">if</span> self.preprocess_flag == <span class="literal">True</span>:</span><br><span class="line">            gradients = self.LogAndSign_Preprocess_Gradient(gradients)</span><br><span class="line">      </span><br><span class="line">        update , next_state = self.Output_Gradient_Increment_And_Update_LSTM_Hidden_State(gradients , prev_state)</span><br><span class="line">        <span class="comment"># Squeeze to make it a single batch again.[1,1,5]-&gt;[5]</span></span><br><span class="line">        update = update.squeeze().squeeze()</span><br><span class="line">       </span><br><span class="line">        <span class="keyword">return</span> update , next_state</span><br><span class="line">    </span><br><span class="line"><span class="comment">#################   优化器模型参数  ##############################</span></span><br><span class="line">Layers = <span class="number">2</span></span><br><span class="line">Hidden_nums = <span class="number">20</span></span><br><span class="line">Input_DIM = DIM</span><br><span class="line">Output_DIM = DIM</span><br><span class="line">output_scale_value=<span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   构造一个优化器  #######</span></span><br><span class="line">LSTM_Optimizee = LSTM_Optimizee_Model(Input_DIM, Output_DIM, Hidden_nums ,Layers , batchsize=batchsize,\</span><br><span class="line">                preprocess=<span class="literal">False</span>,output_scale=output_scale_value)</span><br><span class="line">print(LSTM_Optimizee)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    LSTM_Optimizee = LSTM_Optimizee.cuda()</span><br><span class="line">   </span><br><span class="line"></span><br><span class="line"><span class="comment">######################  优化问题目标函数的学习过程   ###############</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Learner</span><span class="params">( object )</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args :</span></span><br><span class="line"><span class="string">        `f` : 要学习的问题</span></span><br><span class="line"><span class="string">        `optimizee` : 使用的优化器</span></span><br><span class="line"><span class="string">        `train_steps` : 对于其他SGD,Adam等是训练周期，对于LSTM训练时的展开周期</span></span><br><span class="line"><span class="string">        `retain_graph_flag=False`  : 默认每次loss_backward后 释放动态图</span></span><br><span class="line"><span class="string">        `reset_theta = False `  :  默认每次学习前 不随机初始化参数</span></span><br><span class="line"><span class="string">        `reset_function_from_IID_distirbution = True` : 默认从分布中随机采样函数 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Return :</span></span><br><span class="line"><span class="string">        `losses` : reserves each loss value in each iteration</span></span><br><span class="line"><span class="string">        `global_loss_graph` : constructs the graph of all Unroll steps for LSTM's BPTT </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,    f ,   optimizee,  train_steps ,  </span></span></span><br><span class="line"><span class="function"><span class="params">                                            eval_flag = False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            retain_graph_flag=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_theta = False ,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            reset_function_from_IID_distirbution = True)</span>:</span></span><br><span class="line">        self.f = f</span><br><span class="line">        self.optimizee = optimizee</span><br><span class="line">        self.train_steps = train_steps</span><br><span class="line">        <span class="comment">#self.num_roll=num_roll</span></span><br><span class="line">        self.eval_flag = eval_flag</span><br><span class="line">        self.retain_graph_flag = retain_graph_flag</span><br><span class="line">        self.reset_theta = reset_theta</span><br><span class="line">        self.reset_function_from_IID_distirbution = reset_function_from_IID_distirbution  </span><br><span class="line">        self.init_theta_of_f()</span><br><span class="line">        self.state = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        self.global_loss_graph = <span class="number">0</span> <span class="comment">#这个是为LSTM优化器求所有loss相加产生计算图准备的</span></span><br><span class="line">        self.losses = []   <span class="comment"># 保存每个训练周期的loss值</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_theta_of_f</span><span class="params">(self,)</span>:</span>  </span><br><span class="line">        <span class="string">''' 初始化 优化问题 f 的参数 '''</span></span><br><span class="line">        self.DIM = <span class="number">10</span></span><br><span class="line">        self.batchsize = <span class="number">128</span></span><br><span class="line">        self.W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">        self.Y = torch.randn(batchsize,DIM)</span><br><span class="line">        self.x = torch.zeros(self.batchsize,self.DIM)</span><br><span class="line">        self.x.requires_grad = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            self.W = self.W.cuda()</span><br><span class="line">            self.Y = self.Y.cuda()</span><br><span class="line">            self.x = self.x.cuda()</span><br><span class="line">        </span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Reset_Or_Reuse</span><span class="params">(self , x , W , Y , state, num_roll)</span>:</span></span><br><span class="line">        <span class="string">''' re-initialize the `W, Y, x , state`  at the begining of each global training</span></span><br><span class="line"><span class="string">            IF `num_roll` == 0    '''</span></span><br><span class="line"></span><br><span class="line">        reset_theta =self.reset_theta</span><br><span class="line">        reset_function_from_IID_distirbution = self.reset_function_from_IID_distirbution</span><br><span class="line"></span><br><span class="line">       </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_theta == <span class="literal">True</span>:</span><br><span class="line">            theta = torch.zeros(batchsize,DIM)</span><br><span class="line">           </span><br><span class="line">            theta_init_new = torch.tensor(theta,dtype=torch.float32,requires_grad=<span class="literal">True</span>)</span><br><span class="line">            x = theta_init_new</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">        <span class="comment">################   每次全局训练迭代，从独立同分布的Normal Gaussian采样函数     ##################</span></span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span> <span class="keyword">and</span> reset_function_from_IID_distirbution == <span class="literal">True</span> :</span><br><span class="line">            W = torch.randn(batchsize,DIM,DIM) <span class="comment">#代表 已知的数据 # 独立同分布的标准正太分布</span></span><br><span class="line">            Y = torch.randn(batchsize,DIM)     <span class="comment">#代表 数据的标签 #  独立同分布的标准正太分布</span></span><br><span class="line">         </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> num_roll == <span class="number">0</span>:</span><br><span class="line">            state = <span class="literal">None</span></span><br><span class="line">            print(<span class="string">'reset W, x , Y, state '</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            W = W.cuda()</span><br><span class="line">            Y = Y.cuda()</span><br><span class="line">            x = x.cuda()</span><br><span class="line">            x.retain_grad()</span><br><span class="line">          </span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span>  x , W , Y , state</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span><span class="params">(self, num_roll=<span class="number">0</span>)</span> :</span> </span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        Total Training steps = Unroll_Train_Steps * the times of  `Learner` been called</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        SGD,RMS,LSTM 用上述定义的</span></span><br><span class="line"><span class="string">         Adam优化器直接使用pytorch里的，所以代码上有区分 后面可以完善！'''</span></span><br><span class="line">        f  = self.f </span><br><span class="line">        x , W , Y , state =  self.Reset_Or_Reuse(self.x , self.W , self.Y , self.state , num_roll )</span><br><span class="line">        self.global_loss_graph = <span class="number">0</span>   <span class="comment">#每个unroll的开始需要 重新置零</span></span><br><span class="line">        optimizee = self.optimizee</span><br><span class="line">        print(<span class="string">'state is None = &#123;&#125;'</span>.format(state == <span class="literal">None</span>))</span><br><span class="line">     </span><br><span class="line">        <span class="keyword">if</span> optimizee!=<span class="string">'Adam'</span>:</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):     </span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                <span class="comment">#self.global_loss_graph += (0.8*torch.log10(torch.Tensor([i+1]))+1)*loss</span></span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">              </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag) <span class="comment"># 默认为False,当优化LSTM设置为True</span></span><br><span class="line">              </span><br><span class="line">                update, state = optimizee(x.grad, state)</span><br><span class="line">              </span><br><span class="line">                self.losses.append(loss)</span><br><span class="line">             </span><br><span class="line">                x = x + update  </span><br><span class="line">                x.retain_grad()</span><br><span class="line">                update.retain_grad()</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.state = (state[<span class="number">0</span>].detach(),state[<span class="number">1</span>].detach())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses ,self.global_loss_graph </span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>: <span class="comment">#Pytorch Adam</span></span><br><span class="line"></span><br><span class="line">            x.detach_()</span><br><span class="line">            x.requires_grad = <span class="literal">True</span></span><br><span class="line">            optimizee= torch.optim.Adam( [x],lr=<span class="number">0.1</span> )</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(self.train_steps):</span><br><span class="line">                </span><br><span class="line">                optimizee.zero_grad()</span><br><span class="line">                loss = f(W,Y,x)</span><br><span class="line">                </span><br><span class="line">                self.global_loss_graph += loss</span><br><span class="line">                </span><br><span class="line">                loss.backward(retain_graph=self.retain_graph_flag)</span><br><span class="line">                optimizee.step()</span><br><span class="line">                self.losses.append(loss.detach_())</span><br><span class="line">                </span><br><span class="line">            <span class="keyword">return</span> self.losses, self.global_loss_graph</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#######   LSTM 优化器的训练过程 Learning to learn   ###############</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Learning_to_learn_global_training</span><span class="params">(optimizee, global_taining_steps, Optimizee_Train_Steps, UnRoll_STEPS, Evaluate_period ,optimizer_lr=<span class="number">0.1</span>)</span>:</span></span><br><span class="line">    <span class="string">""" Training the LSTM optimizee . Learning to learn</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:   </span></span><br><span class="line"><span class="string">        `optimizee` : DeepLSTMCoordinateWise optimizee model</span></span><br><span class="line"><span class="string">        `global_taining_steps` : how many steps for optimizer training o可以ptimizee</span></span><br><span class="line"><span class="string">        `Optimizee_Train_Steps` : how many step for optimizee opimitzing each function sampled from IID.</span></span><br><span class="line"><span class="string">        `UnRoll_STEPS` :: how many steps for LSTM optimizee being unrolled to construct a computing graph to BPTT.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    global_loss_list = []</span><br><span class="line">    Total_Num_Unroll = Optimizee_Train_Steps // UnRoll_STEPS</span><br><span class="line">    adam_global_optimizer = torch.optim.Adam(optimizee.parameters(),lr = optimizer_lr)</span><br><span class="line"></span><br><span class="line">    LSTM_Learner = Learner(f, optimizee, UnRoll_STEPS, retain_graph_flag=<span class="literal">True</span>, reset_theta=<span class="literal">True</span>,)</span><br><span class="line">  <span class="comment">#这里考虑Batchsize代表IID的话，那么就可以不需要每次都重新IID采样</span></span><br><span class="line">  <span class="comment">#即reset_function_from_IID_distirbution = False 否则为True</span></span><br><span class="line"></span><br><span class="line">    best_sum_loss = <span class="number">999999</span></span><br><span class="line">    best_final_loss = <span class="number">999999</span></span><br><span class="line">    best_flag = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(Global_Train_Steps): </span><br><span class="line"></span><br><span class="line">        print(<span class="string">'\n=======&gt; global training steps: &#123;&#125;'</span>.format(i))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> range(Total_Num_Unroll):</span><br><span class="line">            </span><br><span class="line">            start = timer()</span><br><span class="line">            _,global_loss = LSTM_Learner(num)   </span><br><span class="line"></span><br><span class="line">            adam_global_optimizer.zero_grad()</span><br><span class="line">            global_loss.backward() </span><br><span class="line">       </span><br><span class="line">            adam_global_optimizer.step()</span><br><span class="line">            <span class="comment"># print('xxx',[(z.grad,z.requires_grad) for z in optimizee.lstm.parameters()  ])</span></span><br><span class="line">            global_loss_list.append(global_loss.detach_())</span><br><span class="line">            time = timer() - start</span><br><span class="line">            <span class="comment">#if i % 10 == 0:</span></span><br><span class="line">            print(<span class="string">'-&gt; time consuming [&#123;:.1f&#125;s] optimizee train steps :  [&#123;&#125;] | Global_Loss = [&#123;:.1f&#125;] '</span>\</span><br><span class="line">                  .format(time,(num +<span class="number">1</span>)* UnRoll_STEPS,global_loss,))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (i + <span class="number">1</span>) % Evaluate_period == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">            best_sum_loss, best_final_loss, best_flag  = evaluate(best_sum_loss,best_final_loss,best_flag , optimizer_lr)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> global_loss_list,best_flag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(best_sum_loss,best_final_loss, best_flag,lr)</span>:</span></span><br><span class="line">    print(<span class="string">'\n --&gt; evalute the model'</span>)</span><br><span class="line">    STEPS = <span class="number">100</span></span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>, retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line">    lstm_losses, sum_loss = LSTM_learner()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        best = torch.load(<span class="string">'best_loss.txt'</span>)</span><br><span class="line">    <span class="keyword">except</span> IOError:</span><br><span class="line">        <span class="keyword">print</span> (<span class="string">'can not find best_loss.txt'</span>)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        best_sum_loss = best[<span class="number">0</span>]</span><br><span class="line">        best_final_loss = best[<span class="number">1</span>]</span><br><span class="line">        print(<span class="string">"load_best_final_loss and sum_loss"</span>)</span><br><span class="line">    <span class="keyword">if</span> lstm_losses[<span class="number">-1</span>] &lt; best_final_loss <span class="keyword">and</span>  sum_loss &lt; best_sum_loss:</span><br><span class="line">        best_final_loss = lstm_losses[<span class="number">-1</span>]</span><br><span class="line">        best_sum_loss =  sum_loss</span><br><span class="line">        </span><br><span class="line">        print(<span class="string">'\n\n===&gt; update new best of final LOSS[&#123;&#125;]: =  &#123;&#125;, best_sum_loss =&#123;&#125;'</span>.format(STEPS, best_final_loss,best_sum_loss))</span><br><span class="line">        torch.save(LSTM_Optimizee.state_dict(),<span class="string">'best_LSTM_optimizer.pth'</span>)</span><br><span class="line">        torch.save([best_sum_loss ,best_final_loss,lr ],<span class="string">'best_loss.txt'</span>)</span><br><span class="line">        best_flag = <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> best_sum_loss, best_final_loss, best_flag</span><br></pre></td></tr></table></figure>
<pre><code>USE_CUDA = False


LSTM_Optimizee_Model(
  (lstm): LSTM(10, 20, num_layers=2)
  (Linear): Linear(in_features=20, out_features=10, bias=True)
)</code></pre>
<h3 id="我们先来看看随机初始化的lstm优化器的效果">我们先来看看随机初始化的LSTM优化器的效果</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#############  注意：接上一片段的代码！！   #######################3#</span></span><br><span class="line"><span class="comment">##########################   before learning LSTM optimizee ###############################</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1</span>): </span><br><span class="line">   </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<pre><code>reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True</code></pre>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123202026137.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=945.0716552734375,rms=269.4500427246094,adam=134.2750244140625,lstm=562912.125</code></pre>
<h5 id="随机初始化的lstm优化器没有任何效果loss发散了因为还没训练优化器">随机初始化的LSTM优化器没有任何效果，loss发散了，因为还没训练优化器</h5>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#######   注意：接上一段的代码！！</span></span><br><span class="line"><span class="comment">#################### Learning to learn (优化optimizee) ######################</span></span><br><span class="line">Global_Train_Steps = <span class="number">1000</span> <span class="comment">#可修改</span></span><br><span class="line">Optimizee_Train_Steps = <span class="number">100</span></span><br><span class="line">UnRoll_STEPS = <span class="number">20</span></span><br><span class="line">Evaluate_period = <span class="number">1</span> <span class="comment">#可修改</span></span><br><span class="line">optimizer_lr = <span class="number">0.1</span> <span class="comment">#可修改</span></span><br><span class="line">global_loss_list ,flag = Learning_to_learn_global_training(   LSTM_Optimizee,</span><br><span class="line">                                                        Global_Train_Steps,</span><br><span class="line">                                                        Optimizee_Train_Steps,</span><br><span class="line">                                                        UnRoll_STEPS,</span><br><span class="line">                                                        Evaluate_period,</span><br><span class="line">                                                          optimizer_lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show learning process results </span></span><br><span class="line"><span class="comment">#torch.load('best_LSTM_optimizer.pth'))</span></span><br><span class="line"><span class="comment">#import numpy as np</span></span><br><span class="line"><span class="comment">#import matplotlib</span></span><br><span class="line"><span class="comment">#import matplotlib.pyplot as plt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Global_T = np.arange(len(global_loss_list))</span></span><br><span class="line"><span class="comment">#p1, = plt.plot(Global_T, global_loss_list, label='Global_graph_loss')</span></span><br><span class="line"><span class="comment">#plt.legend(handles=[p1])</span></span><br><span class="line"><span class="comment">#plt.title('Training LSTM optimizee by gradient descent ')</span></span><br><span class="line"><span class="comment">#plt.show()</span></span><br></pre></td></tr></table></figure>
<pre><code>=======&gt; global training steps: 0
reset W, x , Y, state 
state is None = True
-&gt; time consuming [0.2s] optimizee train steps :  [20] | Global_Loss = [4009.4] 
state is None = False
-&gt; time consuming [0.3s] optimizee train steps :  [40] | Global_Loss = [21136.7] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [60] | Global_Loss = [136640.5] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [80] | Global_Loss = [4017.9] 
state is None = False
-&gt; time consuming [0.2s] optimizee train steps :  [100] | Global_Loss = [9107.1] 


 --&gt; evalute the model
reset W, x , Y, state 
state is None = True

...........
...........</code></pre>
<p>输出结果已经省略大部分</p>
<h5 id="接下来看一下优化好的lstm优化器模型和sgdrmspropadam的优化性能对比表现吧">接下来看一下优化好的LSTM优化器模型和SGD，RMSProp，Adam的优化性能对比表现吧~</h5>
<p>鸡冻</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">###############  **注意： **接上一片段的代码****</span></span><br><span class="line"><span class="comment">######################################################################3#</span></span><br><span class="line"><span class="comment">##########################   show contrast results SGD,ADAM, RMS ,LSTM ###############################</span></span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> flag ==<span class="literal">True</span> :</span><br><span class="line">    print(<span class="string">'\n==== &gt; load best LSTM model'</span>)</span><br><span class="line">    last_state_dict = copy.deepcopy(LSTM_Optimizee.state_dict())</span><br><span class="line">    torch.save(LSTM_Optimizee.state_dict(),<span class="string">'final_LSTM_optimizer.pth'</span>)</span><br><span class="line">    LSTM_Optimizee.load_state_dict( torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line">    </span><br><span class="line">LSTM_Optimizee.load_state_dict(torch.load(<span class="string">'best_LSTM_optimizer.pth'</span>))</span><br><span class="line"><span class="comment">#LSTM_Optimizee.load_state_dict(torch.load('final_LSTM_optimizer.pth'))</span></span><br><span class="line">STEPS = <span class="number">100</span></span><br><span class="line">x = np.arange(STEPS)</span><br><span class="line"></span><br><span class="line">Adam = <span class="string">'Adam'</span> <span class="comment">#因为这里Adam使用Pytorch</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">2</span>): <span class="comment">#可以多试几次测试实验，LSTM不稳定</span></span><br><span class="line">    </span><br><span class="line">    SGD_Learner = Learner(f , SGD, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    RMS_Learner = Learner(f , RMS, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    Adam_Learner = Learner(f , Adam, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,)</span><br><span class="line">    LSTM_learner = Learner(f , LSTM_Optimizee, STEPS, eval_flag=<span class="literal">True</span>,reset_theta=<span class="literal">True</span>,retain_graph_flag=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    sgd_losses, sgd_sum_loss = SGD_Learner()</span><br><span class="line">    rms_losses, rms_sum_loss = RMS_Learner()</span><br><span class="line">    adam_losses, adam_sum_loss = Adam_Learner()</span><br><span class="line">    lstm_losses, lstm_sum_loss = LSTM_learner()</span><br><span class="line"></span><br><span class="line">    p1, = plt.plot(x, sgd_losses, label=<span class="string">'SGD'</span>)</span><br><span class="line">    p2, = plt.plot(x, rms_losses, label=<span class="string">'RMS'</span>)</span><br><span class="line">    p3, = plt.plot(x, adam_losses, label=<span class="string">'Adam'</span>)</span><br><span class="line">    p4, = plt.plot(x, lstm_losses, label=<span class="string">'LSTM'</span>)</span><br><span class="line">    p1.set_dashes([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p2.set_dashes([<span class="number">4</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    p3.set_dashes([<span class="number">3</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">2</span>])  <span class="comment"># 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    <span class="comment">#p4.set_dashes([2, 2, 10, 2])  # 2pt line, 2pt break, 10pt line, 2pt break</span></span><br><span class="line">    plt.yscale(<span class="string">'log'</span>)</span><br><span class="line">    plt.legend(handles=[p1, p2, p3, p4])</span><br><span class="line">    plt.title(<span class="string">'Losses'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">    print(<span class="string">"\n\nsum_loss:sgd=&#123;&#125;,rms=&#123;&#125;,adam=&#123;&#125;,lstm=&#123;&#125;"</span>.format(sgd_sum_loss,rms_sum_loss,adam_sum_loss,lstm_sum_loss ))</span><br></pre></td></tr></table></figure>
<pre><code>==== &gt; load best LSTM model
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True</code></pre>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123204117756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=967.908935546875,rms=257.03814697265625,adam=122.87742614746094,lstm=105.06891632080078
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True
reset W, x , Y, state 
state is None = True</code></pre>
<figure>
<img src="https://img-blog.csdnimg.cn/20181123202329192.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><figcaption>在这里插入图片描述</figcaption>
</figure>
<pre><code>sum_loss:sgd=966.5319213867188,rms=277.1605224609375,adam=143.6751251220703,lstm=109.35062408447266</code></pre>
<p>（以上代码在一个文件里面执行。复制粘贴格式代码好像需要Chrome或者IE浏览器打开才行？？？）</p>
<h2 id="实验结果分析与结论">实验结果分析与结论</h2>
<p>可以看到：==<strong>SGD 优化器</strong>对于这个问题已经<strong>不具备优化能力</strong>，RMSprop优化器表现良好，<strong>Adam优化器表现依旧突出，LSTM优化器能够媲美甚至超越Adam（Adam已经是业界认可并大规模使用的优化器了）</strong>==</p>
<h3 id="请注意lstm优化器最终优化策略是没有任何人工设计的经验">请注意：LSTM优化器最终优化策略是没有任何人工设计的经验</h3>
<p><strong>是自动学习出的一种学习策略！并且这种方法理论上可以应用到任何优化问题</strong></p>
<p>换一个角度讲，针对给定的优化问题，LSTM可以逼近或超越现有的任何人工优化器，不过对于大型的网络和复杂的优化问题，这个方法的优化成本太大，优化器性能的稳定性也值得考虑，所以这个工作的创意是独特的，实用性有待考虑~~</p>
<p>以上代码参考Deepmind的<a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">Tensorflow版本</a>，遵照论文思路，加上个人理解，力求最简，很多地方写得不够完备，如果有问题，还请多多指出！</p>
<p><img src="https://img-blog.csdnimg.cn/20181125191428527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"> 上图是论文中给出的结果，作者取最好的实验结果的平均表现（试出了最佳学习率？）展示，我用下面和论文中一样的实验条件（不过没使用NAG优化器），基本达到了论文中所示的同样效果？性能稳定性较差一些</p>
<p>（我怀疑论文的实验Batchsize不是代码中的128？又或者作者把batchsize当作函数的随机采样？我这里把batchsize当作确定的参数，随机采样单独编写）。</p>
<h2 id="实验条件">实验条件：</h2>
<ul>
<li>PyTorch-0.4.1 cpu</li>
<li>优化问题为Quadratic函数:</li>
<li>W : [128,10,10] Y: [128 , 10] x: [128, 10] 从IID的标准Gaussian分布中采样，初始 x = 0</li>
<li><p>全局优化器optimizer使用Adam优化器， 学习率为0.1（或许有更好的选择，没有进行对比实验）</p></li>
<li>CoordinateWise LSTM 使用LSTM_Optimizee_Model：
<ul>
<li>(lstm): LSTM(10, 20, num_layers=2)</li>
<li>(Linear): Linear(in_features=20, out_features=10, bias=True)</li>
</ul></li>
<li>==未使用任何数据预处理==（LogAndSign）和后处理</li>
<li>UnRolled Steps = 20 Optimizee_Training_Steps = 100</li>
<li>*Global_Traing_steps = 1000 原代码=10000，或许进一步优化LSTM优化器，能够到达更稳定的效果。</li>
<li><p>另外，原论文进行了mnist和cifar10的实验，本篇博客没有进行实验，代码部分还有待完善，还是希望读者多读原论文和原代码，多动手编程实验！</p></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learn</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="keyword">return</span> <span class="string">"知识"</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">learning_to</span><span class="params">(learn)</span>:</span></span><br><span class="line">    print(learn())</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"学习策略"</span></span><br><span class="line"></span><br><span class="line">print(learning_to(learn))</span><br></pre></td></tr></table></figure>
<h2 id="后叙">后叙</h2>
<blockquote>
<p>人可以从自身认识与客观存在的差异中学习，来不断的提升认知能力，这是最基本的学习能力。而另一种潜在不容易发掘，但却是更强大的能力--在学习中不断调整适应自身与外界的学习技巧或者规则--其实构建了我们更高阶的智能。比如，我们在学习知识时，我们总会先接触一些简单容易理解的基本概念，遇到一些理解起来困难或者十分抽象的概念时，我们往往不是采取强行记忆，即我们并不会立刻学习跟我们当前认知的偏差非常大的事物，而是把它先放到一边，继续学习更多简单的概念，直到有所“领悟”发现先前的困难概念变得容易理解</p>
</blockquote>
<blockquote>
<p>心理学上，元认知被称作反省认知，指人对自我认知的认知。弗拉威尔称，元认知是关于个人自己认知过程的知识和调节这些过程的能力：对思维和学习活动的知识和控制。那么学会适应性地调整学习策略，也成为机器学习的一个研究课题，a most ordinary problem for machine learning is that although we expect to find the invariant pattern in all data, for an individual instance in a specified dataset，it has its own unique attribute, which requires the model taking different policy to understand them seperately .</p>
</blockquote>
<blockquote>
<p>以上均为原创，转载请注明来源<br>
https://blog.csdn.net/senius/article/details/84483329 or https://yangsenius.github.io/blog/LSTM_Meta/ 溜了溜了</p>
</blockquote>
<h2 id="下载地址与参考">下载地址与参考</h2>
<p><strong><a href="https://yangsenius.github.io/blog/LSTM_Meta/learning_to_learn_by_pytorch.py" target="_blank" rel="noopener">下载代码: learning_to_learn_by_pytorch.py</a></strong></p>
<p><a href="https://github.com/yangsenius/learning-to-learn-by-pytorch" target="_blank" rel="noopener">Github地址</a></p>
<p>参考： <a href="https://arxiv.org/abs/1606.04474" target="_blank" rel="noopener">1.Learning to learn by gradient descent by gradient descent</a> <a href="https://github.com/deepmind/learning-to-learn" target="_blank" rel="noopener">2. Learning to learn in Tensorflow by DeepMind</a> <a href="https://hackernoon.com/learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2" target="_blank" rel="noopener">3.learning-to-learn-by-gradient-descent-by-gradient-descent-4da2273d64f2</a></p>
<p><em>目录</em> [TOC]</p>

      
    </div>
    
    
    

    

    

    
      <div>
        
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Author：</strong>
    yangsenius
  </li>
  <li class="post-copyright-link">
    <strong>Link：</strong>
    <a href="http://senyang-ml.github.io/2018/12/17/learning_to_learn/" title="Learning to learn by gradient descent by gradient descent-PyTorch实践">http://senyang-ml.github.io/2018/12/17/learning_to_learn/</a>
  </li>
  <li class="post-copyright-license">
    <strong>License： </strong>
    Unless otherwise stated,  all blogs use the <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> protocol, please indicate the source
  </li>
</ul>


      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Meta-Learning/" rel="tag"># Meta Learning</a>
          
            <a href="/tags/Reproduce/" rel="tag"># Reproduce</a>
          
            <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/12/AE-hourglass/" rel="next" title="Associative embedding End-to-End Learning for Joint Detection and Grouping">
                <i class="fa fa-chevron-left"></i> Associative embedding End-to-End Learning for Joint Detection and Grouping
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/20/metaanchor/" rel="prev" title="MetaAnchor - Learning to Detect Objects with Customized Anchors - 2018 NeurIPS 解读">
                MetaAnchor - Learning to Detect Objects with Customized Anchors - 2018 NeurIPS 解读 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Toc
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yangsenius</p>
              <p class="site-description motion-element" itemprop="description">Talk is not cheap</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">24</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:yangsenius@seu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          
        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#引言"><span class="nav-number">1.</span> <span class="nav-text">引言</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#文章目录"><span class="nav-number">2.</span> <span class="nav-text">文章目录</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化问题"><span class="nav-number">3.</span> <span class="nav-text">优化问题</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#定义常用的优化器如sgd-rmsprop-adam"><span class="nav-number">3.1.</span> <span class="nav-text">定义常用的优化器如SGD, RMSProp, Adam。</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#接下来-构造优化算法"><span class="nav-number">3.2.</span> <span class="nav-text">接下来 构造优化算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对比不同优化器的优化效果"><span class="nav-number">3.3.</span> <span class="nav-text">对比不同优化器的优化效果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#meta-optimizer-从手工设计优化器迈步到自动设计优化器"><span class="nav-number">4.</span> <span class="nav-text">Meta-optimizer ：从手工设计优化器迈步到自动设计优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#构建lstm优化器"><span class="nav-number">4.0.1.</span> <span class="nav-text">构建LSTM优化器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#优化器本身的参数即lstm的参数代表了我们的更新策略"><span class="nav-number">4.0.2.</span> <span class="nav-text">优化器本身的参数即LSTM的参数，代表了我们的更新策略</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#好了看一下我们使用刚刚初始化的lstm优化器后的优化结果"><span class="nav-number">4.0.2.1.</span> <span class="nav-text">好了，看一下我们使用刚刚初始化的LSTM优化器后的优化结果</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#咦为什么lstm优化器那么差根本没有优化效果"><span class="nav-number">4.0.2.2.</span> <span class="nav-text">咦，为什么LSTM优化器那么差，根本没有优化效果？</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#下面我们就来优化lstm优化器的参数"><span class="nav-number">4.0.3.</span> <span class="nav-text">下面我们就来优化LSTM优化器的参数！</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#下棋手"><span class="nav-number">4.0.3.1.</span> <span class="nav-text">“下棋手 ”</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#特点-2.考虑优化器优化过程的历史全局性信息-3.独立同分布地采样优化问题目标函数的参数"><span class="nav-number">4.0.3.2.</span> <span class="nav-text">特点 ： 2.考虑优化器优化过程的历史全局性信息 3.独立同分布地采样优化问题目标函数的参数</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#通过梯度下降法来优化-优化器"><span class="nav-number">4.0.4.</span> <span class="nav-text">通过梯度下降法来优化 优化器</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#为什么loss值没有改变为什么lstm参数的梯度不存在的"><span class="nav-number">4.0.4.1.</span> <span class="nav-text">为什么loss值没有改变？为什么LSTM参数的梯度不存在的？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#问题出在哪里"><span class="nav-number">4.0.4.2.</span> <span class="nav-text">问题出在哪里？</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#计算图不再丢失了lstm的参数的梯度经过计算图的流动已经产生了"><span class="nav-number">4.0.4.3.</span> <span class="nav-text">计算图不再丢失了，LSTM的参数的梯度经过计算图的流动已经产生了！</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#又出了什么幺蛾子"><span class="nav-number">4.0.5.</span> <span class="nav-text">又出了什么幺蛾子？</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#不同周期下输入lstm的梯度幅值数量级不在一个等级上面"><span class="nav-number">4.0.5.1.</span> <span class="nav-text">不同周期下输入LSTM的梯度幅值数量级不在一个等级上面</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#用梯度的归一化幅值方向二元组替代原梯度作为lstm的输入"><span class="nav-number">4.0.5.2.</span> <span class="nav-text">用梯度的（归一化幅值，方向）二元组替代原梯度作为LSTM的输入</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#以上是代码编写遇到的种种问题下面就是最完整的有效代码了"><span class="nav-number">5.</span> <span class="nav-text">以上是代码编写遇到的种种问题，下面就是最完整的有效代码了！！！</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#我们先来看看随机初始化的lstm优化器的效果"><span class="nav-number">5.1.</span> <span class="nav-text">我们先来看看随机初始化的LSTM优化器的效果</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#随机初始化的lstm优化器没有任何效果loss发散了因为还没训练优化器"><span class="nav-number">5.1.0.1.</span> <span class="nav-text">随机初始化的LSTM优化器没有任何效果，loss发散了，因为还没训练优化器</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#接下来看一下优化好的lstm优化器模型和sgdrmspropadam的优化性能对比表现吧"><span class="nav-number">5.1.0.2.</span> <span class="nav-text">接下来看一下优化好的LSTM优化器模型和SGD，RMSProp，Adam的优化性能对比表现吧~</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验结果分析与结论"><span class="nav-number">6.</span> <span class="nav-text">实验结果分析与结论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#请注意lstm优化器最终优化策略是没有任何人工设计的经验"><span class="nav-number">6.1.</span> <span class="nav-text">请注意：LSTM优化器最终优化策略是没有任何人工设计的经验</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实验条件"><span class="nav-number">7.</span> <span class="nav-text">实验条件：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#后叙"><span class="nav-number">8.</span> <span class="nav-text">后叙</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#下载地址与参考"><span class="nav-number">9.</span> <span class="nav-text">下载地址与参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yangsenius</span>

  
</div>









        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  









  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three-waves.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  











<!-- LOCAL: You can save these files to your site and update links -->
  
<!-- END LOCAL -->


    
      <script src="https://utteranc.es/client.js"
        repo="senyang-ml/Comments"
        issue-term="title"
        label="utt-comment"
        theme="github-dark"
        crossorigin="anonymous"
        async>
</script>

      
    






  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("", "");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  
  <script src="https://www.gstatic.com/firebasejs/4.6.0/firebase.js"></script>
  <script src="https://www.gstatic.com/firebasejs/4.6.0/firebase-firestore.js"></script>
  
  <script>
    (function () {

      firebase.initializeApp({
        apiKey: '',
        projectId: ''
      })

      function getCount(doc, increaseCount) {
        //increaseCount will be false when not in article page

        return doc.get().then(function (d) {
          var count
          if (!d.exists) { //has no data, initialize count
            if (increaseCount) {
              doc.set({
                count: 1
              })
              count = 1
            }
            else {
              count = 0
            }
          }
          else { //has data
            count = d.data().count
            if (increaseCount) {
              if (!(window.localStorage && window.localStorage.getItem(title))) { //if first view this article
                doc.set({ //increase count
                  count: count + 1
                })
                count++
              }
            }
          }
          if (window.localStorage && increaseCount) { //mark as visited
            localStorage.setItem(title, true)
          }

          return count
        })
      }

      function appendCountTo(el) {
        return function (count) {
          $(el).append(
            $('<span>').addClass('post-visitors-count').append(
              $('<span>').addClass('post-meta-divider').text('|')
            ).append(
              $('<span>').addClass('post-meta-item-icon').append(
                $('<i>').addClass('fa fa-users')
              )
              ).append($('<span>').text('visitors ' + count))
          )
        }
      }

      var db = firebase.firestore()
      var articles = db.collection('articles')

      //https://hexo.io/zh-tw/docs/variables.html
      var isPost = 'Learning to learn by gradient descent by gradient descent-PyTorch实践'.length > 0
      var isArchive = '' === 'true'
      var isCategory = ''.length > 0
      var isTag = ''.length > 0

      if (isPost) { //is article page
        var title = 'Learning to learn by gradient descent by gradient descent-PyTorch实践'
        var doc = articles.doc(title)

        getCount(doc, true).then(appendCountTo($('.post-meta')))
      }
      else if (!isArchive && !isCategory && !isTag) { //is index page
        var titles = [] //array to titles

        var postsstr = '' //if you have a better way to get titles of posts, please change it
        eval(postsstr)

        var promises = titles.map(function (title) {
          return articles.doc(title)
        }).map(function (doc) {
          return getCount(doc)
        })
        Promise.all(promises).then(function (counts) {
          var metas = $('.post-meta')
          counts.forEach(function (val, idx) {
            appendCountTo(metas[idx])(val)
          })
        })
      }
    })()
  </script>


  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: ,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


      
  


  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=5.1.4"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=5.1.4"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->


  

</body>
</html>
