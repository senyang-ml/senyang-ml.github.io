<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sen Yang | Computer Vision Researcher</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
    <style>
        /* 为 markmap 容器添加明确的尺寸，确保其正确显示 */
        .markmap {
            width: 100%;
            max-width: 900px;
            min-height: 400px; /* Adjusted for more flexibility */
            height: auto; /* Auto height based on content */
            max-height: 650px; /* Maximum height */
            margin: 0 auto;
            border-radius: 8px;
            overflow: hidden;
            position: relative;
            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); /* Subtle shadow for depth */
            background-color: #f9f9f9; /* Light background for the mindmap area */
            padding: 20px; /* Add some internal padding */
        }

        /* 禁用思维导图拖动 */
        .markmap svg {
            pointer-events: auto;
        }

        .markmap svg .markmap-node {
            cursor: pointer;
        }

        /* 添加视觉指示，帮助用户理解思维导图的边界 */
        .markmap-container {
            position: relative;
            max-width: 900px;
            margin: 0 auto;
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            background: linear-gradient(to bottom right, #f5f5f5, #ffffff);
        }

        /* 优化重置按钮样式 */
        .markmap-controls {
            position: absolute;
            top: 10px;
            right: 10px;
            z-index: 100;
        }

        .markmap-controls .control-button {
            background-color: rgba(0, 123, 255, 0.8);
            color: white;
            border: none;
            padding: 5px 10px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
            transition: all 0.2s ease;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .markmap-controls .control-button:hover {
            background-color: rgba(0, 123, 255, 1);
            transform: translateY(-1px);
            box-shadow: 0 3px 5px rgba(0, 0, 0, 0.15);
        }
        
        .markmap-controls .control-button:active {
            transform: translateY(0);
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }
        
        .mindmap .container {
            padding-top: 20px;
            padding-bottom: 20px;
        }

        /* 为 Mermaid 图表添加一些基础样式，确保其可见性 */
        .mermaid-chart-section {
            margin-top: 40px; /* 与上方内容保持一定间距 */
            padding-bottom: 40px;
        }
        .mermaid {
            text-align: center; /* 让图表居中 */
        }
    </style>
</head>
<body>
    <header>
        <nav>
            <div class="logo">Sen Yang</div>
            <ul class="nav-links">
                <li><a href="#about">About Me</a></li>
                <li><a href="#experience">Experience</a></li>
                <li><a href="#education">Education</a></li>
                <li><a href="#publications">Publications</a></li>
                <li><a href="#skills">Skills</a></li>
                <li><a href="#contact">Contact</a></li>
            </ul>
            <div class="language-switch">
                <span class="active">EN</span> | <a href="index.html">中文</a>
            </div>
        </nav>
    </header>

    <section class="hero">
        <div class="container">
            <div class="hero-content">
                <div class="hero-image">
                    <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=z5O3DLcAAAAJ&citpid=5" alt="Sen Yang Photo">
                </div>
                <div class="hero-text">
                    <h1>Sen Yang</h1>
                    <h2>Computer Vision | Multimodal Large Language Models | Autonomous Driving</h2>
                    <p>Senior R&D Engineer at Baidu, focusing on Computer Vision, Multimodal Large Language Models, and Autonomous Driving research.</p>
                    
                    <div class="contact-brief">
                        <p><i class="fas fa-envelope"></i> Email: <a href="mailto:yangsenius@gmail.com">yangsenius@gmail.com</a></p>
                        <p><i class="fas fa-graduation-cap"></i> Google Scholar: <a href="https://scholar.google.com/citations?user=z5O3DLcAAAAJ" target="_blank" rel="noopener noreferrer">Profile</a></p>
                        <p><i class="fas fa-blog"></i> Blog: <a href="https://senyang-ml.github.io" target="_blank" rel="noopener noreferrer">senyang-ml.github.io</a></p>
                    </div>
                    
                    <div class="social-links">
                        <a href="mailto:yangsenius@gmail.com" title="Email"><i class="fas fa-envelope"></i></a>
                        <a href="https://scholar.google.com/citations?user=z5O3DLcAAAAJ" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="fas fa-graduation-cap"></i></a>
                        <a href="https://github.com/senyang-ml" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a>
                        <a href="https://senyang-ml.github.io" title="Blog" target="_blank" rel="noopener noreferrer"><i class="fas fa-blog"></i></a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Mindmap Section -->
    <section id="mindmap" class="mindmap">
        <div class="container">
            <h2 class="section-title">TL;DR (Overview)</h2>
            <div class="markmap-container">
                <div class="markmap-controls">
                    <button id="reset-view-btn" class="control-button" title="Refresh">
                        <i class="fas fa-sync-alt"></i> Refresh
                    </button>
                </div>
                <div class="markmap">
                    <pre class="markmap-content"># **Sen Yang's Personal Website**
## About Me
- **Computer Vision Researcher**
- Research Interests:
  - Computer Vision
  - Multimodal Large Language Models
  - Autonomous Driving
## Education
- **Ph.D.**: Southeast University (2019.5-2023.3)
- Master: Southeast University (2017.9-2019.1)
- Bachelor: Jilin University (2013.9-2017.7)
## Work Experience
- **Baidu VIS Senior R&D Engineer** (2023.7-Present)
- Tencent TPG Intern (2021.12-2022.8)
- Megvii Intern (2021.1-2021.10)
## Research Publications
- **Autonomous Driving**
  - TopoSD: Topology-Enhanced Lane Segment Perception
  - MGMapNet: Multi-Granularity Representation Learning
- **Multimodal Large Models**
  - Vision Remember: Alleviating Visual Forgetting in Efficient MLLM
- **Pose Estimation**
  - Detecting and grouping keypoints
  - Capturing the motion of every joint
  - Searching part-specific neural fabrics
  - SimCC: A Simple Coordinate Classification
  - TokenPose: Learning Keypoint Tokens
  - TransPose: Keypoint Localization via Transformer
## Technical Skills
- **Multimodal Large Models**
  - MLLM Architectures: LLaVA, Qwen2.5-VL, LISA
  - Training Techniques: SFT, Autoregressive Models, RL
  - Visual Token Compression, Large-scale Distributed Training
- **Autonomous Driving Perception**
  - BEV Visual Mapping, Temporal Modeling
  - Multimodal Fusion: Vision + Map Structured Data
  - Navigation Map Integration, Probabilistic Planning
- **Deep Learning Frameworks**
  - PyTorch, Python, C++
  - Transformer Models, GPU/Ascend NPU Development
## Contact Information
- Email: yangsenius@gmail.com
- Blog: senyang-ml.github.io
- Google Scholar Profile</pre>
                </div>
                <div class="markmap-tip" style="text-align: center; padding: 10px; color: #666; font-size: 14px;"></div>
            </div>
        </div>
    </section>

    <!-- New Mermaid Gantt Chart Section -->
    <section id="experience-gantt" class="mermaid-chart-section">
        <div class="container">
            <!-- <h2 class="section-title">Personal Experience Timeline</h2> -->
            <div class="mermaid">
gantt
    dateFormat  YYYY.MM
    title Sen Yang Personal Experience Timeline

    section Education Background
    Bachelor's Degree           :edu1, 2013.09, 2017.07
    Master's Degree           :edu2, 2017.09, 2019.01
    Ph.D. Degree           :edu3, 2019.05, 2023.03

    section Work Experience
    Megvii Intern       :work1, 2021.01, 2021.10
    Tencent PCG Intern        :work2, 2021.12, 2022.08
    Baidu Senior R&D Engineer :work3, 2023.07, 2025.06
            </div>
        </div>
    </section>
    
    <section id="about" class="about">
        <div class="container">
            <h2 class="section-title">About Me</h2>
            <div class="about-content">
                <p>I am a research engineer at Baidu, primarily engaged in computer vision, multimodal large language models, and autonomous driving. I received my Ph.D. from Southeast University in 2023. My research focuses on computer vision and deep learning, with particular attention to 2D/3D human pose estimation, autonomous driving perception, and visual multimodal foundation models. I am passionate about developing innovative solutions that combine cutting-edge research with practical applications.</p>
                <p>My research interests include:</p>
                <ul class="interests-list">
                    <li>Computer Vision</li>
                    <li>Deep Learning</li>
                    <li>Human Pose Estimation</li>
                    <li>Autonomous Driving Perception</li>
                    <li>Multimodal Foundation Models</li>
                </ul>
            </div>
        </div>
    </section>

    <section id="experience" class="experience">
        <div class="container">
            <h2 class="section-title">Work and Internship Experience</h2>
            <div class="experience-grid">
                <div class="experience-card job-fulltime">
                    <div class="card-header">
                        <span class="icon"><i class="fas fa-briefcase"></i></span>
                        <h3>Baidu VIS</h3>
                    </div>
                    <p class="position">Senior R&D Engineer</p>
                    <p class="date">2023.7 - Present</p>
                    <p class="description">Responsible for in-depth research and innovative applications in multimodal large models, computer vision perception, and decision-making algorithms, aiming to push the boundaries of technology and solve complex challenges. My work encompasses the entire process from cutting-edge algorithm design to product deployment, focusing on translating theoretical breakthroughs into practical business value and achieving significant progress in multiple core areas.</p>
                </div>
                <div class="experience-card job-internship">
                    <div class="card-header">
                        <span class="icon"><i class="fas fa-lightbulb"></i></span>
                        <h3>Tencent PCG</h3>
                    </div>
                    <p class="position">Intern</p>
                    <p class="date">2021.12 - 2022.8</p>
                    <p class="description">Responsible for 3D human reconstruction and motion generation project. Proposed an independent token representation method based on the parameterized SMPL model, achieving high-precision 3D human reconstruction and joint motion capture, improving 3DPW metrics by 8%. The paper was published in ICLR-2023 (spotlight, top25%).</p>
                </div>
                <div class="experience-card job-internship">
                    <div class="card-header">
                        <span class="icon"><i class="fas fa-lightbulb"></i></span>
                        <h3>Megvii Technology</h3>
                    </div>
                    <p class="position">Intern</p>
                    <p class="date">2021.1 - 2021.10</p>
                    <p class="description">Participated in human pose estimation projects. Designed a Transformer-based pose estimation model using token representation (ICCV-2021). Researched attention patterns in Transformer (Pattern Recognition). Pioneered a new coordinate classification paradigm, SimCC, breaking through the precision bottleneck of traditional regression and heatmap methods (ECCV 2022 Oral, adopted as a core solution by mainstream pose estimation frameworks).</p>
                </div>
            </div>
        </div>
    </section>

    <section id="education" class="education">
        <div class="container">
            <h2 class="section-title">Education Background</h2>
            <div class="education-timeline">
                <div class="timeline-event">
                    <div class="timeline-dot highlight"><i class="fas fa-graduation-cap"></i></div>
                    <div class="timeline-content">
                        <div class="degree">Bachelor</div>
                        <div class="school">Jilin University, College of Communication Engineering</div>
                        <div class="major">Automation</div>
                        <div class="date">2013.09 - 2017.07</div>
                    </div>
                </div>
                <div class="timeline-event">
                    <div class="timeline-dot"><i class="fas fa-graduation-cap"></i></div>
                    <div class="timeline-content">
                        <div class="degree">Master</div>
                        <div class="school">Southeast University, School of Automation</div>
                        <div class="major">Pattern Recognition and Intelligent Systems</div>
                        <div class="date">2017.09 - 2019.01</div>
                    </div>
                </div>
                <div class="timeline-event">
                    <div class="timeline-dot highlight"><i class="fas fa-graduation-cap"></i></div>
                    <div class="timeline-content">
                        <div class="degree">Ph.D.</div>
                        <div class="school">Southeast University, School of Automation</div>
                        <div class="major">Pattern Recognition and Intelligent Systems</div>
                        <div class="date">2019.03 - 2023.05</div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section id="publications" class="publications">
        <div class="container">
            <h2 class="section-title">Research Publications</h2>
            <div class="publications-filter">
                <button class="filter-btn active" data-filter="all">All</button>
                <button class="filter-btn" data-filter="pose">Pose Estimation</button>
                <button class="filter-btn" data-filter="autonomous">Autonomous Driving</button>
            </div>
            <div class="publications-grid">
                <!-- New Paper - HisTrackMap -->
                <div class="publication-card" data-category="autonomous">
                    <h3>HisTrackMap: Global Vectorized High-Definition Map Construction via History Map Tracking</h3>
                    <p class="authors">Jing Yang*, <strong>Sen Yang*</strong>, Xiao Tan, Hanli Wang.</p>
                    <p class="venue">arXiv preprint arXiv:2503.07168, 2025</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2503.07168" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                        <a href="https://yj772881654.github.io/HisTrackMap" class="btn" target="_blank" rel="noopener noreferrer">Project</a>
                    </div>
                </div>
                
                <!-- Paper 1 -->
                <div class="publication-card" data-category="autonomous">
                    <h3>TopoSD: Topology-Enhanced Lane Segment Perception with SDMap Prior</h3>
                    <p class="authors"><strong>Sen Yang</strong>, Minyue Jiang, Ziwei Fan, Xiaolu Xie, Xiao Tan, Yingying Li, Errui Ding, Liang Wang, Jingdong Wang.</p>
                    <p class="venue">2024 Preprint</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2411.14751" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                    </div>
                </div>
                
                <!-- Paper 2 -->
                <div class="publication-card" data-category="autonomous">
                    <h3>MGMapNet: Multi-Granularity Representation Learning for End-to-End Vectorized HD Map Construction</h3>
                    <p class="authors">Jing Yang*, Minyue Jiang*, <strong>Sen Yang*</strong>, Xiao Tan, Yingying Li, Errui Ding, Hanli Wang, Jingdong Wang.</p>
                    <p class="venue">ICLR 2025</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/pdf/2410.07733.pdf" class="btn" target="_blank" rel="noopener noreferrer">PDF</a>
                    </div>
                </div>
                
                <!-- Paper 3 -->
                <div class="publication-card" data-category="pose">
                    <h3>Detecting and grouping keypoints for multi-person pose estimation using instance-aware attention</h3>
                    <p class="authors"><strong>Sen Yang</strong>, Ze Feng, Zhicheng Wang, Yanjie Li, Shoukui Zhang, Zhibin Quan, Shu-tao Xia, Wankou Yang.</p>
                    <p class="venue">Pattern Recognition</p>
                    <div class="publication-links">
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322007117" class="btn" target="_blank" rel="noopener noreferrer">Journal Paper</a>
                    </div>
                </div>
                
                <div class="publication-card" data-category="pose">
                    <h3>Capturing the motion of every joint: 3D human pose and mesh recovery with independent tokens</h3>
                    <p class="authors"><strong>Sen Yang</strong>, Wen Heng, Gang Liu, Guozhong Luo, Wankou Yang, Gang Yu.</p>
                    <p class="venue">ICLR 2023 (spotlight, top 25%)</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2303.00298" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                        <a href="https://github.com/yangsenius/INT_HMR_Model" class="btn" target="_blank" rel="noopener noreferrer">Code</a>
                        <a href="https://yangsenius.github.io/INT_HMR_Model/" class="btn" target="_blank" rel="noopener noreferrer">Project</a>
                    </div>
                </div>
                
                <div class="publication-card" data-category="pose">
                    <h3>Searching part-specific neural fabrics for human pose estimation</h3>
                    <p class="authors"><strong>Sen Yang</strong>, Wankou Yang, Zhen Cui.</p>
                    <p class="venue">Pattern Recognition</p>
                    <div class="publication-links">
                        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001339" class="btn" target="_blank" rel="noopener noreferrer">Journal Paper</a>
                        <a href="https://github.com/yangsenius/ssa_pose" class="btn" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                </div>
                
                <div class="publication-card" data-category="pose">
                    <h3>SimCC: A Simple Coordinate Classification perspective for human pose estimation</h3>
                    <p class="authors">Yanjie Li, <strong>Sen Yang</strong>, Peidong Liu, Shoukui Zhang, Yunxiao Wang, Zhicheng Wang, Wankou Yang, Shu-Tao Xia.</p>
                    <p class="venue">ECCV 2022 (oral, top 5%) (cited 200+ times)</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2107.03332" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                        <a href="https://github.com/leeyegy/SimDR" class="btn" target="_blank" rel="noopener noreferrer">Code</a>
                        <a href="https://zhuanlan.zhihu.com/p/664996998" class="btn" target="_blank" rel="noopener noreferrer">Zhihu</a>
                    </div>
                </div>
                
                <div class="publication-card" data-category="pose">
                    <h3>TokenPose: Learning Keypoint Tokens for Human Pose Estimation</h3>
                    <p class="authors">Yanjie Li, Shoukui Zhang, Zhicheng Wang, <strong>Sen Yang</strong>, Wankou Yang, Shu-Tao Xia, Erjin Zhou.</p>
                    <p class="venue">ICCV 2021 (cited 400+ times)</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2104.03516" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                        <a href="https://github.com/leeyegy/TokenPose" class="btn" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                </div>
                
                <div class="publication-card" data-category="pose">
                    <h3>TransPose: Keypoint Localization via Transformer</h3>
                    <p class="authors"><strong>Sen Yang</strong>, Zhibin Quan, Mu Nie, Wankou Yang.</p>
                    <p class="venue">ICCV 2021 (cited 500+ times)</p>
                    <div class="publication-links">
                        <a href="https://arxiv.org/abs/2012.14214" class="btn" target="_blank" rel="noopener noreferrer">Paper</a>
                        <a href="https://github.com/yangsenius/TransPose" class="btn" target="_blank" rel="noopener noreferrer">Code</a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Professional Skills Section -->
    <section id="skills" class="skills">
        <div class="container">
            <h2 class="section-title">Professional Skills</h2>
            <div class="skills-content">
                <ul>
                    <li>Solid theoretical and practical experience in computer vision, with research and development experience in multiple sub-fields; focused on deep learning, Transformer models, and human pose estimation during Ph.D. studies; accumulated strong research and engineering practice experience in deep learning and large models in various work and internship projects.</li>
                    <li>Proficient in deep learning model design and optimization, integrating cross-domain innovative thinking to transform theoretical advantages into practical engineering applications, with strong code implementation capabilities.</li>
                    <li>Experienced in large enterprise project development and cross-departmental collaboration, emphasizing efficient communication and teamwork.</li>
                    <li>Proficient in mainstream development frameworks such as PyTorch, familiar with programming languages such as Python and C++, and skilled in Linux development.</li>
                    <li>Strong self-driven learning ability (Transfer & Meta Learning), passionate about cutting-edge technologies, constantly seeking new knowledge, proficient in learning and utilizing tools, and valuing efficiency.</li>
                </ul>
            </div>
        </div>
    </section>

    <section id="contact" class="contact">
        <div class="container">
            <h2 class="section-title">Contact Me</h2>
            <div class="contact-info">
                <div class="contact-item">
                    <i class="fas fa-envelope"></i>
                    <p>yangsenius@gmail.com</p>
                </div>
                <div class="contact-item">
                    <i class="fas fa-blog"></i>
                    <p><a href="https://senyang-ml.github.io" target="_blank" rel="noopener noreferrer">https://senyang-ml.github.io</a></p>
                </div>
            </div>
        </div>
    </section>

    <footer>
        <div class="container">
            <p>&copy; 2024 Sen Yang. All rights reserved.</p>
        </div>
    </footer>

    <script src="js/main.js"></script>
    <!-- Include markmap-autoloader -->
    <script src="https://cdn.jsdelivr.net/npm/markmap-autoloader"></script>
    <!-- Include Mermaid JS library -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        // Optional: Configure Mermaid theme, etc.
        mermaid.initialize({ startOnLoad: true, theme: 'default' });
    </script>

    <!-- Your publication filter script, unchanged -->
    <script>
        // Markmap drag range limit and center button logic have been optimized via Markmap library's fit() method and CSS styles, no extra JS control needed.
        // Ensure Markmap container and button styles are defined in CSS.
    </script>

    <script src="https://cdn.jsdelivr.net/npm/markmap-lib@0.17.0/dist/index.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.0/dist/index.min.js"></script>
    <script type="text/javascript">
        // 初始化思维导图
        window.initMarkmap = function() {
            const markmapDiv = document.querySelector('.markmap');
            if (!markmapDiv) return;

            // 检查库是否加载
            if (!window.markmap) {
                console.error("Markmap library not loaded");
                setTimeout(window.initMarkmap, 100);
                return;
            }

            try {
                const { Markmap, deriveOptions } = window.markmap;
                const pre = markmapDiv.querySelector('pre');
                const content = pre ? pre.textContent.trim() : markmapDiv.textContent.trim();

                // 如果内容为空，等待内容填充
                if (!content) {
                    console.log("Markmap content is empty, waiting for data...");
                    setTimeout(window.initMarkmap, 100);
                    return;
                }

                // 如果已经初始化过，先清空
                if (window.mm) {
                    markmapDiv.innerHTML = '';
                    const newPre = document.createElement('pre');
                    newPre.className = 'markmap-content';
                    newPre.textContent = content;
                    markmapDiv.appendChild(newPre);
                }

                const mm = Markmap.create(markmapDiv, {
                    zoom: false,
                    pan: false,
                    autoFit: true
                }, content);

                // 居中显示
                mm.fit();

                // 保存到全局变量
                window.mm = mm;
            } catch (e) {
                console.error("Failed to create mindmap:", e);
            }
        };

        // 页面加载完成后初始化
        window.onload = function() {
            // 初始化思维导图
            window.initMarkmap();

            // 添加重置按钮事件 - 直接刷新整个页面
            const resetBtn = document.getElementById('reset-view-btn');
            if (resetBtn) {
                resetBtn.addEventListener('click', function() {
                    // 添加旋转动画效果
                    const icon = this.querySelector('i');
                    if (icon) {
                        icon.style.transition = 'transform 0.5s ease';
                        icon.style.transform = 'rotate(360deg)';
                    }

                    // 刷新整个页面
                    window.location.reload();
                });
            }
        };
    </script>
</body>
</html> 