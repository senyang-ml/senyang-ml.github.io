<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Human Pose Estimation," />




  


  <link rel="alternate" href="/atom.xml" title="Sen Yang" type="application/atom+xml" />






<meta name="description" content="G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读 博客地址：https:&#x2F;&#x2F;yangsenius.github.io&#x2F;blog&#x2F;2-pifpaf&#x2F;  arxiv地址: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1903.06593github地址: https">
<meta property="og:type" content="article">
<meta property="og:title" content="G-RMI-&gt; PersonLab -&gt; PifPaf  -- Human Pose Estimation">
<meta property="og:url" content="http://senyang-ml.github.io/2019/07/17/pifpaf/index.html">
<meta property="og:site_name" content="Sen Yang">
<meta property="og:description" content="G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读 博客地址：https:&#x2F;&#x2F;yangsenius.github.io&#x2F;blog&#x2F;2-pifpaf&#x2F;  arxiv地址: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1903.06593github地址: https">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190322204206204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190322204225926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdn.net/20151229152931179">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190714160727112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190714162530980.png">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190704131509721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190704131528767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70">
<meta property="article:published_time" content="2019-07-17T03:35:08.000Z">
<meta property="article:modified_time" content="2020-03-13T10:58:00.000Z">
<meta property="article:author" content="yangsenius">
<meta property="article:tag" content="Human Pose Estimation">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img-blog.csdnimg.cn/20190322204206204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://senyang-ml.github.io/2019/07/17/pifpaf/"/>





  <title>G-RMI-> PersonLab -> PifPaf  -- Human Pose Estimation | Sen Yang</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sen Yang</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-blogs">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Blogs
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/research/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://senyang-ml.github.io/2019/07/17/pifpaf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="yangsenius">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sen Yang">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">G-RMI-> PersonLab -> PifPaf  -- Human Pose Estimation</h1>
        

        <div class="post-meta">
                  

          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posté le</span>
              
              <time title="Post aangemaakt" itemprop="dateCreated datePublished" datetime="2019-07-17T11:35:08+08:00">
                2019-07-17
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h3 id="G-RMI-gt-PersonLab-gt-PifPaf-Composite-Fields-for-Human-Pose-Estimation-CVPR-2019-论文解读"><a href="#G-RMI-gt-PersonLab-gt-PifPaf-Composite-Fields-for-Human-Pose-Estimation-CVPR-2019-论文解读" class="headerlink" title="G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读"></a>G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读</h3><blockquote>
<p>博客地址：<a href="https://yangsenius.github.io/blog/2-pifpaf/" target="_blank" rel="noopener">https://yangsenius.github.io/blog/2-pifpaf/</a></p>
</blockquote>
<p>arxiv地址: <a href="https://arxiv.org/abs/1903.06593" target="_blank" rel="noopener">https://arxiv.org/abs/1903.06593</a><br>github地址: <a href="https://github.com/vita-epfl/openpifpaf" target="_blank" rel="noopener">https://github.com/vita-epfl/openpifpaf</a></p>
<p>今年的CVPR19的论文最近已经在CVF Openaccess 网站上放出来了。</p>
<a id="more"></a>

<p>还记得去年18CVPR论文出来的时候，我把所有有关的人体姿态估计的论文的题目和概要大致都看了，得出的一个浅显的结论就是：3D姿态估计、密集姿态估计要流行起来了。这是因为在去年CVPR18的论文中，出现了大量的3D有关的论文而少有2D姿态估计研究（比如在MPII, COCO keypoint数据集上的方法挺少，可能2d姿态的都去发了ECCV18）。</p>
<p>而今年19CVPR的姿态估计好像又呈现出一次小爆发</p>
<p>COCO数据集上的性能又来到了一次新高：似乎74mAP已经被突破了（HRNet, 0.770 mAP, ECSI, 0.746 mAP）。。。</p>
<p>各位研究者们，是不是感觉到了精度上、性能上的压力。。。深度调参还是方法革新，这是个问题.</p>
<p>众多论文中，我先阅读了这篇，OpenPIFPAF。 因为它奇怪的名字好像是茫茫论文海中出现的那个与众不同的一篇，吸引我去一探Ta的全貌与究竟</p>
<p>读完后，我觉得OpenPifpaf继承了几篇姿态估计论文的工作：</p>
<ul>
<li>openpose</li>
<li>G-RMI</li>
<li>PersonLAB (应该说大部分核心的想法来自于PersonLab)</li>
</ul>
<p>并致力于解决几个棘手的问题：</p>
<ul>
<li>Bottom-up的多人姿态解析问题</li>
<li>自动驾驶中，图像中小尺寸人体的问题</li>
</ul>
<p>其实很有必要介绍一下先前的工作</p>
<h2 id="G-RMI"><a href="#G-RMI" class="headerlink" title="G-RMI"></a>G-RMI</h2><p>G-RMI 是google的一篇自上而下处理姿态估计问题的开篇</p>
<p>通过Faster-RCNN检测得到包含单个人体的bounding box，然后再进行单人姿态估计</p>
<p><img src="https://img-blog.csdnimg.cn/20190322204206204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>本论文在预测$K$个表示置信度的heatmaps之外，又引入了offset fields的方法，用$2\times K$个heatmaps表示，即每个heatmap的位置上预测一个$F_k(x_{i})=l_k-x_{i}$的位移偏量，用$l_k$来表示真实位置，其中$x_i,k \in \mathbb{Z}_+^2$ $i,k$表示位置索引和关键点类型。</p>
<p>$$<br>h_{k}\left(x_{i}\right)=1 \text { if }\left|x_{i}-l_{k}\right| \leq R<br>$$</p>
<p>$$<br>F_k(x_{i})=l_k-x_{i}<br>$$<br>After generating the heatmaps and offsets, we aggregate them to produce highly localized activation maps $f_{k}\left(x_{i}\right)$ as follows:<br>$$<br>f_{k}\left(x_{i}\right)=\sum_{j} \frac{1}{\pi R^{2}} G\left(x_{j}+F_{k}\left(x_{j}\right)-x_{i}\right) h_{k}\left(x_{j}\right)<br>$$</p>
<p>其中第三个公式中的$G(\cdot)$论文中说它是双线性插值核，并用霍夫投票的形式。在今年19CVPR的openpifpaf论文，又再次利用这个公式，不过用一个高斯核来代替了$G(\cdot)$函数，我从中推断出这是起到了平滑取值的作用，就像我们在构造产生grountruth heatmaps那样的做法。下面的$\pi R^{2}​$是一个归一化，和高斯核那样类似。</p>
<p>注：这个$G(\cdot)$函数其实是很多人理解这篇论文的绊脚石。实际上,这就是对上采样后的heatmaps再次进行一次平滑.</p>
<blockquote>
<p>A different approach<br>addressing this issue would be to predict activation maps,as in [27], which allow for multiple predictions of the same keypoint. However, the size of the activation maps, and thus<br>the localization precision, is limited by the size of the net’s output feature maps, which is a fraction of the input imagesize, due to the use of max-pooling with decimation.In order to address the above limitations, we adopt acombined classification and regression approach. For each<br>spatial position, we first classify whether it is in the vicin-ity of each of the K keypoints or not (which we call a “heatmap”), then predict a 2-D local offset vector to get amore precise estimate of the corresponding keypoint loca-tion. Note that this approach is inspired by work on object detection, where a similar setup is used to predict bounding boxes, e.g. [14, 37]. Figure 2 illustrates these three output channels per keypoint.<br><img src="https://img-blog.csdnimg.cn/20190322204225926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L011cmRvY2tfQw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
</blockquote>
<p>训练loss是：</p>
<p>$$<br>L(\theta)=\lambda_{h} L_{h}(\theta)+\lambda_{o} L_{o}(\theta)<br>$$</p>
<p>$\lambda_{h}=4$ and $\lambda_{o}=1$ is a scalar factor to balance.</p>
<p>We use a single ResNet model with two convolutional output heads. The output of the firshead passes through a sigmoid function to yield the heatmap probabilities $h_{k}\left(x_{i}\right)$ for each position $x_{i}$ and each keypoint $k$ . The training target $\overline{h}<em>{k}\left(x</em>{i}\right)$  is a map of zeros and ones, with $\overline{h}<em>{k}\left(x</em>{i}\right)=1$  if $\left|x_{i}-l_{k}\right| \leq R$ and 0 otherwise. The corresponding loss function $L_{h}(\theta)$ is the sum of logistic losses for each position and keypoint separately. </p>
<p>$$<br>L_{o}(\theta)=\sum_{k=1 : K} \sum_{i :\left|l_{k}-x_{i}\right| \leq R} H\left(\left|F_{k}\left(x_{i}\right)-\left(l_{k}-x_{i}\right)\right|\right)<br>$$</p>
<p>where $H(u)$ is the Huber robust loss, $l_{k}$ is the position of the $k$ -th keypoint, and we only compute the loss for positions $x_{i}$ within a disk of radius $R$ from each keypoint.</p>
<p>Huber robust loss的函数图像为：</p>
<p><img src="https://img-blog.csdn.net/20151229152931179" alt="在这里插入图片描述"></p>
<h2 id="G-RMI的开创行思路-keypoints-location-disk-mask-logistic-classication-and-short-range-offset解决了下采样导致对量化误差问题"><a href="#G-RMI的开创行思路-keypoints-location-disk-mask-logistic-classication-and-short-range-offset解决了下采样导致对量化误差问题" class="headerlink" title="G-RMI的开创行思路:keypoints location disk mask logistic classication and short-range offset解决了下采样导致对量化误差问题!!"></a>G-RMI的开创行思路:keypoints location disk mask logistic classication and short-range offset解决了下采样导致对量化误差问题!!</h2><p><strong>作者构造出<code>0,1</code>取值构成的<code>kepoints location  masks heatmaps</code>和<code>某关键点对应对heatmap的每个grid位置相对于其真实位置的偏移</code>的<code>分类</code>加<code>回归</code>预测方法!!</strong></p>
<p>这一点<code>PersonLab</code>和<code>PifPaf</code>都沿袭了这一思路</p>
<p>而<code>PersonLab</code>在此基础上,为了解决多人关联肢体的算法设计问题,又继续引入了<code>mid-range pairwise offset</code>来针对<code>instance association</code>这一问题, 可以说将将<code>G-RMI</code>的方法拓展到多人问题上.</p>
<h1 id="PersonLab"><a href="#PersonLab" class="headerlink" title="PersonLab"></a>PersonLab</h1><p>PersonLab在构造监督标签和网络预测表示上面下了不少功夫。<br>公式总览：</p>
<p>$\textbf{K heatmaps: } \qquad\qquad\qquad\qquad\qquad p_{k}(x)=1 \text { if } x \in \mathcal{D}<em>{R}\left(y</em>{j, k}\right), \mathcal{D}_{R}(y)={x :|x -y| \leq R}$</p>
<p>$\textbf{K short-range 2-D offset fields:  } \qquad S_{k}(x)=y_{j, k}-x$</p>
<p>$\textbf{K Hough score maps:  }\qquad \qquad \qquad h_{k}(x)=\frac{1}{\pi R^{2}} \sum_{i=1 : N} p_{k}\left(x_{i}\right) B\left(x_{i}+S_{k}\left(x_{i}\right)-x\right)$</p>
<p>$\textbf{2(K-1) mid-range 2-D offset fields:  }\quad M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}<em>{R}\left(y</em>{j, k}\right)\right], \text{from k-th to l-th}$</p>
<p>$\textbf{K long-range 2-D offset fields:  }\qquad \qquad L_{k}(x)=y_{j, k}-x$</p>
<p><img src="https://img-blog.csdnimg.cn/20190714160727112.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">)在Personlab中,$h_k(x)$,即hough投票后对高精度score maps并不提供实例相关的信息, 而需要一种机制来<code>group together the keypoints belonging to each individual instance</code>. 因此, 作者继续构造了<code>mid-range pairwise offeset</code> 负责<code>connect the dots</code>.  </p>
<p><code>mid-range pairwise offeset</code> 也是 2D的offset fields:  $M_{k, l}(x)$</p>
<blockquote>
<p>We compute 2$(K-1)$  such offset fields, one for each directededge connecting pairs $(k, l)$ of keypoints which are adjacent to each other in a tree-structured kinematic graph of the person, see Figs<br><img src="https://img-blog.csdnimg.cn/20190714162530980.png" alt="在这里插入图片描述"></p>
</blockquote>
<p>这里的2$(K-1)=2\times16$ 个向量场指的是 上图16种肢体连接的正反2种方向:$(k,l)和(l,k)$的偏移向量场 $M_{k, l}(x)=\left(y_{j, l}-x\right)\left[x \in \mathcal{D}<em>{R}\left(y</em>{j, k}\right)\right]$ , 这个向量场还是在disk对半径范围内的. </p>
<p><code>Recurrent offset refinement</code>： 作者在预测一些大尺寸人体时,有时候mid-range pairwise offsets很长,精度可能不准,所以用了<code>Recurrent offset refinement</code>:</p>
<p>$$<br>M_{k, l}(x) \leftarrow x^{\prime}+S_{l}\left(x^{\prime}\right), \text { where } x^{\prime}=M_{k, l}(x),<br>$$<br>来进一步提高精度,迭代2次上述对公式. 其中,$S_{l}\left(x^{\prime}\right)$是short-range offset.</p>
<h3 id="Fast-greedy-decoding"><a href="#Fast-greedy-decoding" class="headerlink" title="Fast greedy decoding"></a>Fast greedy decoding</h3><p>有了所有人体的关键点的预测位置和每个关键点的<code>mid-range pairwise offset</code>, 接下来要做的就是 进行将属于同一个人体的关键点组合成一个实例的机制。</p>
<p><code>PersonLab</code> 构造出一个<code>优先级的队列</code>，根据Hough score maps  $h_{k}(x)$上的<code>局部最大值</code>的位置（这里强调局部最大值，是因为可能会有false positive 的位置）以及其score高于一定的阈值的（实验取0.01） 来<code>按得分大小顺序放入队列</code>，这些放入队列中的关键点应该被称为了<code>seed</code>点（pifpaf实际上没有解释清楚这个seed点），从最高响应值的seed点位置开始，以此不断找到其在<code>tree-structure</code>上的连接关键点。</p>
<p>在这个算法迭代的每一步中，如果发现当前的<code>seed</code>关键点落入了已经关联到先前某个人体$j^{\prime},$的某个$k$类型对seed关键点的 $\mathcal{D}<em>{r}\left(y</em>{j^{\prime}, k}\right)$ 半径内，那么就意味着，这个半径区域内有可能有两个同样类型的关键点，那么我们就可以开辟一个新的人体实例$j$,作为另外一个人体的$k$类型关键点作为<code>seed</code>. 其中,通过<code>seed</code>点计算与其<code>adjacent</code>的关键点的公式是:$y_{j, l}=y_{j, k}+M_{k, l}\left(y_{j, k}\right)$.</p>
<p>这种机制对于所有关键点的是公平的,即 根据高得分的关键点位置作为起始seed,(我想起了Associative Embedding 对待起始点是从头部开始), 然而实际中容易检测的点往往是起始位置点, 这种方法能够一定程度处理遮挡问题. </p>
<blockquote>
<p>the position $x_{i}$ of the current candidate detection seed of type $k$ is within a disk $\mathcal{D}<em>{r}\left(y</em>{j^{\prime}, k}\right)$ of the corresponding keypoint of previously detected person instances $j^{\prime},$ then we reject it;</p>
</blockquote>
<p>Personlab的decoding 大致代码如下:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"> config.EDGES = [</span><br><span class="line">        (<span class="number">0</span>, <span class="number">14</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">13</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">4</span>),</span><br><span class="line">        (<span class="number">0</span>, <span class="number">1</span>),</span><br><span class="line">        (<span class="number">14</span>, <span class="number">16</span>),</span><br><span class="line">        (<span class="number">13</span>, <span class="number">15</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">10</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">7</span>),</span><br><span class="line">        (<span class="number">10</span>, <span class="number">11</span>),</span><br><span class="line">        (<span class="number">7</span>, <span class="number">8</span>),</span><br><span class="line">        (<span class="number">11</span>, <span class="number">12</span>),</span><br><span class="line">        (<span class="number">8</span>, <span class="number">9</span>),</span><br><span class="line">        (<span class="number">4</span>, <span class="number">5</span>),</span><br><span class="line">        (<span class="number">1</span>, <span class="number">2</span>),</span><br><span class="line">        (<span class="number">5</span>, <span class="number">6</span>),</span><br><span class="line">        (<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">    ]</span><br><span class="line">    </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">group_skeletons</span><span class="params">(keypoints, mid_offsets)</span>:</span></span><br><span class="line">    <span class="comment"># keypoints 是hough score maps 所有局部最大值位置产生对所有候选关键点  </span></span><br><span class="line">    <span class="comment"># keypoints 数组每个元素为&#123;'xy':[x,y],'id':k,'conf':score&#125;</span></span><br><span class="line">    keypoints.sort(key=(<span class="keyword">lambda</span> kp: kp[<span class="string">'conf'</span>]), reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 按照从大到小排序</span></span><br><span class="line">    skeletons = []</span><br><span class="line">    <span class="comment"># skeletons 表示多个人体骨架,每个元素是单人的骨架坐标集合</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># config.EDGES 表示16个单向的骨架连接边,dir_edges 构造出双向的连接边</span></span><br><span class="line">    dir_edges = config.EDGES + [edge[::<span class="number">-1</span>] <span class="keyword">for</span> edge <span class="keyword">in</span> config.EDGES]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 为每个关键点生成跟它相连的关键点的集合,比如 左肘部有[左肩膀,左手腕]</span></span><br><span class="line">    skeleton_graph = &#123;i:[] <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP)&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(config.NUM_KP):</span><br><span class="line">            <span class="keyword">if</span> (i,j) <span class="keyword">in</span> config.EDGES <span class="keyword">or</span> (j,i) <span class="keyword">in</span> config.EDGES:</span><br><span class="line">                skeleton_graph[i].append(j)</span><br><span class="line">                skeleton_graph[j].append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 优先级队列</span></span><br><span class="line">    <span class="keyword">while</span> len(keypoints) &gt; <span class="number">0</span>:</span><br><span class="line">    	<span class="comment"># pop出最大置信度的候选关键点</span></span><br><span class="line">        kp = keypoints.pop(<span class="number">0</span>)</span><br><span class="line">		<span class="comment"># 判断该类型(根据id)关键点有没有落入了之前的某个骨架的同类关键点上</span></span><br><span class="line">		<span class="comment"># 计算距离是否在r=10(论文里面提到)内</span></span><br><span class="line">        <span class="keyword">if</span> any([np.linalg.norm(kp[<span class="string">'xy'</span>]-s[kp[<span class="string">'id'</span>], :<span class="number">2</span>]) &lt;= <span class="number">10</span> <span class="keyword">for</span> s <span class="keyword">in</span> skeletons]):</span><br><span class="line">            <span class="comment"># 如果是,则抑制该关键点(非极大值抑制)</span></span><br><span class="line">            <span class="comment">#　如果否，则认为该类型关键点属于另外一个人体，那么就开辟新的实例</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        <span class="comment">#　构造新的骨架</span></span><br><span class="line">        this_skel = np.zeros((config.NUM_KP, <span class="number">3</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将该类型关键点的坐标和置信度赋值给该骨架</span></span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], :<span class="number">2</span>] = kp[<span class="string">'xy'</span>]</span><br><span class="line">        this_skel[kp[<span class="string">'id'</span>], <span class="number">2</span>] = kp[<span class="string">'conf'</span>]</span><br><span class="line">        </span><br><span class="line">		<span class="comment"># 此处 引入该函数</span></span><br><span class="line">	    <span class="comment">###########################</span></span><br><span class="line">		<span class="function"><span class="keyword">def</span> <span class="title">iterative_bfs</span><span class="params">(graph, start, path=[])</span>:</span></span><br><span class="line">    		<span class="string">'''iterative breadth first search from start'''</span></span><br><span class="line">    		<span class="comment"># 此处的graph是所有的每个关键点与它相连的关键点的构成的多叉树结构</span></span><br><span class="line">    		<span class="comment"># start 表示关键点的类型</span></span><br><span class="line">    		<span class="comment"># 构造队列</span></span><br><span class="line">    		q=[(<span class="literal">None</span>,start)]</span><br><span class="line">    		</span><br><span class="line">		    visited = []</span><br><span class="line">		    <span class="keyword">while</span> q:</span><br><span class="line">		        <span class="comment"># 拿出第一个元素v (关键点l（子点）,关键点k（父点）) 初始 关键点l=None</span></span><br><span class="line">		        v=q.pop(<span class="number">0</span>)</span><br><span class="line">		        <span class="comment"># 如果其对应的关键点k之前没访问过</span></span><br><span class="line">		        <span class="keyword">if</span> <span class="keyword">not</span> v[<span class="number">1</span>] <span class="keyword">in</span> visited:</span><br><span class="line">		            <span class="comment"># 记录访问</span></span><br><span class="line">		            visited.append(v[<span class="number">1</span>])</span><br><span class="line">		            <span class="comment"># 记录路径</span></span><br><span class="line">		            path=path+[v]</span><br><span class="line">		            <span class="comment"># 将在图结构graph中,找到关键点k直接相连的所有子关键点k_1,k_2,...</span></span><br><span class="line">		            </span><br><span class="line">		            <span class="comment"># 放到队列中[(关键点l_1关键点k_1),(关键点l_2,关键点k_2),...]</span></span><br><span class="line">		            q=q+[(v[<span class="number">1</span>], w) <span class="keyword">for</span> w <span class="keyword">in</span> graph[v[<span class="number">1</span>]]]</span><br><span class="line">		        	<span class="comment">#  然后子点变成父点，下一次迭代生出更多的子点</span></span><br><span class="line">		        <span class="comment"># 循环,这样的话,不论初始点是什么,我们都可以找到一个包含完整人体树结构tree-structure的路径,</span></span><br><span class="line">		        <span class="comment"># 包含所有 [(None,关键点k),(关键点k,关键点k_1),(关键点1_x,关键点k_2),...,(关键点16_x,关键点16)] 一共会有17个path, 然而就会多余出1个连接,就是初始连接.</span></span><br><span class="line">		    <span class="keyword">return</span> path</span><br><span class="line">		    <span class="comment">#假如是Lwrist 为起始点的话: path=[('Lwrist', 'Lelbow'), ('Lelbow', 'Lshoulder'), ('Lshoulder', 'nose'), ('Lshoulder', 'Lhip'), ('nose', 'Rshoulder'), ('nose', 'Reye'), ('nose', 'Leye'), ('Lhip', 'Lknee'), ('Rshoulder', 'Relbow'), ('Rshoulder', 'Rhip'), ('Reye', 'Rear'), ('Leye', 'Lear'), ('Lknee', 'Lankle'), ('Relbow', 'Rwrist'), ('Rhip', 'Rknee'), ('Rknee', 'Rankle')]</span></span><br><span class="line">		<span class="comment">###########################</span></span><br><span class="line">		</span><br><span class="line">		<span class="comment"># 此处对skeleton_graph是每个关键点与它相连的关键点的图结构</span></span><br><span class="line">        path = iterative_bfs(skeleton_graph, kp[<span class="string">'id'</span>])[<span class="number">1</span>:] <span class="comment"># 此处1:开始,即去掉多余的连接</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> path:</span><br><span class="line">            <span class="comment"># 判断第一条边的起始关键点的置信度是不是为0, 因为已经this_skel[kp['id'], 2] = kp['conf'], 所以第一次肯定不为0;但是如果第一次之后没有找到新的关键点的话,那么以后的循环都要continue</span></span><br><span class="line">            <span class="keyword">if</span> this_skel[edge[<span class="number">0</span>],<span class="number">2</span>] == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># dir——edges 索引0-31</span></span><br><span class="line">            mid_idx = dir_edges.index(edge)</span><br><span class="line">            <span class="comment"># mid_offsets shape=[h,w,32x2] 输出feature map的每个位置</span></span><br><span class="line">            offsets = mid_offsets[:,:,<span class="number">2</span>*mid_idx:<span class="number">2</span>*mid_idx+<span class="number">2</span>]</span><br><span class="line">            <span class="comment"># 计算当前给定的关键点基于grid的位置</span></span><br><span class="line">            from_kp = tuple(np.round(this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>]).astype(<span class="string">'int32'</span>))</span><br><span class="line">            <span class="comment"># 计算当前关键点的位置，加上，此位置针对该类型有向连接的预测的偏移x,y向量，得到的候选位置，</span></span><br><span class="line">            <span class="comment"># 比如我们当前的关键点类型为左肘部，有向连接为（左肘，左手腕），那么根据该位置的mid-offset预测的左手腕的位置就知道了，但是这是根据mid-offset预测得到的位置，如果我们在hough score maps上也同样在该附近位置预测到了左手腕的位置，那么就说明mid-offset的预测也是合理的。</span></span><br><span class="line">            proposal = this_skel[edge[<span class="number">0</span>],:<span class="number">2</span>] + offsets[from_kp[<span class="number">1</span>], from_kp[<span class="number">0</span>], :]</span><br><span class="line">			<span class="comment"># 所以接下的matches，是在优先级队列中找到候选的所有左手腕（假设）的关键点，并记录其在优先级队列中的位置i。（找到最佳匹配点后，需要把它从队列中pop出）</span></span><br><span class="line">            matches = [(i, keypoints[i]) <span class="keyword">for</span> i <span class="keyword">in</span> range(len(keypoints)) <span class="keyword">if</span> keypoints[i][<span class="string">'id'</span>] == edge[<span class="number">1</span>]] <span class="comment"># edge[1]表示有向线段的末端点（左手腕）</span></span><br><span class="line">			<span class="comment"># 通过计算mid-offset预测出的有向线段末端点位置，与score maps 上该类型的位置的距离，我们可以得到很有可能的匹配关键点（&lt;32）</span></span><br><span class="line">            matches = [match <span class="keyword">for</span> match <span class="keyword">in</span> matches <span class="keyword">if</span> np.linalg.norm(proposal-match[<span class="number">1</span>][<span class="string">'xy'</span>]) &lt;= <span class="number">32</span>]</span><br><span class="line">            <span class="comment"># 找不到匹配点的话，就队列中的下一个关键点</span></span><br><span class="line">            <span class="keyword">if</span> len(matches) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="comment"># 根据匹配到的该类型关键点与预测的距离进行排序，距离越小，排序靠前</span></span><br><span class="line">            matches.sort(key=<span class="keyword">lambda</span> m: np.linalg.norm(m[<span class="number">1</span>][<span class="string">'xy'</span>]-proposal))</span><br><span class="line">			<span class="comment"># 排在最前面的关键点，即matches[0]，的坐标作为grid上的位置</span></span><br><span class="line">            to_kp = np.round(matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'xy'</span>]).astype(<span class="string">'int32'</span>)</span><br><span class="line">            <span class="comment"># 记录其confidence</span></span><br><span class="line">            to_kp_conf = matches[<span class="number">0</span>][<span class="number">1</span>][<span class="string">'conf'</span>]</span><br><span class="line">            <span class="comment"># 根据其在队列中的位置，将其pop出来</span></span><br><span class="line">            keypoints.pop(matches[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 把该匹配到的点的位置记录到骨架该处有向线段的末端位置上</span></span><br><span class="line">            </span><br><span class="line">            this_skel[edge[<span class="number">1</span>],:<span class="number">2</span>] = to_kp</span><br><span class="line">            this_skel[edge[<span class="number">1</span>], <span class="number">2</span>] = to_kp_conf</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 此处我们进行path路径上的顺寻，进入下一个有向连接，</span></span><br><span class="line">            <span class="comment"># 这里的path = iterative_bfs(skeleton_graph, kp['id']) 函数是个非常巧妙的扩散方式，</span></span><br><span class="line">            <span class="comment">#它从骨架上的任意一点出发，按照固定的顺序散播到骨架上的所有16个有向连接上（上游点（父），下游点（子））。</span></span><br><span class="line">            <span class="comment">#那么根据起始的任意一种人体关键点，这一个算法就可以在优先级队列中将很有可能属于该人体的所有关键点的候选点group到该人体上。</span></span><br><span class="line">			<span class="comment"># 但是,如果考虑到有一种枢纽的关键点，没有找到合适的点，那么不完整的人体骨架将会产生，</span></span><br><span class="line">            <span class="comment"># 但是在队列后面属于人体的关键点还会产生一个候选骨架，那么非极大值抑制skeleton就是必要的一项了</span></span><br><span class="line">        skeletons.append(this_skel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> skeletons</span><br><span class="line"></span><br><span class="line"> <span class="comment">## https://github.com/octiapp/KerasPersonLab/blob/master/post_proc.py</span></span><br><span class="line"> <span class="comment">## https://github.com/senyang-ml/OKS-NMS</span></span><br></pre></td></tr></table></figure>
<h1 id="OpenPIFPAF"><a href="#OpenPIFPAF" class="headerlink" title="OpenPIFPAF"></a>OpenPIFPAF</h1><p>在G-RMI、PersonLab的基础上，引入了PAF和PIF 复合结构，实际上具备显式含义的向量场。</p>
<p>即在图像每个location的像素位置，寄托更多的复合含义，编码具有直观含义的向量</p>
<p>PIF针对每一种类型的关键点，PAF针对每一种关联肢体（两个有关part的连接连线）</p>
<p>对于COCO，有17个关键点，19个连接（论文默认设置）</p>
<p><strong>PIF和PAF是训练Encoder网络用的监督标签，如何构造这两种标签，来指导监督Encoder网络训练，是本论文很关键的部分，后面的decoder 部分完全依赖于PIF和PAF的预测值。本文的PIF和PAF设计，可谓是将人工先验知识发挥到了极致！</strong></p>
<h2 id="PIF"><a href="#PIF" class="headerlink" title="PIF"></a>PIF</h2><p>PIF是个$K\times H \times W \times 5$的结构， K表示关键点的数量，COCO为17个</p>
<p> They are composed of a scalar component for confidence, a vector component that points to the closest body part of the particular type and another scalar component for the size of the joint. More formally, at every output location spread $b​$ (details in Section 3.4$)​$ and a scale $\sigma​$ and can be written as</p>
<p>$$<br>\mathbf{p}^{i j}=\left{p_{c}^{i j}, p_{x}^{i j}, p_{y}^{i j}, p_{b}^{i j}, p_{\sigma}^{i j}\right}​<br>$$</p>
<p>因为作者主要针对小尺寸人体图片，那么得到的置信度图 confidence map 是非常粗糙的，为了进一步地提升confidence map 的定位精度，作者使用偏量位移maps 来提升confidence map 的分辨率，得到一个<code>高分辨率的confidence map</code>（这个高分辨率的置信图发挥着<code>产生姿态种子点</code>和<code>评价候选点得分</code>的作用），如下公式：<br>$$<br>f(x, y)=\sum_{i j} p_{c}^{i j} \mathcal{N}\left(x, y | p_{x}^{i j}, p_{y}^{i j}, p_{\sigma}^{i j}\right)<br>$$<br>这个公式我发现，很大程度上借鉴了G-RMI中的上述公式。用一个未归一化的高斯核，以及可学习的范围因子$\sigma$来代替G-RMI中的双线性插值核以及归一化的分母，通过上述公式计算一个高分辨率的图（这里的高分辨率尺寸应该是原图尺寸，因为关键点的坐标标签真实值是基于原图的像素大小等级的）的响应值，我个人理解为是一种利用预测值的高斯上采样插值法（$p_{x}^{i j}, p_{y}^{i j}$是预测出的小尺寸置信图每个位置$(i,j)$基于其自身grid位置$(i,j)$的偏移量，$p_{\sigma}^{i j}$应该是高分辨图中每个位置的得分受到周围多大范围的预测值的影响，这部分应关注源码）。</p>
<p>这么做的缘故是，我认为是，<code>想保证不论在何种尺寸（量化等级下）都能克服量化误差的影响，因为heatmap是基于grid的，离散的取值，而真实的位置是不基于grid，并且是连续的位置，我通过预测真实位置与grid位置的偏移、以及grid上的置信度，就能进而获知真实的精确位置</code>。（我个人理解这样的好处就是，定位精度是float级别的，而不是int级别的，这个实际上在小尺寸的图像上是非常重要的一种策略。这种思想源自于G-RMI, 我认为这是一个解决量化误差问题的非常好的方式, 像SimpleBaseline,CPN运用取1/4偏移的方式,是一种人为的假设.）</p>
<p><img src="https://img-blog.csdnimg.cn/20190704131509721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="PAF"><a href="#PAF" class="headerlink" title="PAF"></a>PAF</h2><p>PAF是个$N\times H \times W \times 7$的结构， N表示关联肢体的数量，默认为19，</p>
<p>作者使用的bottom-up的方法，必然要解决：关联检测的关键点的位置形成隶属的人体的这一问题，就必须用一定的表示手段和策略来实现。</p>
<p>作者提出了PAF，来将关键点连接一起形成姿态。</p>
<p>在输出的每个位置，PAFs预测一个置信度、两个分别指向关联一起的两个part的向量、两个宽度。用下面来表示：<br>$$<br>\mathbf{a}^{i j}=\left{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right}<br>$$</p>
<p>作者接下来说了这样一句话，</p>
<blockquote>
<p>Both endpoints are localized with regressions that do not suffer from discretizations as they occur in grid-<br>based methods. This helps to resolve joint locations of close-by persons precisely and to resolve them into<br>distinct annotations。 我目前的理解是，两个端点定位的回归，不再受困于 grid-based方法中出现的离散化问题！这就帮助对于离得很近的关键点精确位置，并区分它们的标注。</p>
</blockquote>
<p>在COCO数据集，一共有19个连接关联两种类型的关键点。算法在每个feature map的位置，构造PAFs成分时，采用了两步：</p>
<p>首先，找到关联的两个关键点中最近的那一个的位置，来决定其向量成分中的一个。</p>
<p>然后，groundtruth pose决定了另外一个向量成分。第二个点不必是最近的，也可以是很远的。</p>
<blockquote>
<p>一开始我没有，怎么理解这么做的含义。后来意识到，这样就相当于，对于每一种类型的关联肢体，比如左肩膀和左屁股连接。对应的PAF中，每个位置都会优先确定理它最近的关键点的位置（考虑多个人体的情况下），然后指向另外一端的向量就自然得到了。</p>
</blockquote>
<p>并且在训练的时候，向量成分所指向的parts对必须是相关联的，每个向量的x，y方向必须指向同一个关键点的。</p>
<p><img src="https://img-blog.csdnimg.cn/20190704131528767.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3Nlbml1cw==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<h2 id="Adaptive-Regression-Loss"><a href="#Adaptive-Regression-Loss" class="headerlink" title="Adaptive Regression Loss"></a>Adaptive Regression Loss</h2><p>定位偏差可能对于大尺寸人体来讲，是小的影响，但是对于小尺寸人体，这个偏差就会成为主要的问题。本研究通过引入尺度依赖到$L_1 - type​$的loss函数里，</p>
<h2 id="Greedy-Decoding"><a href="#Greedy-Decoding" class="headerlink" title="Greedy Decoding"></a>Greedy Decoding</h2><p>通过PIF和PAF来得到poses。这个快速贪心的算法过程和PersonLab中的相似。</p>
<p>一个姿态由一个种子点(高分辨率PIF的最高响应位置)开始，一旦一个关键点的估计完成，决策就是最终不变的了。（贪心）</p>
<p>A new pose is seeded by PIF vectors with the highest values in the high resolution confidence map $f(x, y)$ defined in equation $1 .$ Starting from a seed, connections to other joints are added with the help of PAF fields. The algorithm is fast and greedy. Once a connection to a new joint has been made, this decision is final.</p>
<p>Multiple PAF associations can form connections between the current and the next joint. Given the loca-<br>tion of a starting joint $\vec{x},$ the scores $s$ of PAF associations a are calculated with</p>
<p>$$<br>s(\mathbf{a}, \vec{x})=a_{c} \quad \exp \left(-\frac{\left|\vec{x}-\vec{a}<em>{1}\right|</em>{2}}{b_{1}}\right) f_{2}\left(a_{x 2}, a_{y 2}\right)<br>$$</p>
<p>这个$s(\mathbf{a},\vec{x})$表示每个location属于part association的得分，得分越高，代表这个更有可能是part association区域部分那么,如果$s(\mathbf{a},\vec{x})$越大,那么就期望$a_c$越大,$\left(-\frac{\left|\vec{x}-\vec{a}<em>{1}\right|</em>{2}}{b_{1}}\right)$越大,$\frac{\left|\vec{x}-\vec{a}<em>{1}\right|</em>{2}}{b_{1}}$越小,那么就期望PAF某位置的$\mathbf{a}$ 对应的$\mathbf{a}=\left{a_{c}^{i j}, a_{x 1}^{i j}, a_{y 1}^{i j}, a_{b 1}^{i j}, a_{x 2}^{i j}, a_{y 2}^{i j}, a_{b 2}^{i j}\right}$向量中, 其指向的端点1和当前种子点距离最近, 并且期望该位置指向的另外一个端点2的置信度响应高, 这些期望和该位置是属于这两个关键点(端点)连接肢体的期望是一致的. 一旦我们的初始种子点确立后,我们就可以根据预测的PAF找到其关联的肢体区域和另外一个关键点位置,作为下一次的寻找的种子点.然后,重复这个过程,直到该种子点对应的人体全部找到.(这实际运用了人体躯干的连通性的潜在知识). 作者提倒:  </p>
<blockquote>
<p><code>To confirm the proposed position of the new joint, we run reverse matching. This process is repeated until a full pose is obtained. We apply non-maximum suppression at the keypoint level as in [34]. The suppression radius is dynamic and based on the predicted scale component ofthe PIF field. We do not refine any fields neither during
training nor test time.</code></p>
</blockquote>
<p>这个设计是巧妙的,<strong>因为我们在构造PAF的时候,请注意到,$(a_{x1},a_{y1})​$ 是PAF输出map的某位置$\mathbf{a}$最近的关键点的位置（请看Figure 4b），以此来判断离该位置$\mathbf{a}$最近的关键点是不是$\vec{x}$</strong>。如果当前$\vec{x}​$和$(a_{x1},a_{y1})​$的距离就可以作为当前位置是不是指向$\vec{x}​$的判断,因为如果两点重合的话,距离为0,指数取值为最大值1. 并且该位置对应的另外一个端点的取值具有高响应, 那么这就意味着:</p>
<p><strong>$s(\mathbf{a}, \vec{x})$的髙得分位置,很有可能处在指向$\vec{x}$端点的肢体关联部分的区域！</strong></p>
<p><strong>换句话说：</strong></p>
<p>$PIF$是计算得到的<code>高分辨率置信度图</code>负责提供候选的关键点。$s(\mathbf{a}, \vec{x})$得分公式，利用$PAF$预测值计算在其输出feature map每一个位置的得分，来判断两种关键点之间的连接（如左肘部和左手腕），因为涉及到多人，（参考OpenPose，对于单个人体的单个肢体连接，只有一种连接是合理的），论文提到的To confirm the proposed position of the new joint, we run reverse matching，我认为就是来确定某人体的某个肢体连接的唯一性、合理性的手段，具体还是要看源码。</p>
<p><del>找到$(a_{x2},a_{y2})$的位置(通过髙响应$s(\mathbf{a}, \vec{x})$)的位置?还是通过PIF,PAF的预测值得到?这个目前有待考证，我在后面会阅读实现源码,继续更新博客)</del>，<code>高分辨率置信度图</code>负责提供候选的关键点的位置。</p>
<p>那么，通过这样的一个贪心的快速算法, 我们根据初始的某个关键点就能同时确立多个人体位置,</p>
<!-- 
## 占位符
。。。
。。。
## 占位符
。。。
。。。 -->
<h4 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h4><p>注：可以看出这一系列的论文（GRMI，PersonLab，Openpifpaf，part-based）相比与针对网络结构进行改进（SimpleBaseline，HRNET）的文章看，更加关注几何关系上的问题以及网络的输出表示形式。PersonLab，Openpifpaf面对更加有挑战性的BBOX-FREE方法，以及小尺寸，遮挡问题进行处理，确确实实能给人持续往下深入的启示和实际应用的潜力。针对改网络结构的文章，譬如HRNET，SEU-POSE，SIMPLEBASELINE，CPN等等，致力于寻找最有的卷积结构设计，而不怎么关注一些棘手的问题（用模型本身的能力来克服），为姿态估计行业引领性能的标准，并不断去探索神经网络结构可能发挥的极限。前者更适合去研究新方法，突破现有检测器约束的姿态估计框架，去挑战多人姿态估计的难题，后者给我们提供了，固有框架内可以进一步提升性能的很多实用的经验和技巧，让我们更加洞察神经网络的结构的特性，并充分利用神经网络结构设计的潜在能力。</p>
<p>哪个才能更好解决人体姿态估计问题的手段呢？</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Human-Pose-Estimation/" rel="tag"># Human Pose Estimation</a>
          
        </div>
      

      
      
        <div class="post-widgets">
        
          <div class="wp_rating">
            <div id="wpac-rating"></div>
          </div>
        

        

        
        </div>
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/07/17/hello-world/" rel="next" title="Hello, 这是我的新博客">
                <i class="fa fa-chevron-left"></i> Hello, 这是我的新博客
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/07/20/pytorch-multigpu/" rel="prev" title="PyTorch使用torch.nn.DataParallel进行多GPU训练的一个BUG，已解决">
                PyTorch使用torch.nn.DataParallel进行多GPU训练的一个BUG，已解决 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table Des Matières
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Ensemble
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">yangsenius</p>
              <p class="site-description motion-element" itemprop="description">Talk is not cheap</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">articles</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="mailto:yangsenius@seu.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#G-RMI-gt-PersonLab-gt-PifPaf-Composite-Fields-for-Human-Pose-Estimation-CVPR-2019-论文解读"><span class="nav-number">1.</span> <span class="nav-text">G-RMI-&gt; PersonLab -&gt; PifPaf Composite Fields for Human Pose Estimation - CVPR 2019 论文解读</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-RMI"><span class="nav-number"></span> <span class="nav-text">G-RMI</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#G-RMI的开创行思路-keypoints-location-disk-mask-logistic-classication-and-short-range-offset解决了下采样导致对量化误差问题"><span class="nav-number"></span> <span class="nav-text">G-RMI的开创行思路:keypoints location disk mask logistic classication and short-range offset解决了下采样导致对量化误差问题!!</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#PersonLab"><span class="nav-number"></span> <span class="nav-text">PersonLab</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Fast-greedy-decoding"><span class="nav-number">1.</span> <span class="nav-text">Fast greedy decoding</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#OpenPIFPAF"><span class="nav-number"></span> <span class="nav-text">OpenPIFPAF</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#PIF"><span class="nav-number"></span> <span class="nav-text">PIF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#PAF"><span class="nav-number"></span> <span class="nav-text">PAF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Regression-Loss"><span class="nav-number"></span> <span class="nav-text">Adaptive Regression Loss</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Greedy-Decoding"><span class="nav-number"></span> <span class="nav-text">Greedy Decoding</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#思考"><span class="nav-number">0.1.</span> <span class="nav-text">思考</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">yangsenius</span>

  
</div>









        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  
  <script type="text/javascript">
  wpac_init = window.wpac_init || [];
  wpac_init.push({widget: 'Rating', id: ,
    el: 'wpac-rating',
    color: 'fc6423'
  });
  (function() {
    if ('WIDGETPACK_LOADED' in window) return;
    WIDGETPACK_LOADED = true;
    var mc = document.createElement('script');
    mc.type = 'text/javascript';
    mc.async = true;
    mc.src = '//embed.widgetpack.com/widget.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
  })();
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
